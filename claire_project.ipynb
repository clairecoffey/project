{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "claire_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clairecoffey/project/blob/master/claire_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep76GcW0r5EF",
        "colab_type": "text"
      },
      "source": [
        "# Fairness and the bias-variance trade-off \n",
        "\n",
        "## Claire Coffey\n",
        "\n",
        "## June 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt8eZN7L71OB",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are studying bias and variance errors in the context of fairness, by exploring recidivism data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5j4K9fEtccc",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPz0FbDrdOB",
        "colab_type": "text"
      },
      "source": [
        "Imports: first import the relevant libraries used throughout. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cpVvJ1Dwm48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  from sklearn import svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncHkSsNUw48L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# svm.SVC?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3xUTfnrkM0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojFZOY-LtsdL",
        "colab_type": "text"
      },
      "source": [
        "# Read in recidivism data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLIFxjZ9roCK",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are studying recidivism data. We utilise the COMPAS recidivism dataset, which uses recidivism data from Broward County jail and has been explored in the following studies:\n",
        "\n",
        "\"The accuracy, fairness, and limits of predicting recidivism\", paper available at:\n",
        "https://advances.sciencemag.org/content/4/1/eaao5580#corresp-1\n",
        "\n",
        "\"Machine Bias\" ProPublica article, available at:\n",
        "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
        "\n",
        "The dataset used can be found at:\n",
        "https://github.com/propublica/compas-analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmO2E_MbHUGY",
        "colab_type": "text"
      },
      "source": [
        "Here we import and read in the recidivism data. Currently, we are using a selection of 1000 samples from this dataset for our predictions (the first 1000 samples of the dataset)\n",
        "\n",
        "We use a selection \n",
        "of fields from this dataset to predict recidivism classification (1 = will reoffend; 0 = will not reoffend). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk62wPdCURBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdCKnj1kqViI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_file():\n",
        "  full_data = False\n",
        "  print(\"loading data\")\n",
        "  if full_data:\n",
        "    # full dataset\n",
        "    file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/compas-scores-two-years%20-%20compas-scores-two-years.csv?token=ABPC6VNTFTGQBANNUJY2O4C6XGJGY\"\n",
        "  else:\n",
        "    # small subset of first 500/1000/2000 people\n",
        "    # file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/500-compas-scores-two-years%20-%20Sheet1%20(1).csv?token=ABPC6VOW7CBEIIGZVE6ZJYS6YKNHO\"\n",
        "    # file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/1000-compas-scores-two-years%20-%20Sheet1.csv\"\n",
        "    file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/2000-compas-scores-2-years.csv\"\n",
        "\n",
        "  # load CSV contents\n",
        "  all_data = pd.read_csv(file_path, delimiter=',', dtype={'sex': 'category', \n",
        "                                                          'age_cat': 'category',\n",
        "                                                          'race': 'category',\n",
        "                                                          'c_charge_degree': 'category',\n",
        "                                                          'c_charge_desc': 'category',\n",
        "                                                          'r_charge_degree': 'category',\n",
        "                                                          'r_charge_desc': 'category',\n",
        "                                                          'vr_charge_degree': 'category',\n",
        "                                                          'vr_charge_desc': 'category'\n",
        "                                                          })\n",
        "  print('loaded data')\n",
        "  #shuffle into random order so we aren't always testing/training with the same people\n",
        "  #but reset index (each individual still has the same ID)\n",
        "  all_data = all_data.sample(frac=1).reset_index(drop=True)\n",
        "  return all_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1WpLQEMZUNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all_data = load_file()\n",
        "# all_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19AV7fOG0z4h",
        "colab_type": "text"
      },
      "source": [
        "## Import and process data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB8DQVf7g1Nn",
        "colab_type": "text"
      },
      "source": [
        "We import the data into a pandas DataFrame. We begin by cleaning the data, so the crime descriptions are simplified, removing duplicate categories. For example, we merge descriptions such as 'possession of cocaine' and 'possess cocaine', or 'burglary/weapon' and 'burglary and weapon', by removing prepositions, and replacing abreviations and similies. \n",
        "\n",
        "Then,  the categorical data is  split into different fields for each category, and encoded as 0 or 1. For example, an individual with characteristic \"sex: male\" would be encoded as \"male: 1, female: 0\". The sex category is then removed.\n",
        "\n",
        "We then consider which fields to use for prediction. This includes the removal of any fields/columns which contain many NaN values, since these cannot be handled by the classifiers. We choose to remove the columns with many NaNs rather than using an alternative approach such as replacing them with the average so as not to introduce other types of bias. We also then remove rows/individuals containing any further NaN values so there is no longer any NaN values present in the data. \n",
        "\n",
        "We then normalise all of the data in the dataframe, so that when fed into the classifier, the predicitons are not skewed (and potentially different forms of bias introduced).  We do this by using the StandardScaler in the sklearn preprocessing library, and we normalise the data to have a variance of 1.\n",
        "\n",
        "Finally, we define the number of testing/training samples desired and split the data into these two sets appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzJytmnMtkPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def clean_descriptions(description):\n",
        "  description = description.replace(' and ', ' ')\n",
        "  description = description.replace(' / ', ' ')\n",
        "  description = description.replace('possession', 'posess')\n",
        "  description = description.replace('possessing', 'posess')\n",
        "  description = description.replace('with', 'w/')\n",
        "  description = description.replace('w/ ', 'w/')\n",
        "  description = description.replace('w/', ' ')\n",
        "  description = description.replace('attempted', 'att')\n",
        "  description = description.replace('attempt', 'att')\n",
        "  description = description.replace('aggravated', 'agg')\n",
        "  description = description.replace('aggrav', 'agg') \n",
        "  description = description.replace(' of ', ' ')\n",
        "  return description\n",
        "\n",
        "def import_data(all_data):\n",
        "  num_testing_samples = 1600\n",
        "\n",
        "  encoded_sex = (pd.get_dummies(all_data['sex']))\n",
        "  all_data = all_data.drop(columns=['sex'])\n",
        "  all_data = all_data.join(encoded_sex)\n",
        "\n",
        "  encoded_age_cat = (pd.get_dummies(all_data['age_cat']))\n",
        "  all_data = all_data.drop(columns=['age_cat'])\n",
        "  all_data = all_data.join(encoded_age_cat)\n",
        "\n",
        "  encoded_race = (pd.get_dummies(all_data['race']))\n",
        "  all_data = all_data.drop(columns=['race'])\n",
        "  all_data = all_data.join(encoded_race)\n",
        "\n",
        "  encoded_c_charge_degree = (pd.get_dummies(all_data['c_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['c_charge_degree'])\n",
        "  all_data = all_data.join(encoded_c_charge_degree, rsuffix='_c')\n",
        "\n",
        "  #these are joined with suffixes because otherwise columns overlap \n",
        "  all_data['c_charge_desc'] = all_data['c_charge_desc'].astype(str).str.lower()\n",
        "  all_data['c_charge_desc'] = all_data['c_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_c_charge_desc = (pd.get_dummies(all_data['c_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['c_charge_desc'])\n",
        "  all_data = all_data.join(encoded_c_charge_desc, rsuffix='_c')\n",
        "\n",
        "  encoded_r_charge_degree = (pd.get_dummies(all_data['r_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['r_charge_degree'])\n",
        "  all_data = all_data.join(encoded_r_charge_degree, rsuffix='_r')\n",
        "\n",
        "  all_data['r_charge_desc'] = all_data['r_charge_desc'].astype(str).str.lower()\n",
        "  all_data['r_charge_desc'] = all_data['r_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_r_charge_desc = (pd.get_dummies(all_data['r_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['r_charge_desc'])\n",
        "  all_data = all_data.join(encoded_r_charge_desc, rsuffix='_r')\n",
        "\n",
        "  encoded_vr_charge_degree = (pd.get_dummies(all_data['vr_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['vr_charge_degree'])\n",
        "  all_data = all_data.join(encoded_vr_charge_degree, rsuffix='_vr')\n",
        "\n",
        "  all_data['vr_charge_desc'] = all_data['vr_charge_desc'].astype(str).str.lower()\n",
        "  all_data['vr_charge_desc'] = all_data['vr_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_vr_charge_desc = (pd.get_dummies(all_data['vr_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['vr_charge_desc'])\n",
        "  all_data = all_data.join(encoded_vr_charge_desc, rsuffix='_vr')\n",
        "\n",
        "  all_data = all_data.drop(columns=['nan'])\n",
        "  all_data = all_data.drop(columns=['nan_vr'])\n",
        "  all_data = all_data.drop(columns=['nan_r'])\n",
        "\n",
        "  #drop columns not used for predictions, including info such as names, and columns with many NaN values \n",
        "  # all_data_simplified = all_data.drop(columns=['two_year_recid', 'r_days_from_arrest', 'id','name','first','last','dob','days_b_screening_arrest','c_jail_in','c_jail_out','c_case_number','c_offense_date','c_arrest_date','r_case_number','r_offense_date','r_jail_in','r_jail_out','vr_case_number','vr_offense_date','in_custody','out_custody','start','end','violent_recid', 'age','arrest case no charge'])\n",
        "  all_data.columns = map(str.lower, all_data.columns)\n",
        "  #dont use individual crimes, too slow, only use severity of crimes and other info\n",
        "  all_data_simplified = all_data[['juv_fel_count','juv_misd_count','juv_other_count','priors_count','is_recid','is_violent_recid','event','female','male','25 - 45','greater than 45','less than 25','african-american','asian','caucasian','hispanic','native american','other','f','m','(f1)','(f2)','(f3)','(f6)','(m1)','(m2)','(mo3)']]\n",
        "\n",
        "  #remove rows containing NaN values \n",
        "  all_data_simplified = all_data_simplified.dropna()\n",
        "\n",
        "  #Renormalise the data so we have unit variance and mean 0 using built-in preprocessing method in sklearn\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  all_data_scaled = pd.DataFrame(scaler.fit_transform(all_data_simplified),columns=all_data_simplified.columns)\n",
        "\n",
        "  # print(\"testing normalisation, printing mean and variance: \")\n",
        "  # print(all_data_scaled.mean())\n",
        "  # print(all_data_scaled.var())\n",
        "\n",
        "  all_data_and_labels = all_data_scaled.join(all_data[['two_year_recid']])\n",
        "\n",
        "  #split into training and testing with specific number of testing samples\n",
        "  #for now just set testing set to be first num_testing_samples samples in table \n",
        "  testing_data_and_labels = all_data_and_labels[:num_testing_samples]\n",
        "\n",
        "  # if(demographic_to_test != 'all'):\n",
        "    # testing_data_and_labels =  pd.DataFrame.reset_index(testing_data_and_labels.loc[testing_data_and_labels[demographic_to_test] > 0],drop=True)\n",
        "\n",
        "  #and training set to be the remainder\n",
        "  training_data_and_labels = all_data_and_labels[num_testing_samples:]\n",
        "\n",
        "  return training_data_and_labels, testing_data_and_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUKy0e7IBQYI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Vx5Lghqe2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training_data_and_labels, testing_data_and_labels = import_data(all_data)\n",
        "# training_data_and_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5FvDlXt4se",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmKSjOeK4aRj",
        "colab_type": "text"
      },
      "source": [
        "##Selecting Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs36jQoqYmpp",
        "colab_type": "text"
      },
      "source": [
        "Here we select the classification model to use. We are using a selection of built-in classifiers in scikit-learn. \n",
        "\n",
        "Currently, we are using RBF SVM models (https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html). \n",
        "\n",
        "We define the boolean values ```vary_gamma ``` and  ```vary_c``` to define whether we are varying the gamma or C parameters; C defines the misclassification penalty and gamma defines the spread of the kernel. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZv_kuCWYo_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, neighbors, svm, gaussian_process, tree, ensemble, neural_network, metrics\n",
        "\n",
        "def define_classifiers():\n",
        "\n",
        "  vary_gamma = False\n",
        "  vary_c = False \n",
        "  polynomial = True\n",
        "  gammas = []\n",
        "  cs = []\n",
        "  classifiers = []\n",
        "  degrees = []\n",
        "\n",
        "  if vary_gamma:\n",
        "    gammas = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1,5,10,50,100,500,1000]\n",
        "    # gammas = [100, 1000, 10000, 100000]\n",
        "    cs=[10000]\n",
        "    # cs = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
        "    # gammas = [0.001]\n",
        "    # gammas = [1]\n",
        "    # c_val = 1000\n",
        "    #fix size of C if varying gamma\n",
        "\n",
        "    for gamma_val in gammas:\n",
        "      for c_val in cs:\n",
        "        classifiers.append(svm.SVC(gamma=gamma_val,C=c_val, probability=True))\n",
        "\n",
        "  if vary_c:\n",
        "    cs = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 10000000]\n",
        "    gamma_val = 1\n",
        "    #fix size of gamma if varying C\n",
        "    for c_val in cs:\n",
        "      classifiers.append(svm.SVC(gamma=gamma_val,C=c_val, probability=True))\n",
        "\n",
        "  if polynomial:\n",
        "    degrees = [1,2,3,4,5,6,7,8]\n",
        "    # degrees = [2]\n",
        "    for degree in degrees:\n",
        "      classifiers.append(svm.SVC(kernel='poly', degree=degree, probability=True))\n",
        "\n",
        "  return classifiers, gammas, cs, degrees\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQD8oVE-a4-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define_classifiers()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7PUBJeb4SQh",
        "colab_type": "text"
      },
      "source": [
        "## Bootstrapping "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxB1XKpC4fYi",
        "colab_type": "text"
      },
      "source": [
        "The classification process then uses a bootstrapping procedure with the chosen model, to generate predictions of recidivism classifications (1 = will not reoffend (positive case); 0 = will reoffend (negative case)).\n",
        "\n",
        "Bootstrapping (https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_41) is a sampling with replacement procedure. The sample size is the same as the size of the (training) dataset. The bootstrapping procedure is run many times to generate different training datasets, which will then be used for classification. In turn, the classification results will be used to calculate and study the bias and variance errors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzZ6oTWgv3FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_bootstrap(training_data_and_labels):\n",
        "  # this is one bootstrap sample \n",
        "  indices = np.random.randint(0,training_data_and_labels.shape[0] , training_data_and_labels.shape[0])\n",
        "  indices.sort()\n",
        "  data_points = []\n",
        "\n",
        "  for i in indices:\n",
        "    data_points.append(training_data_and_labels.iloc[i])\n",
        "\n",
        "  b_sample = pd.DataFrame(data_points)\n",
        "  \n",
        "  return b_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RPB3I-bwLF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b_sample = do_bootstrap(training_data_and_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yArDHhIdCfIS",
        "colab_type": "text"
      },
      "source": [
        "### Calculate average prediction for each individual over all bootstrap samples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjQeJu-rCpkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_avg_prediction(predictions):\n",
        "  #each row is bootstrap sample, each column an individual\n",
        "  return majority_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0p40u-y5sf2",
        "colab_type": "text"
      },
      "source": [
        "## Perform classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDUs2m5A6yP3",
        "colab_type": "text"
      },
      "source": [
        "Fit the model on the training data (which is one bootstrap data sample as defined above)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaofDUc-P8Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(clf, b_sample, testing_data_and_labels):\n",
        "\n",
        "    #training data is everything apart from two year recid 0/1 label from the bootstrap sample\n",
        "    X_train = b_sample.drop(columns=['two_year_recid'])\n",
        "    y_train = b_sample['two_year_recid']\n",
        "    X_test = testing_data_and_labels.drop(columns=['two_year_recid'])\n",
        "    y_test = testing_data_and_labels['two_year_recid']\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    soft_score = clf.decision_function(X_test)\n",
        "    y_true = y_test\n",
        "\n",
        "    return y_pred, y_true, soft_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8e8STm77eG",
        "colab_type": "text"
      },
      "source": [
        "Perform classification for each bootstrap sample separately, and store these in a DataFrame, to be passed into the bias/variance calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm8R-Gt-OmNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(training_data_and_labels, testing_data_and_labels, clf):\n",
        "    count = 0\n",
        "\n",
        "    num_bootstraps = len(training_data_and_labels);\n",
        "    while count <= num_bootstraps:\n",
        "      b_sample = do_bootstrap(training_data_and_labels)\n",
        "      y_pred, y_true, soft_score = fit_model(clf, b_sample, testing_data_and_labels)\n",
        "      if(count == 0):\n",
        "        predictions = pd.DataFrame(pd.Series(y_pred)).transpose()\n",
        "        #true labels are the same for every sample so we only need 1 row in df\n",
        "        true_labels = pd.DataFrame(pd.Series(y_true)).transpose()\n",
        "      else:\n",
        "        predictions = predictions.append(pd.DataFrame(pd.Series(y_pred)).transpose())\n",
        "      count += 1\n",
        "      \n",
        "    return predictions, true_labels, soft_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H75T2WV0OqIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classify(training_data_and_labels, testing_data_and_labels, clf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMfhfngzvTX",
        "colab_type": "text"
      },
      "source": [
        "## Correcting for Fairness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBQkNPgD4D6o",
        "colab_type": "text"
      },
      "source": [
        "#Fairness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frcBqdbB3M22",
        "colab_type": "text"
      },
      "source": [
        "The definition of fairness is disputed, and there is not a single correct approach to ensuring fairness in machine learning. In general, as stated in https://arxiv.org/pdf/1711.08513.pdf, fairness in machine learning can be approached in two ways: fairness of the dataset itself; fairness of the model.\n",
        "\n",
        "Since we cannot control the process by which the data is collected, and the recidivism dataset already exists (likely with human and societal biases built-in), we will not be focusing on the former category. Although, there have been recent trends within the fairness and machine learning communities to argue the importance of the fairness in data collection, for example, in http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, the authors discuss the necessity of correcting for bias in the dataset, an approach which may actually increase the accuracy of the predictions, in contrast to approaches that exclusively focus on correcting for fairness in the models, at the expense of accuracy. Another area in which recent trends in fairness research have addressed is the importance of developing context-aware fairness measurements (https://arxiv.org/pdf/1805.05859.pdf). However, in our project we will focus on model-based fairness correction - ensuring the machine learning models are not perpetuating existing biases, or introducing new biases. We do this by using a widely used and accepted fairness measurement which is context-independent, known as **Equalised Odds** (http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf). This approach is not without criticism, however it provides a clear and well-motivated approach to achieving fair predictions across subgroups with different protected characteristics. We attempt to correct for fairness in relation to the protected characteristics found in the recidivism dataset (sex, race, age). Once our models are 'fair' in relation to this description, we can explore the relationship between bias and variance errors and the potential discovery of discrimination against new categories. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vab-_uQEx46Q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Equalised Odds\n",
        "\n",
        "As stated above, we are considering fairness in relation to the equalised odds metric (http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf). The definition as stated in this paper is as follows: \\\\\n",
        "We say that a predictor $\\hat{Y}$ satisfies equalized odds with respect to\n",
        "protected attribute $A$ and outcome $Y$, if $\\hat{Y}$ and $A$ are independent conditional on $Y$. Therefore, if the classification labels are $Y$ and $\\hat{Y}$, for an outcome $ y=1 $, $\\hat{Y}$ has equal true positive rates across all demographic groups, for example, the categories not female and female will have equal true positive rates. For an outcome  $ y=0 $, $\\hat{Y}$ has equal false positive rates across all demographic groups. This enforces equal bias and accuracy in all demographics. This can formally be stated as:\n",
        "$$ Pr \\left\\{ \\hat{Y}=1 | A = 0, Y = y \\right\\} = Pr \\left\\{ \\hat{Y}=1 | A = 0, Y = y \\right\\} , y \\in \\left\\{ 0,1 \\right\\}$$\n",
        "\n",
        "This approach punishes models that only perform well on the majority demographics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cat-Jl0kJu2C",
        "colab_type": "text"
      },
      "source": [
        "The following code is the implementation of equalised odds from the paper http://papers.nips.cc/paper/7151-on-fairness-and-calibration.pdf, the code is from the github repository https://github.com/gpleiss/equalized_odds_and_calibration/blob/master/eq_odds.py.\n",
        "\n",
        "This implementation equalises false positives and false negatives across demoographics, since in general, African Americans receive\n",
        "a disproportionate number of F.P. predictions as compared with Caucasians when automated risk tools are used in practice. In the context of recidivism, the 'positive case' is in fact a prediction of 0: the individual is predicted not to reoffend, so equalising false positives in this case fits with the equalised odds definition above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDXXgyFKJZOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_scores(predictions, soft_scores):\n",
        "\n",
        "  maximum = soft_scores.max()\n",
        "  minimum = soft_scores.min()\n",
        "\n",
        "  normalise = lambda x: ((x-minimum)/(maximum-minimum))\n",
        "  n_soft_scores = []\n",
        "  for score in soft_scores:\n",
        "    soft_score = normalise(score)\n",
        "    n_soft_scores.append(soft_score)\n",
        "\n",
        "  return n_soft_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2WbfvPkLImE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def constrain_with_equalised_odds(true_labels, group_ids, demographic, soft_scores):\n",
        "\n",
        "  #create csv file containing relevant fields: prediction, label, group \n",
        "  predictions = pd.DataFrame(soft_scores, columns=['prediction'])\n",
        "  true_labels = pd.melt(true_labels).rename(columns={'value':'label'})\n",
        "  group_ids = pd.melt(group_ids).rename(columns={'value':'group'})\n",
        "  eq_odds_input = pd.concat([predictions, true_labels], axis=1)\n",
        "  eq_odds_input = pd.concat([eq_odds_input, group_ids], axis=1)\n",
        "  eq_odds_input = eq_odds_input.drop(columns=[\"variable\"])\n",
        "  eq_odds_input.to_csv('/content/drive/My Drive/project_data/eq_odds.csv', index=True)\n",
        "\n",
        "  !python2 \"/content/drive/My Drive/project_data/eq_odds.py\" \"/content/drive/My Drive/project_data/eq_odds.csv\"\n",
        "\n",
        "  eq_odds_pred_group_0 = pd.read_csv(filepath_or_buffer='/content/group_0.csv', delimiter=',', header=0)\n",
        "  eq_odds_pred_group_1 = pd.read_csv(filepath_or_buffer='/content/group_1.csv', delimiter=',',header=0 )\n",
        "  eq_odds_pred = pd.concat([eq_odds_pred_group_0,eq_odds_pred_group_1])\n",
        "  eq_odds_pred =  pd.DataFrame.reset_index(eq_odds_pred,drop=True)\n",
        "  eq_odds_pred['round_predictions'] = [0 if (row < 0.5) else 1 for row in eq_odds_pred['predictions']]\n",
        "\n",
        "  return eq_odds_pred['round_predictions'], eq_odds_pred['true_labels']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5hobrODGwef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calc_conf_matrix(y_true, y_pred):\n",
        "  # get confusion matrix and compute tn,fp,fn,tp\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true.iloc[0].to_numpy(), y_pred.iloc[0].to_numpy()).ravel()\n",
        "  print(\"true negatives:\", tn, \"rate:\" , tn/(tn+fp+fn+tp), \"false positives:\", fp, \"rate:\", fp/(tn+fp+fn+tp) ,\"false negatives:\", fn,\"rate:\", fn/(tn+fp+fn+tp), \"true positives:\",tp,\"rate:\", tp/(tn+fp+fn+tp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnKB0sC3uU6-",
        "colab_type": "text"
      },
      "source": [
        "# Compute bias/variance errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdwnNxEnWqth",
        "colab_type": "text"
      },
      "source": [
        "Using all these bootstrap predictions, we calculate the average misclassification error which is the same as the zero-one loss. This is done by first calculating the overall misclassification loss across bootstrap sample predictions, finding the average misclassification error for each datapoint (individual). As described in:\n",
        "http://www.cems.uwe.ac.uk/~irjohnso/coursenotes/uqc832/tr-bias.pdf\n",
        "\n",
        "We can then decompose the error into the errors due to bias, and the errors due to variance, in order to study the behaviour of the model and the bias/variance tradeoff. This decomposition for classification is described in: \n",
        "https://homes.cs.washington.edu/~pedrod/bvd.pdf\n",
        "https://pdfs.semanticscholar.org/9253/f3e13bca7e845e60394d85ddaec0d4cfc6d6.pdf  \n",
        "\n",
        "<!-- https://www.stat.berkeley.edu/users/breiman/arcall96.pdf.  -->\n",
        "\n",
        "<!-- The error is also comprised of an error due to noise (in addition to bias and variance). However, as stated in http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, the noise is dependent on the data, not the model, so comparing the discrimination level in the form of bias and variance errors, the noise terms cancel since they are independent of the model. Therefore, differences in bias can be explored even without knowing the underlying noise of the data.  -->\n",
        "\n",
        "We calculate the bias and variance errors for each individual, following zero-one loss rules under misclassification loss, as described by Domingos https://homes.cs.washington.edu/~pedrod/bvd.pdf, http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/. We can then calculate the overall average bias error and variance error for the prediction. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7WHEx8dq6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_bias_variance(predictions, true_labels):\n",
        "\n",
        "  # print(\"predictions: \")\n",
        "  # print(predictions)\n",
        "  # print(\"true labels: \")\n",
        "\n",
        "  biases = []\n",
        "  variances = []\n",
        "  avg_errors = []\n",
        "  misclassified_individuals = []\n",
        "  losses = []\n",
        "  noises = []\n",
        "  \n",
        "\n",
        "  # calculate the bias and variance for each value of X,y\n",
        "  # for misclassification loss\n",
        "\n",
        "  # print(\"predictions:\")\n",
        "  # print(predictions)\n",
        "  # print(\"predictions isna:\")\n",
        "  # print(predictions.isna())\n",
        "  # print(\"predictions isna any:\")\n",
        "  # print(predictions.isna().any())\n",
        "  # print(predictions.loc[predictions.isna().any(axis=1), :])\n",
        "  # print(predictions.loc[(~predictions.applymap(np.isreal)).any(axis=1), :])\n",
        "\n",
        "  main_predictions_first = predictions.mode(dropna=False)  \n",
        "  main_predictions = main_predictions_first.copy()\n",
        "  main_predictions = main_predictions.iloc[0,:]\n",
        "\n",
        "  # main_predictions = pd.melt(main_predictions)\n",
        "  # main_predictions = main_predictions.iloc[:,1]\n",
        "\n",
        "  #  print(predictions.isnull().values.any())\n",
        "  # #to get the main prediction for each datapoint \n",
        "  # print(main_predictions_first) \n",
        "  # main_predictions = pd.melt(main_predictions_first).iloc[:,1]\n",
        "  # print(main_predictions)\n",
        "\n",
        "\n",
        "  #compare main (modal) prediction to true prediction \n",
        "  main_predictions_misclassified_relative_to_true = main_predictions_first.apply(lambda z : z != true_labels.iloc[0], axis=1)\n",
        "  #find whether each element is misclassified for each bootstrap sample \n",
        "  #find if each prediction is the same as the true prediction \n",
        "  predictions_misclassified_relative_to_true = predictions.apply(lambda x : x != true_labels.iloc[0], axis=1)\n",
        "  #find if the main (mode) prediction is the same as the actual prediction\n",
        "  predictions_misclassified_relative_to_main = predictions.apply(lambda y : y != main_predictions, axis = 1)\n",
        "\n",
        "  # predictions_misclassified_noise = noise_predictions.apply(lambda z : z != true_labels.iloc[0], axis=1)\n",
        "\n",
        "  main_misclassified_true_counts = main_predictions_misclassified_relative_to_true.apply(np.sum)\n",
        "  #count number of times misclassified for each datapoint across all bootstrap samples \n",
        "  misclassified_true_counts = predictions_misclassified_relative_to_true.apply(np.sum)\n",
        "  misclassified_main_counts = predictions_misclassified_relative_to_main.apply(np.sum)\n",
        "  # misclassified_noise_counts = predictions_misclassific_noise.apply(np.sum)\n",
        "\n",
        "\n",
        "  #average misclassification error for each individual/datapoint \n",
        "  #same as probability of incorrect classification\n",
        "  avg_true_errors = misclassified_true_counts.apply(lambda a : np.divide(a,len(predictions)))\n",
        "  avg_main_errors = misclassified_main_counts.apply(lambda b : np.divide(b,len(predictions)))\n",
        "  avg_main_true_errors = main_misclassified_true_counts.apply(lambda c : np.divide(c,len(main_predictions_first)))\n",
        "  # avg_noise_errors = misclassified_noise_counts.apply(lambda c : np.divide(c, len(predictions)))\n",
        "\n",
        "  for i in range(len(avg_true_errors)):\n",
        "    # if average error is less than 0.5 then it means the main prediction is the same as the optimal one\n",
        "    avg_main_true_error = avg_main_true_errors[i]\n",
        "    avg_true_error = avg_true_errors[i]\n",
        "    avg_main_error = avg_main_errors[i]\n",
        "    bias = 0 if avg_main_true_error <=0.5 else 1\n",
        "    variance = avg_main_error\n",
        "    #noise is the underlying variance of the data: error when the optimal and true classifications are the same\n",
        "    #independent of model, we can say it is 0 as doesn't effect bias/var relationship\n",
        "    #and when we tested this, it returned a value of 0 (all labels predicted correctly across all samples so no loss incurred)\n",
        "    noise = 0\n",
        "    c1 = ((2*(1-avg_true_error))-1)\n",
        "    c2 = 1 if avg_main_true_error <= 0.5 else -1\n",
        "    #loss according to domingos' decomposition\n",
        "    loss = (c1*noise) + bias + (c2*variance)\n",
        "    biases.append(bias)\n",
        "    variances.append(variance)\n",
        "    noises.append(noise)\n",
        "    losses.append(loss)\n",
        "    if avg_true_error > 0.5:\n",
        "      misclassified_individuals.append(i)\n",
        "\n",
        "  avg_bias = np.mean(biases)\n",
        "  # avg_var = abs(np.mean(avg_errors) - avg_bias)\n",
        "  avg_var = np.mean(variances)\n",
        "  # avg_error = np.mean(avg_errors)\n",
        "  avg_loss = np.mean(losses)\n",
        "  avg_noise = np.mean(noises)\n",
        "\n",
        "  print(\"average loss:\") \n",
        "  print(avg_loss)\n",
        "  print(\"average noise:\")\n",
        "  print(avg_noise)\n",
        "  print(\"average bias:\")\n",
        "  print(avg_bias)\n",
        "  print(\"average variance:\")\n",
        "  print(avg_var)\n",
        "\n",
        "  return avg_bias, avg_var, avg_loss, misclassified_individuals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ygDcLeBeNq",
        "colab_type": "text"
      },
      "source": [
        "## Identifying Categories of Discrimination "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAxWqE2JBiT9",
        "colab_type": "text"
      },
      "source": [
        "We hope to address the question: Are models that exhibit high bias errors likely to introduce new categories of discrimination? \n",
        "\n",
        "We can therefore look at the bias and variance errors for different models.\n",
        "\n",
        "\n",
        "We want to see that if the variance is low and bias high, is it consistently discriminating against a certain subgroup, potentially introducing a new type of discrimination? Unlike other work such as http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, it doesn't have to be a protected characteristic.\n",
        "\n",
        "\n",
        "\n",
        "I think perhaps after fairness correction, we can look at who is misclassified and then what they have in common? Or, we can extract each \"subgroup\" based on whatever characteristics and analyse these - i.e. did FPR/FNR go up/down for a different subgroup after fairness correction?  Do this in same way as we do for \"demographics\" initially used for fairness correction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS-RcQVHudhq",
        "colab_type": "text"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kav2TxMqeTy2",
        "colab_type": "text"
      },
      "source": [
        "Creating the appropriate plots to visualise our results. We plot: \n",
        "\n",
        "1.   Bias error vs Variance error\n",
        "2.   Gamma value of RBF SVM vs Variance error\n",
        "3.   Gamma value of RBF SVM vs Bias error\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcztpNvvfDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt                                  \n",
        "def plot_bias_variance(biases, variances, gammas, cs, degrees, losses):   \n",
        "  # print(\"plotting bias/var\") \n",
        "  # plt.scatter(biases, variances)                                              \n",
        "  # plt.title('bias vs variance errors')                                     \n",
        "  # plt.xlabel('bias')                                                       \n",
        "  # plt.ylabel('variance')                                                   \n",
        "  # plt.show()\n",
        "\n",
        "  # plt.scatter(gammas, variances)\n",
        "  # plt.xscale('log')                                              \n",
        "  # plt.title('RBF SVM, C = 100000 \\n gamma size vs variance errors')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('variance')                                                   \n",
        "  # plt.show()                                                            \n",
        "\n",
        "  # plt.scatter(gammas, biases)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, C = 100000 \\n gamma size vs bias errors')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('bias')                                                   \n",
        "  # plt.show()            \n",
        "\n",
        "  # plt.scatter(gammas, losses)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, C = 100000 \\n gamma size vs total error')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('error')                                                   \n",
        "  # plt.show()   \n",
        "\n",
        "  # plt.scatter(cs, biases)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, gamma=0.1, C value vs bias errors')                                     \n",
        "  # plt.xlabel('C value')                                                       \n",
        "  # plt.ylabel('bias')                                                   \n",
        "  # plt.show()       \n",
        "\n",
        "  # plt.scatter(cs, variances)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, gamma=0.1, C value vs variance errors')                                     \n",
        "  # plt.xlabel('C value')                                                       \n",
        "  # plt.ylabel('variance')                                                   \n",
        "  # plt.show()    \n",
        "\n",
        "  # plt.scatter(cs, losses)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, gamma=0.1, C value vs zero-one loss')                                     \n",
        "  # plt.xlabel('C value')                                                       \n",
        "  # plt.ylabel('loss')                                                   \n",
        "  # plt.show()   \n",
        "\n",
        "  plt.scatter(biases, variances)                                              \n",
        "  plt.title('bias vs variance errors')                                     \n",
        "  plt.xlabel('bias')                                                       \n",
        "  plt.ylabel('variance')                                                   \n",
        "  plt.show()\n",
        "\n",
        "  plt.scatter(degrees, variances)\n",
        "  plt.title('poly SVM, degree size vs variance errors')                                     \n",
        "  plt.xlabel('degree')                                                       \n",
        "  plt.ylabel('variance')                                                   \n",
        "  plt.show()                                                            \n",
        "\n",
        "  plt.scatter(degrees, biases)                               \n",
        "  plt.title('poly SVM, degree size vs bias errors')                                     \n",
        "  plt.xlabel('degree')                                                       \n",
        "  plt.ylabel('bias')                                                   \n",
        "  plt.show()            \n",
        "\n",
        "  plt.scatter(degrees, losses)                               \n",
        "  plt.title('poly SVM, degree size vs generalisation errors')                                     \n",
        "  plt.xlabel('degree')                                                       \n",
        "  plt.ylabel('error')                                                   \n",
        "  plt.show()  \n",
        "\n",
        "#just an example of if we want to plot the misclassified individuals against a characteristic from the dataframe \n",
        "#might help to look for patterns \n",
        "def plot_misclassified(misclassified):\n",
        "  misclassified.reset_index().plot(kind='scatter', x='index', y='age') \n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmVUHkRN8YMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download CSV file containing all the info for the individuals who are consistently misclassified (i.e. >50% of the time, resulting in bias errors)\n",
        "def download_misclassified(misclassified, name):\n",
        "  # misclassified = np.asarray(misclassified)\n",
        "  csv_name = name+'.csv'\n",
        "  # misclassified = pd.DataFrame(misclassified)\n",
        "  # print(misclassified)\n",
        "  # np.savetxt(csv_name, misclassified, delimiter=\",\")\n",
        "  misclassified.to_csv(r''+name+'.csv', index=False)\n",
        "  # from google.colab import files\n",
        "  # files.download(csv_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nbxldKcpOHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwemXVIVuhGq",
        "colab_type": "text"
      },
      "source": [
        "# Main method (execute code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vpU8QERWV4i",
        "colab_type": "text"
      },
      "source": [
        "Main method to run the system, executing methods in appropriate sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCBsC158supR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  all_data = load_file()\n",
        "  # demographics = ['african-american', 'caucasian', 'hispanic', 'asian', 'other', 'male', 'female', 'less than 25', '25 - 45', 'greater than 45']\n",
        "  demographics = ['african-american']\n",
        "  # demographics = [ 'hispanic', 'asian', 'other', 'male', 'female', 'less than 25', '25 - 45', 'greater than 45']\n",
        "  testing_data_and_labels_list = []\n",
        "  training_data_and_labels_list = []\n",
        "  training_data_and_labels, testing_data_and_labels = import_data(all_data)\n",
        "  training_data_and_labels_list.append(training_data_and_labels)\n",
        "  testing_data_and_labels_list.append(testing_data_and_labels)\n",
        "\n",
        "  biases = []\n",
        "  variances = []\n",
        "  total_errors = []\n",
        "  avg_losses = []\n",
        "  biases_eq = []\n",
        "  variances_eq = []\n",
        "  total_errors_eq = []\n",
        "  avg_losses_eq = []\n",
        "  classifiers, gammas, cs, degrees = define_classifiers()\n",
        "  all_misclassified = pd.DataFrame()\n",
        "  all_misclassified_eq = pd.DataFrame()\n",
        "  # misclassified = []\n",
        "  # misclassified_eq= []\n",
        "  equalised_odds = False\n",
        "  k = 0\n",
        "\n",
        "  for classifier in classifiers:\n",
        "    print(classifier)\n",
        "    # clf = classifiers[classifier_names.index(classifier)]\n",
        "    clf = classifier\n",
        "    for i in range(len(training_data_and_labels_list)):\n",
        "      # print('testing ', demographics[i], '=', true_case)\n",
        "      predictions, true_labels, soft_scores = classify(training_data_and_labels_list[i], testing_data_and_labels_list[i], clf)  \n",
        "      # predictions, true_labels = classify_noise(training_data_and_labels_list[i], testing_data_and_labels_list[i], clf)\n",
        "      # noise_predictions = predictions\n",
        "      soft_scores = calculate_scores(predictions, soft_scores)\n",
        "      majority_predictions = predictions.mode(numeric_only=True)\n",
        "      calc_conf_matrix(true_labels, majority_predictions)\n",
        "      if(equalised_odds):\n",
        "        for demographic in demographics:\n",
        "          group_ids = [] \n",
        "          print(\"Equalising odds for \", demographic)\n",
        "          group_ids_normalised = testing_data_and_labels[demographic]\n",
        "          for n_id in group_ids_normalised:\n",
        "            group_id = 0 if n_id < 0 else 1\n",
        "            group_ids.append(group_id)\n",
        "          group_ids = pd.DataFrame([group_ids])\n",
        "          all_eq_predictions = []\n",
        "          for j in range (len(predictions.index)):\n",
        "            eq_predictions, true_eq_labels = constrain_with_equalised_odds(true_labels, group_ids, demographic, soft_scores)\n",
        "            all_eq_predictions.append(np.array(eq_predictions))\n",
        "          all_eq_predictions = pd.DataFrame(all_eq_predictions)\n",
        "          true_eq_labels = pd.DataFrame(np.array(true_eq_labels), columns=['two_year_recid'])\n",
        "          true_eq_labels = true_eq_labels.transpose()\n",
        "      if(equalised_odds):\n",
        "        #if not equalised odds we have to only use half of the predictions somehow otherwise it's not fair \n",
        "        #because in equalised odds we use half of it to determine parameters (validation)\n",
        "        predictions = predictions.iloc[:,:int(len(predictions.columns)/2)]\n",
        "        true_labels = true_labels.iloc[:,:int(len(true_labels.columns)/2)]\n",
        "      # noise_predictions = noise_predictions.iloc[:,:int(len(predictions.columns)/2)]\n",
        "      # print(predictions)\n",
        "      # fake_predictions = predictions.replace(predictions, 0)\n",
        "      # fake_true_labels = true_labels.replace(true_labels, 1)\n",
        "      bias, variance, avg_loss, misclassified_individuals = compute_bias_variance(predictions, true_labels)\n",
        "      biases.append(bias)\n",
        "      variances.append(variance)\n",
        "      avg_losses.append(avg_loss)\n",
        "      if(equalised_odds):\n",
        "        print(\"after fairness correction:\")\n",
        "        bias_eq, variance_eq, avg_loss_eq, misclassified_individuals_eq = compute_bias_variance(all_eq_predictions, true_eq_labels)\n",
        "        biases_eq.append(bias_eq)\n",
        "        variances_eq.append(variance_eq)\n",
        "        avg_losses_eq.append(avg_loss_eq)\n",
        "      #get the individuals which are misclassified on average (hence contributing to bias errors)\n",
        "      print(\"misclassified before: \", misclassified_individuals)\n",
        "      if(equalised_odds):\n",
        "        print(\"misclassified after: \", misclassified_individuals_eq)\n",
        "      for i in range(len(testing_data_and_labels_list)):\n",
        "        if(len(misclassified_individuals) > 0 ):\n",
        "          misclassified = testing_data_and_labels_list[i].iloc[misclassified_individuals]\n",
        "          # print(misclassified)\n",
        "          all_misclassified = all_misclassified.append(misclassified)\n",
        "          # all_misclassified = pd.concat(all_misclassified, misclassified)\n",
        "        if(equalised_odds):\n",
        "          if(len(misclassified_individuals_eq) > 0):\n",
        "            misclassified_eq = testing_data_and_labels_list[i].iloc[misclassified_individuals_eq]\n",
        "            # all_misclassified_eq = pd.concat(all_misclassified_eq, misclassified_eq)\n",
        "            all_misclassified_eq = all_misclassified_eq.append(misclassified_eq)\n",
        "      download_misclassified(all_misclassified, str(k)+'_misclassified_before')\n",
        "      if(equalised_odds):\n",
        "        download_misclassified(all_misclassified_eq, str(k)+'_misclassified_after')\n",
        "      k+=1\n",
        "      # plot_misclassified(misclassified)\n",
        "      \n",
        "  print(\"before fairness correction:\")\n",
        "  plot_bias_variance(biases, variances, gammas, cs, degrees, avg_losses)\n",
        "  if(equalised_odds):\n",
        "    print(\"after fairness correction:\")\n",
        "    plot_bias_variance(biases_eq, variances_eq, gammas, cs, degrees, avg_losses_eq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA8uQhlb8ARv",
        "colab_type": "code",
        "outputId": "7f01afed-905f-4b0b-d975-682e34ce85a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data\n",
            "loaded data\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=1, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "true negatives: 864 rate: 0.54 false positives: 42 rate: 0.02625 false negatives: 0 rate: 0.0 true positives: 694 rate: 0.43375\n",
            "predictions:\n",
            "    0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna:\n",
            "     0      1      2      3      4     ...   1595   1596   1597   1598   1599\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna any:\n",
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "1595    False\n",
            "1596    False\n",
            "1597    False\n",
            "1598    False\n",
            "1599    False\n",
            "Length: 1600, dtype: bool\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "main_predictions_first:\n",
            "   0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0   1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[1 rows x 1600 columns]\n",
            "first row only\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "average loss:\n",
            "0.026260937499999998\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.02625\n",
            "average variance:\n",
            "1.0937499999999998e-05\n",
            "misclassified before:  [67, 147, 183, 214, 219, 243, 275, 328, 333, 352, 451, 463, 494, 496, 544, 586, 616, 627, 708, 722, 766, 844, 868, 879, 883, 901, 910, 1004, 1086, 1093, 1125, 1140, 1177, 1276, 1366, 1371, 1416, 1452, 1465, 1513, 1522, 1530]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=2, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "true negatives: 859 rate: 0.536875 false positives: 47 rate: 0.029375 false negatives: 27 rate: 0.016875 true positives: 667 rate: 0.416875\n",
            "predictions:\n",
            "    0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna:\n",
            "     0      1      2      3      4     ...   1595   1596   1597   1598   1599\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna any:\n",
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "1595    False\n",
            "1596    False\n",
            "1597    False\n",
            "1598    False\n",
            "1599    False\n",
            "Length: 1600, dtype: bool\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "main_predictions_first:\n",
            "   0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0   1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[1 rows x 1600 columns]\n",
            "first row only\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "average loss:\n",
            "0.057785937499999995\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.04625\n",
            "average variance:\n",
            "0.028807812500000002\n",
            "misclassified before:  [63, 67, 147, 161, 169, 183, 209, 214, 219, 275, 294, 296, 328, 332, 333, 335, 340, 349, 352, 451, 463, 494, 496, 544, 586, 606, 616, 629, 708, 712, 722, 766, 771, 804, 820, 844, 868, 879, 883, 901, 910, 962, 1004, 1013, 1019, 1080, 1086, 1093, 1123, 1125, 1140, 1152, 1177, 1211, 1212, 1218, 1252, 1276, 1281, 1366, 1371, 1396, 1416, 1420, 1452, 1458, 1465, 1481, 1513, 1522, 1530, 1573, 1578, 1586]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "true negatives: 866 rate: 0.54125 false positives: 40 rate: 0.025 false negatives: 6 rate: 0.00375 true positives: 688 rate: 0.43\n",
            "predictions:\n",
            "    0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna:\n",
            "     0      1      2      3      4     ...   1595   1596   1597   1598   1599\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna any:\n",
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "1595    False\n",
            "1596    False\n",
            "1597    False\n",
            "1598    False\n",
            "1599    False\n",
            "Length: 1600, dtype: bool\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "main_predictions_first:\n",
            "   0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0   1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[1 rows x 1600 columns]\n",
            "first row only\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "average loss:\n",
            "0.03868125\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.02875\n",
            "average variance:\n",
            "0.013900000000000001\n",
            "misclassified before:  [67, 147, 161, 183, 189, 209, 214, 219, 275, 294, 328, 333, 349, 352, 451, 463, 494, 496, 544, 586, 616, 627, 708, 722, 766, 844, 868, 879, 883, 901, 910, 1004, 1013, 1086, 1093, 1125, 1174, 1177, 1366, 1371, 1416, 1452, 1465, 1513, 1522, 1530]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=4, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "true negatives: 873 rate: 0.545625 false positives: 33 rate: 0.020625 false negatives: 73 rate: 0.045625 true positives: 621 rate: 0.388125\n",
            "predictions:\n",
            "    0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   1.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna:\n",
            "     0      1      2      3      4     ...   1595   1596   1597   1598   1599\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna any:\n",
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "1595    False\n",
            "1596    False\n",
            "1597    False\n",
            "1598    False\n",
            "1599    False\n",
            "Length: 1600, dtype: bool\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "main_predictions_first:\n",
            "   0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0   1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[1 rows x 1600 columns]\n",
            "first row only\n",
            "0       1.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "0       1.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "average loss:\n",
            "0.09159375\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.06625\n",
            "average variance:\n",
            "0.049634375\n",
            "misclassified before:  [1, 29, 48, 52, 63, 67, 80, 84, 87, 99, 111, 126, 131, 144, 147, 161, 189, 206, 209, 214, 219, 234, 237, 253, 261, 275, 286, 292, 294, 296, 300, 309, 315, 328, 332, 340, 347, 349, 352, 451, 463, 494, 520, 544, 548, 586, 592, 655, 698, 708, 712, 728, 731, 766, 769, 771, 844, 847, 868, 879, 883, 888, 901, 962, 972, 1004, 1013, 1019, 1081, 1093, 1105, 1106, 1123, 1125, 1166, 1170, 1174, 1177, 1212, 1238, 1247, 1281, 1283, 1338, 1344, 1347, 1366, 1368, 1371, 1397, 1416, 1420, 1451, 1452, 1465, 1498, 1507, 1513, 1522, 1530, 1556, 1558, 1570, 1573, 1578, 1581]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=5, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "true negatives: 880 rate: 0.55 false positives: 26 rate: 0.01625 false negatives: 137 rate: 0.085625 true positives: 557 rate: 0.348125\n",
            "predictions:\n",
            "    0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna:\n",
            "     0      1      2      3      4     ...   1595   1596   1597   1598   1599\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna any:\n",
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "1595    False\n",
            "1596    False\n",
            "1597    False\n",
            "1598    False\n",
            "1599    False\n",
            "Length: 1600, dtype: bool\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "main_predictions_first:\n",
            "   0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0   1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "1   NaN   NaN   NaN   NaN   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN\n",
            "\n",
            "[2 rows x 1600 columns]\n",
            "first row only\n",
            "0       1.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "0       1.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "average loss:\n",
            "0.12745625\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.100625\n",
            "average variance:\n",
            "0.063109375\n",
            "misclassified before:  [1, 29, 48, 52, 61, 63, 67, 80, 84, 99, 106, 111, 114, 126, 131, 140, 144, 146, 161, 185, 189, 203, 206, 209, 214, 215, 216, 219, 222, 226, 232, 234, 237, 253, 256, 261, 272, 275, 286, 292, 294, 296, 300, 309, 315, 316, 327, 331, 332, 347, 349, 352, 355, 374, 387, 415, 434, 440, 451, 458, 476, 479, 494, 519, 520, 544, 548, 574, 586, 592, 651, 655, 681, 698, 708, 714, 728, 731, 746, 769, 771, 774, 788, 844, 846, 847, 867, 868, 875, 879, 883, 888, 897, 901, 988, 996, 1002, 1004, 1013, 1028, 1033, 1037, 1039, 1076, 1081, 1093, 1105, 1106, 1125, 1141, 1145, 1155, 1161, 1166, 1168, 1170, 1174, 1177, 1197, 1213, 1215, 1227, 1238, 1241, 1247, 1263, 1267, 1271, 1275, 1281, 1283, 1302, 1321, 1338, 1344, 1347, 1366, 1368, 1382, 1397, 1420, 1424, 1436, 1451, 1452, 1465, 1469, 1485, 1490, 1495, 1498, 1513, 1516, 1530, 1544, 1556, 1558, 1565, 1570, 1580, 1581]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=6, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "true negatives: 885 rate: 0.553125 false positives: 21 rate: 0.013125 false negatives: 310 rate: 0.19375 true positives: 384 rate: 0.24\n",
            "predictions:\n",
            "    0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0    1.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "0    0.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna:\n",
            "     0      1      2      3      4     ...   1595   1596   1597   1598   1599\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna any:\n",
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "1595    False\n",
            "1596    False\n",
            "1597    False\n",
            "1598    False\n",
            "1599    False\n",
            "Length: 1600, dtype: bool\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "main_predictions_first:\n",
            "   0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0   1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[1 rows x 1600 columns]\n",
            "first row only\n",
            "0       1.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "0       1.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       1.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "average loss:\n",
            "0.23752031250000002\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.206875\n",
            "average variance:\n",
            "0.0845390625\n",
            "misclassified before:  [1, 11, 25, 28, 29, 32, 36, 38, 41, 44, 48, 52, 61, 63, 67, 69, 76, 80, 81, 83, 84, 87, 93, 98, 99, 104, 106, 108, 111, 114, 116, 126, 131, 140, 144, 146, 153, 154, 161, 164, 172, 181, 185, 189, 195, 200, 201, 203, 205, 206, 208, 209, 214, 215, 216, 217, 219, 220, 222, 226, 232, 234, 237, 239, 248, 251, 253, 255, 256, 261, 263, 272, 275, 282, 283, 286, 289, 292, 294, 296, 300, 304, 309, 315, 316, 327, 331, 332, 340, 341, 342, 347, 349, 352, 354, 355, 363, 374, 379, 387, 389, 406, 412, 415, 431, 434, 440, 443, 445, 446, 449, 456, 458, 476, 479, 487, 494, 498, 501, 511, 512, 514, 519, 520, 524, 526, 536, 544, 548, 549, 551, 556, 569, 574, 586, 589, 592, 604, 623, 628, 651, 655, 659, 666, 668, 681, 691, 693, 696, 698, 708, 712, 714, 715, 720, 728, 731, 732, 740, 741, 746, 753, 754, 758, 769, 771, 774, 781, 788, 790, 793, 800, 801, 803, 806, 829, 830, 846, 847, 853, 857, 860, 861, 867, 868, 875, 880, 888, 893, 894, 896, 897, 898, 901, 918, 921, 926, 940, 944, 949, 955, 962, 965, 972, 983, 988, 989, 992, 993, 996, 999, 1002, 1004, 1008, 1013, 1026, 1028, 1032, 1033, 1037, 1039, 1057, 1076, 1081, 1085, 1089, 1090, 1091, 1093, 1105, 1106, 1114, 1123, 1125, 1129, 1130, 1141, 1145, 1149, 1155, 1161, 1163, 1166, 1168, 1170, 1174, 1176, 1183, 1195, 1197, 1199, 1205, 1206, 1209, 1212, 1213, 1215, 1221, 1227, 1238, 1241, 1247, 1263, 1267, 1271, 1275, 1281, 1283, 1285, 1302, 1303, 1304, 1307, 1321, 1323, 1337, 1338, 1340, 1344, 1345, 1347, 1352, 1366, 1368, 1382, 1386, 1389, 1397, 1412, 1418, 1420, 1424, 1427, 1436, 1442, 1446, 1451, 1452, 1462, 1465, 1469, 1470, 1485, 1490, 1495, 1498, 1499, 1504, 1506, 1507, 1510, 1513, 1515, 1516, 1530, 1534, 1537, 1539, 1543, 1544, 1550, 1556, 1558, 1565, 1566, 1570, 1573, 1575, 1578, 1580, 1581]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=7, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "true negatives: 892 rate: 0.5575 false positives: 14 rate: 0.00875 false negatives: 410 rate: 0.25625 true positives: 284 rate: 0.1775\n",
            "predictions:\n",
            "    0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    1.0   0.0   0.0   1.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna:\n",
            "     0      1      2      3      4     ...   1595   1596   1597   1598   1599\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna any:\n",
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "1595    False\n",
            "1596    False\n",
            "1597    False\n",
            "1598    False\n",
            "1599    False\n",
            "Length: 1600, dtype: bool\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "main_predictions_first:\n",
            "   0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "1   NaN   NaN   NaN   NaN   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN\n",
            "\n",
            "[2 rows x 1600 columns]\n",
            "first row only\n",
            "0       0.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       0.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "0       0.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       0.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "average loss:\n",
            "0.2795625\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.264375\n",
            "average variance:\n",
            "0.07501875\n",
            "misclassified before:  [0, 1, 3, 11, 25, 28, 29, 32, 36, 38, 41, 42, 44, 45, 46, 48, 52, 61, 63, 67, 69, 71, 76, 79, 80, 81, 83, 84, 87, 90, 93, 98, 99, 101, 104, 106, 108, 111, 112, 114, 116, 119, 126, 128, 131, 140, 141, 144, 146, 153, 154, 155, 161, 164, 166, 168, 172, 181, 185, 189, 190, 195, 200, 201, 202, 203, 205, 206, 208, 209, 214, 215, 216, 217, 218, 220, 222, 226, 227, 230, 232, 234, 237, 239, 247, 248, 251, 253, 255, 256, 258, 260, 261, 263, 265, 272, 282, 283, 286, 289, 292, 294, 296, 300, 304, 307, 309, 315, 316, 319, 327, 331, 332, 340, 341, 342, 347, 349, 354, 355, 363, 368, 369, 374, 376, 379, 387, 389, 397, 404, 406, 412, 415, 422, 425, 431, 434, 435, 439, 440, 443, 445, 446, 449, 456, 458, 476, 479, 487, 498, 501, 511, 512, 514, 519, 520, 524, 526, 536, 538, 544, 548, 549, 551, 556, 561, 569, 572, 574, 582, 589, 592, 603, 604, 610, 615, 623, 626, 628, 645, 651, 655, 659, 660, 664, 666, 668, 679, 681, 690, 691, 692, 693, 696, 698, 708, 712, 714, 715, 720, 728, 730, 731, 732, 736, 740, 741, 744, 746, 753, 754, 758, 767, 769, 771, 774, 781, 788, 790, 793, 800, 801, 803, 806, 808, 819, 825, 827, 829, 830, 833, 840, 846, 847, 852, 853, 857, 860, 861, 867, 868, 875, 880, 888, 893, 894, 896, 897, 898, 901, 918, 921, 926, 938, 940, 944, 948, 949, 955, 962, 964, 965, 972, 974, 978, 979, 981, 983, 984, 988, 989, 992, 993, 996, 999, 1002, 1004, 1006, 1008, 1013, 1026, 1028, 1032, 1033, 1037, 1039, 1042, 1049, 1057, 1064, 1076, 1081, 1084, 1085, 1089, 1090, 1091, 1093, 1098, 1105, 1106, 1111, 1114, 1116, 1125, 1129, 1130, 1133, 1141, 1145, 1149, 1153, 1155, 1156, 1161, 1163, 1166, 1168, 1170, 1174, 1176, 1183, 1195, 1197, 1198, 1199, 1205, 1206, 1209, 1212, 1213, 1215, 1221, 1227, 1238, 1241, 1245, 1247, 1251, 1253, 1263, 1267, 1271, 1275, 1281, 1283, 1285, 1293, 1294, 1302, 1303, 1304, 1307, 1321, 1323, 1332, 1337, 1338, 1340, 1344, 1345, 1347, 1352, 1368, 1374, 1382, 1386, 1389, 1391, 1397, 1401, 1412, 1417, 1418, 1420, 1424, 1427, 1436, 1439, 1442, 1446, 1451, 1452, 1462, 1465, 1469, 1470, 1485, 1488, 1490, 1495, 1498, 1499, 1504, 1506, 1507, 1510, 1515, 1516, 1517, 1530, 1531, 1534, 1537, 1539, 1543, 1544, 1550, 1556, 1558, 1559, 1565, 1566, 1570, 1571, 1573, 1575, 1576, 1578, 1580, 1581, 1587, 1590]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=8, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "true negatives: 895 rate: 0.559375 false positives: 11 rate: 0.006875 false negatives: 496 rate: 0.31 true positives: 198 rate: 0.12375\n",
            "predictions:\n",
            "    0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "..   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "0    0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna:\n",
            "     0      1      2      3      4     ...   1595   1596   1597   1598   1599\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "..    ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "0   False  False  False  False  False  ...  False  False  False  False  False\n",
            "\n",
            "[400 rows x 1600 columns]\n",
            "predictions isna any:\n",
            "0       False\n",
            "1       False\n",
            "2       False\n",
            "3       False\n",
            "4       False\n",
            "        ...  \n",
            "1595    False\n",
            "1596    False\n",
            "1597    False\n",
            "1598    False\n",
            "1599    False\n",
            "Length: 1600, dtype: bool\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 1600 columns]\n",
            "main_predictions_first:\n",
            "   0     1     2     3     4     5     ...  1594  1595  1596  1597  1598  1599\n",
            "0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
            "1   NaN   NaN   NaN   NaN   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN\n",
            "\n",
            "[2 rows x 1600 columns]\n",
            "first row only\n",
            "0       0.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       0.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "0       0.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       0.0\n",
            "4       0.0\n",
            "       ... \n",
            "1595    0.0\n",
            "1596    0.0\n",
            "1597    0.0\n",
            "1598    0.0\n",
            "1599    0.0\n",
            "Name: 0, Length: 1600, dtype: float64\n",
            "average loss:\n",
            "0.320109375\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.316875\n",
            "average variance:\n",
            "0.064134375\n",
            "misclassified before:  [0, 1, 3, 8, 11, 25, 27, 28, 29, 32, 36, 38, 40, 41, 42, 44, 45, 46, 48, 49, 52, 61, 63, 67, 69, 71, 72, 76, 79, 80, 81, 83, 84, 87, 90, 93, 98, 99, 101, 104, 106, 108, 111, 112, 113, 114, 116, 119, 121, 126, 128, 131, 140, 141, 144, 146, 153, 154, 155, 161, 164, 166, 168, 172, 179, 181, 185, 189, 190, 195, 198, 200, 201, 202, 203, 205, 206, 208, 209, 214, 215, 216, 217, 218, 220, 222, 226, 227, 230, 232, 234, 237, 239, 247, 248, 251, 253, 255, 256, 258, 260, 261, 263, 265, 267, 272, 274, 282, 283, 286, 289, 292, 294, 296, 300, 304, 307, 309, 315, 316, 319, 327, 331, 332, 340, 341, 342, 345, 347, 349, 354, 355, 361, 363, 368, 369, 374, 376, 378, 379, 381, 387, 389, 397, 399, 401, 404, 406, 412, 415, 422, 425, 430, 431, 433, 434, 435, 439, 440, 443, 445, 446, 449, 452, 456, 458, 476, 479, 487, 498, 501, 504, 506, 511, 512, 514, 518, 519, 520, 524, 526, 530, 536, 538, 548, 549, 551, 556, 561, 567, 569, 572, 574, 576, 582, 587, 589, 592, 597, 600, 603, 604, 610, 615, 623, 626, 628, 645, 649, 651, 655, 659, 660, 664, 666, 668, 679, 681, 690, 691, 692, 693, 695, 696, 698, 708, 712, 714, 715, 720, 724, 728, 730, 731, 732, 736, 737, 740, 741, 744, 746, 753, 754, 758, 767, 769, 771, 772, 774, 781, 788, 790, 793, 796, 799, 800, 801, 803, 806, 808, 814, 819, 820, 825, 827, 829, 830, 833, 836, 840, 846, 847, 852, 853, 857, 858, 860, 861, 865, 867, 875, 876, 880, 881, 884, 888, 893, 894, 896, 897, 898, 901, 918, 920, 921, 926, 938, 940, 944, 948, 949, 955, 962, 963, 964, 965, 972, 974, 978, 979, 981, 983, 984, 988, 989, 990, 992, 993, 996, 999, 1002, 1004, 1006, 1008, 1013, 1017, 1019, 1026, 1028, 1032, 1033, 1037, 1039, 1042, 1046, 1049, 1053, 1057, 1064, 1076, 1081, 1083, 1084, 1085, 1089, 1090, 1091, 1093, 1095, 1098, 1100, 1105, 1106, 1108, 1109, 1111, 1114, 1116, 1123, 1125, 1129, 1130, 1133, 1136, 1137, 1141, 1145, 1149, 1153, 1155, 1156, 1161, 1163, 1166, 1168, 1170, 1174, 1176, 1183, 1185, 1193, 1195, 1197, 1198, 1199, 1203, 1204, 1205, 1206, 1209, 1212, 1213, 1215, 1220, 1221, 1227, 1238, 1239, 1241, 1245, 1247, 1251, 1253, 1263, 1264, 1267, 1271, 1275, 1277, 1281, 1283, 1285, 1290, 1293, 1294, 1302, 1303, 1304, 1307, 1321, 1323, 1329, 1332, 1337, 1338, 1340, 1344, 1345, 1347, 1352, 1368, 1374, 1382, 1386, 1389, 1391, 1397, 1399, 1401, 1406, 1412, 1417, 1418, 1420, 1424, 1427, 1434, 1436, 1439, 1440, 1442, 1444, 1446, 1451, 1452, 1458, 1460, 1462, 1464, 1465, 1466, 1469, 1470, 1479, 1481, 1485, 1488, 1490, 1492, 1495, 1498, 1499, 1504, 1506, 1507, 1510, 1514, 1515, 1516, 1517, 1518, 1531, 1534, 1535, 1537, 1539, 1543, 1544, 1550, 1556, 1558, 1559, 1565, 1566, 1570, 1571, 1573, 1575, 1576, 1577, 1578, 1580, 1581, 1584, 1586, 1587, 1590]\n",
            "before fairness correction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5gcZZ328e/NJIQBhICMLjmQIMQsUXwJDqDLiiwCE1BJrlXXICIoLuIFLi4aJau7aERUsrvqKi6geOIUTiFvXk8BBQ+okUwIEhKYNYRDMkEJh3CchST83j/qGajpVM/0JFPp6Zn7c119pbpO/eua7rqrnqdSrYjAzMys0g71LsDMzAYnB4SZmRVyQJiZWSEHhJmZFXJAmJlZIQeEmZkVckDUiaSQtH+96xgokn4p6cP1rmNrSbpY0r/Wu45GJukBSUen4X+R9J0SXqOU9VoxB0SDkTRO0g2SHpX0pKS7JZ0qaSdJGyQdVbDMVyVdn4YfkPSCpL0q5lmWQmvi9nkng0tEnBERX6h3HUNFRFwQEdt0wCDpSElrB3q9VjsHROO5HFgDTABeCZwM/CUi/he4BvhAfmZJTcCJwA9yo+9P47rnORDYudyyt46kEfWuwfx3yCvaFv3dPg2zPSPCj618AA8As4GVwBPA94CdctP/EVgFPA4sBMbkpgWwP3AI8BegKTft74E/VnnNZ4CDqkz7G+BpYOfcuOOBR4ARuZo/CyzJzfPvwGdSTRNrfO/HAPcCTwLfBH4FfDg3/UPAPWm7LAIm5KYdC3SkZb+VXxY4Ffgt8FXgMeB8YFSq8aG0rS4GmnPrewdwJ7AB+B3whio1K633EeApYDnw+jTt+8D5afj/pe3c/XgRODVN+2vg5vQ37QD+ocprvRdorxj3z8DC3N9lZfp7dQKfrLKeJuA/gEfJgv2s9Hfq/nvuDlwGPJzWc373Zylty9vStnsiLX9cbt19LVv5d9gPuCU9fxS4Ehhd8X04Og1/DrgiDe8EXJGW2wAsAV6dpn0wfU6eBlYDH0njdwG60rbv/juMya83zXcCsCKt95fAARX1fBK4i+yzdg2572fBtu7tMxvAmcCf0nY8ElgLfBr4M9mB2yjga8C69PgaMCotXzT/XsCPUu2PA78Bdqj3fq3HNql3AY38SB/Au4HxwJ7pC9W9kzkqfYkOTh+cbwC/rvjA7Z+GV1Z8cW8EPlHlNX+eXmcmsE/B9P8B3p97fjXwtYqajybbuR1AtgNaS3ZGUlNApA/208C7gZFkO75NvLyTn04WjAcAI8gC6Xe5ZZ8iC8ERwNnARnoGxCbgY2l6M9lOamHaxq8g24F/Kc0/lWyHf1h6L6ek9ziqoO42YCkwmiwsDgD2TtO+3/23q1jmuPRlH0+201pDtlMbkV77UWBKwXI7p200KTduCTAzDT8MvCUN7wEcXGVbn5E+H+PSfD+nZ0DcCFySansVcDsv72RPTdv2H9O2+Wh6L6px2cq/w/5kBwajgBbg1xR8ttLw53g5ID6S/mY7pzreCOyWpr2dLHgEvBV4rntbkHaqFdsjv97XAs+mmkYCnyL73O2Yq+d2smDZk2znf0aV7Vz1M5v7vt6c1tOcatsEfCVtj2ZgDrA4bcsWsoOVL+TeS+X8XyI72BmZHm/p/tsMlkfdC2jkR/oAnpF7fjxwXxq+DLgwN23X9GWdmPvAdQfEp4Er0/Ce6Uuyd5XX3AP4MtlR02ayI+dDctM/C9yUhndL65paUfPRab4vAdPSB38EtQfEB4DFueciC5nunfxPgdNy03dIdUxIy/6+Ytk19AyIhyqmPwvslxv3ZuD+NPzf3V/C3PQO4K0FdR9FFqBvouJIjYKAINsBPQL8bXr+XuA3FfNcApxXZTtdAfxbGp5E7uyO7GzoI6QdZS/b+hbSTjs9Pzr9nUYArwaep+fZ1InArbltuSo3bee07F/VuOxDfdQ2A1hW+dlKw5/j5R35h+jlzK5inQuAs9PwkfQeEP8KXFvxOesEjszVkz9YuhC4uMrrVv3M5r6vR+WmHwm8QM8Wg/uA43PP24AHepl/DvB/SfuBwfhwH8S2W5MbfpDsaIX074PdEyLiGbJT7LEF67gCeKekXYB/INsJPVz0YhHxREScGxGvI/uS3wkskKQ0y+XA30kaQ3aEf19ELCtY1eXA+8h2BD+s5Y3mjCH3viP7tOe3wwTg66nTvPv0WWTvvWjZHh2RFetqIduxLc2t72dpfPdrfaJ7Wpo+npf/Di+JiFvImsMuAh6RdKmk3YreoKTdyb68n42I23KvdVjFa51EtsMtchUv9/W8D1gQEc+l5+8iO6B4UNKvJL25yjp6bC+23M4jgYdz9VxCdgTb7c+599/92rvWuGz+tZD0aknzJHVKeorsc9vjYocqLidrspknaZ2kCyWNTOs8TtJiSY+nGo6vcZ2w5XfsxVRz/jv259zwc2TvvUhvn9luayqWWR9Z319hPfTcHxTNP5fsrOUmSaslnVultrpxQGy78bnhfchO4Un/TuiekHb+ryQ7wukhIjqB35M1u5xM9oXqU0Q8Sta+3H0KTUQ8SNaW+f60rh9UWfZBsrbU44H5tbxezsPk3ncKp/x2WEN21Ds692iOiN+lZcdVLDuOniI3/ChZW/TrcuvaPSK6v+hrgC9WvNbOEXF1lff9XxHxRmAK2RnCrMp5JO1AtnO/NSIurXhfv6p4rV0j4qNVttPNQIukg8iC4qpcHUsiYjrZDnkBcG2VdfTYXmy5nZ8H9srVs1s6eOhLLctGxTIXpHEHRsRuZJ8x0YeI2BgRn4+IKWT9ZO8APiBpFHAD2Wf41RExGvhJbp2Vr1+p8jvW/Tnc4jtWg94+sy+9lcq31ls99NwfbDF/RDwdEZ+IiNeQ9aWcI+ltW1F7aRwQ2+7MdOnpnmQdvdek8VcDH5R0UPoiXAD8ISIeqLKeH5K1oR5ILztsSV+R9HpJIyS9gqxdeVVEPJab7QdknZmHk3UkVnMa2WnzswWvc6qkarX+GHidpL9PV2P8Ez2Poi8GZkt6XVrX7pLek1v2QEkz0rJnUv0IvPuo8NvAVyW9Kq1vrKS2NMu3gTMkHabMLpLenrZN5Xs6JM03kqzZ6n/JOkErfZGsXf7sivE/Al4r6WRJI9PjEEkHVKl9I3Ad2ZHinmSBgaQdJZ0kafc0z1NV6oAsOM5O73k0WXNk9/ofBm4C/kPSbpJ2kLSfpLdWWRfbuOwryDqLn5Q0loJwLSLp7yQdmK6oe4qsqfVFYEey9vj1wCZJx5FdwNDtL8Ar09lckWuBt0t6W/qbfoIs9H5XZf7e9PaZrdXVwGcltaTLyP+N7CyrkKR3SNo/BduTZE3G1T4HdeGA2HZXkX3RVpO1QZ4PEBE/J2sjvYHsKHA/so7lam4kO/q4MdcUUGTnNO+G9JoTyI4+8m4g2yH9olpTVarxvohorzJ5PFlneNFyjwLvIesLeYysff23uek3knXGzUtNEXeTdfbml70wLTsFaCf7YlfzabJT8cVpfT8HJqf1tZN1wn6T7OqTVWTNZkV2IwuUJ8hO/x8j23lXOpGsn+IJSc+kx0kR8TTZDmwm2ZHhn3m507Gaq8j6Da6LiE258ScDD6T3cwZZU1WRb5N9vu4ClpEdYW8i25lA1qezIy9fSXc9sHcv9eT1d9nPk1108SRZ0Nd65vlXad1PkXUU/wq4PG3PfyLb0T9B1gy3sHuhiLiXbKe7OjX99Gg2jIgOsrOYb5Cdab4TeGdEvFBjXfl1Vf3M9sP5ZJ/lu8iukLsjjatmEtln+RmyFoRvRcSt/XzNUnVfzWBbIR1hfziFwUCs7z6y09wBWd821nITWWfhPSW/zg5kfRAnDbYvx2CUjrIvjogJfc5sto18BjFISHoXWRvlLfWuBSAiji0rHCS1SRqdmt7+hazNeXEZr9XoJDVLOj41KY4FziM7gzQrXWP8b74hTtIvyZpaTk5t7kPdm8maXrqbN2ZERFd9Sxq0RNa0cw1ZZ/2Pydq2zUrnJiYzMyvkJiYzMys0ZJqY9tprr5g4cWK9yzAzayhLly59NCJaiqYNmYCYOHEi7e3Vrtg0M7Mikh6sNs1NTGZmVsgBYWZmhRwQZmZWyAFhZmaFHBBmZlZoyFzFZGY23CxY1sncRR2s29DFmNHNzGqbzIypRT85s3UcEGZmDWjBsk5mz19O18bsxr6dG7qYPX85wICFhJuYzMwa0NxFHS+FQ7eujZuZu6hjwF7DAWFm1oDWbSi+v2W18VvDAWFm1oDGjG7u1/it4YAwM2tAs9om0zyyqce45pFNzGqbPGCv4U5qM7MG1N0R7auYzMxsCzOmjh3QQKjkJiYzMyvkgDAzs0IOCDMzK+SAMDOzQg4IMzMr5IAwM7NCpQaEpGmSOiStknRuwfQzJC2XdKek2yRNyU2bnZbrkNRWZp1mZral0gJCUhNwEXAcMAU4MR8AyVURcWBEHARcCPxnWnYKMBN4HTAN+FZan5mZbSdlnkEcCqyKiNUR8QIwD5ienyEinso93QWINDwdmBcRz0fE/cCqtD4zM9tOyvyf1GOBNbnna4HDKmeSdCZwDrAjcFRu2cUVy27x3wUlnQ6cDrDPPvsMSNFmZpapeyd1RFwUEfsBnwY+289lL42I1ohobWlpKadAM7NhqsyA6ATG556PS+OqmQfM2MplzcxsgJUZEEuASZL2lbQjWafzwvwMkiblnr4d+FMaXgjMlDRK0r7AJOD2Ems1M7MKpfVBRMQmSWcBi4Am4LsRsULSHKA9IhYCZ0k6GtgIPAGckpZdIelaYCWwCTgzIjYXvpCZmZVCEdH3XA2gtbU12tvb612GmVlDkbQ0IlqLptW9k9rMzAYnB4SZmRVyQJiZWSEHhJmZFXJAmJlZoTJvtWFm1nAWLOtk7qIO1m3oYszoZma1TWbG1C3u9DMsOCDMzJIFyzqZPX85XRuz/3bVuaGL2fOXAwzLkHATk5lZMndRx0vh0K1r42bmLuqoU0X15YAwM0vWbejq1/ihzgFhZpaMGd3cr/FDnQPCzCyZ1TaZ5pE9f7yyeWQTs9om16mi+nIntZlZ0t0R7auYMg4IM7OcGVPHDttAqOQmJjMzK+SAMDOzQg4IMzMr5IAwM7NCDggzMyvkgDAzs0IOCDMzK+SAMDOzQg4IMzMr5IAwM7NCDggzMytUakBImiapQ9IqSecWTD9H0kpJd0n6haQJuWmbJd2ZHgvLrNPMzLZU2s36JDUBFwHHAGuBJZIWRsTK3GzLgNaIeE7SR4ELgfemaV0RcVBZ9ZmZWe/KPIM4FFgVEasj4gVgHjA9P0NE3BoRz6Wni4FxJdZjZmb9UGZAjAXW5J6vTeOqOQ34ae75TpLaJS2WNKNoAUmnp3na169fv+0Vm5nZSwbF70FIej/QCrw1N3pCRHRKeg1wi6TlEXFffrmIuBS4FKC1tTW2W8FmZsNAmWcQncD43PNxaVwPko4GPgOcEBHPd4+PiM7072rgl8DUEms1M7MKZQbEEmCSpH0l7QjMBHpcjSRpKnAJWTg8khu/h6RRaXgv4HAg37ltZmYlK62JKSI2SToLWAQ0Ad+NiBWS5gDtEbEQmAvsClwnCeChiDgBOAC4RNKLZCH25Yqrn8zMrGSKGBpN962trdHe3l7vMszMGoqkpRHRWjRtUHRSm9nQtWBZJ3MXdbBuQxdjRjczq20yM6b2dkGjDRYOCDMrzYJlncyev5yujZsB6NzQxez5ywEcEg3A92Iys9LMXdTxUjh069q4mbmLOupUkfWHA8LMSrNuQ1e/xtvg4oAws9KMGd3cr/E2uDggzKw0s9om0zyyqce45pFNzGqbXKeKrD/cSW1mpenuiPZVTI3JAWFmpZoxdawDoUG5icnMzAo5IMzMrJADwszMCjkgzMyskAPCzMwKOSDMzKyQA8LMzAo5IMzMrJADwszMCjkgzMyskAPCzMwKOSDMzKyQA8LMzAo5IMzMrJADwszMCjkgzMyskAPCzMwKlRoQkqZJ6pC0StK5BdPPkbRS0l2SfiFpQm7aKZL+lB6nlFmnmZltqbSAkNQEXAQcB0wBTpQ0pWK2ZUBrRLwBuB64MC27J3AecBhwKHCepD3KqtXMzLZU5hnEocCqiFgdES8A84Dp+Rki4taIeC49XQyMS8NtwM0R8XhEPAHcDEwrsVYzM6tQZkCMBdbknq9N46o5Dfhpf5aVdLqkdknt69ev38Zyzcwsb1B0Ukt6P9AKzO3PchFxaUS0RkRrS0tLOcWZmQ1TZQZEJzA+93xcGteDpKOBzwAnRMTz/VnWzMzKU2ZALAEmSdpX0o7ATGBhfgZJU4FLyMLhkdykRcCxkvZIndPHpnFmZradjChrxRGxSdJZZDv2JuC7EbFC0hygPSIWkjUp7QpcJwngoYg4ISIel/QFspABmBMRj5dVq5mZbUkRUe8aBkRra2u0t7fXuwwzs4YiaWlEtBZNGxSd1GZmNvg4IMzMrFCfAaHM+L7mMzOzoaXPgIisk+In26EWMzMbRGptYrpD0iGlVmJmZoNKrZe5HgacJOlB4FlAZCcXbyitMjMzq6taA6Kt1CrMzGzQqamJKSIeBEYD70yP0WmcmZkNUTUFhKSzgSuBV6XHFZI+VmZhZmZWX7U2MZ0GHBYRzwJI+grwe+AbZRVmZmb1VetVTAI2555vTuPMzGyIqvUM4nvAHyTdmJ7PAC4rpyQzMxsM+gwISTuQ/RzoL4G/TaM/GBHLSqzLzMzqrM+AiIgXJV0UEVOBO7ZDTWZmNgjU2gfxC0nvUvrRBjMzG/pqDYiPANcBz0t6StLTkp4qsS4zM6uzWvsgpkXEb7dDPWZmNkjUcjfXF4FvbodazMxsEHEfhJmZFepPH8S1uA/CzGzYqPU/yu0OnATsGxFzJO0D7F1eWWZmVm+1nkFcBLwJODE9fxr3S5iZDWk1/2BQRBwsaRlARDwhaccS6zIzszqr9Qxio6QmIAAktQAvllaVmZnVXa0B8V/AjcCrJH0RuA24oLSqzMys7mr9RbkrgU8BXwIeBmZExHV9LSdpmqQOSasknVsw/QhJd0jaJOndFdM2S7ozPRbW9nbMzGyg1NoHQUTcC9xb6/ypSeoi4BhgLbBE0sKIWJmb7SHgVOCTBavoioiDan09MzMbWDUHxFY4FFgVEasBJM0DpgMvBUREPJCmuT/DzGyQqbUPYmuMBdbknq9N42q1k6R2SYslzSiaQdLpaZ729evXb0utZmZWocyA2FYTIqIVeB/wNUn7Vc4QEZdGRGtEtLa0tGz/Cs3MhrAyA6ITGJ97Pi6Nq0lEdKZ/V5P9mt3UgSzOzMx6V2ZALAEmSdo3/ae6mUBNVyNJ2kPSqDS8F3A4ub4LMzMrX2kBERGbgLOARcA9wLURsULSHEknAEg6RNJa4D3AJZJWpMUPANol/RG4FfhyxdVPZmZWMkVEvWsYEK2trdHe3l7vMszMGoqkpam/dwuDuZPazMzqyAFhZmaFHBBmZlbIAWFmZoUcEGZmVsgBYWZmhcq8WZ+ZlWTBsk7mLupg3YYuxoxuZlbbZGZM7c+tzsz65oAwazALlnUye/5yujZuBqBzQxez5y8HcEjYgHITk1mDmbuo46Vw6Na1cTNzF3XUqSIbqhwQZg1m3Yaufo0321oOCLMGM2Z0c7/Gm20tB4RZg5nVNpnmkU09xjWPbGJW2+Q6VWRDlTupzRpMd0e0r2KysjkgzBrQjKljHQhWOjcxmZlZIQeEmZkVckCYmVkhB4SZmRVyQJiZWSEHhJmZFXJAmJlZIQeEmZkVckCYmVkhB4SZmRUqNSAkTZPUIWmVpHMLph8h6Q5JmyS9u2LaKZL+lB6nlFmnmZltqbSAkNQEXAQcB0wBTpQ0pWK2h4BTgasqlt0TOA84DDgUOE/SHmXVamZmWyrzDOJQYFVErI6IF4B5wPT8DBHxQETcBbxYsWwbcHNEPB4RTwA3A9NKrNXMzCqUGRBjgTW552vTuAFbVtLpktolta9fv36rCzUzsy01dCd1RFwaEa0R0drS0lLvcszMhpQyA6ITGJ97Pi6NK3tZMzMbAGX+YNASYJKkfcl27jOB99W47CLgglzH9LHA7IEv0SyzYFmnf6HNrEJpZxARsQk4i2xnfw9wbUSskDRH0gkAkg6RtBZ4D3CJpBVp2ceBL5CFzBJgThpnNuAWLOtk9vzldG7oIoDODV3Mnr+cBct80mrDmyKi3jUMiNbW1mhvb693GdaADv/yLXRu6Npi/NjRzfz23KPqUJHZ9iNpaUS0Fk1r6E5qs4GwriAcehtvNlw4IGzYGzO6uV/jzYYLB4QNe7PaJtM8sqnHuOaRTcxqm1yniswGhzKvYjJrCN1XK/kqJrOeHBBmZCHhQDDryU1MZmZWyAFhZmaFHBBmZlbIAWFmZoUcEGZmVsgBYWZmhRwQZmZWyAFhZmaFHBBmZlbIAWFmZoUcEGZmVsgBYWZmhRwQZmZWyAFhZmaFHBBmZlbIvwfRQBYs6/SP2pjZduOAaBALlnUye/5yujZuBqBzQxez5y8HcEiYWSncxNQg5i7qeCkcunVt3MzcRR11qsjMhjoHRINYt6GrX+PNzLaVA6JBjBnd3K/xZmbbqtSAkDRNUoekVZLOLZg+StI1afofJE1M4ydK6pJ0Z3pcXGadjWBW22SaRzb1GNc8solZbZPrVJGZDXWldVJLagIuAo4B1gJLJC2MiJW52U4DnoiI/SXNBL4CvDdNuy8iDiqrvkbT3RHtq5jMbHsp8yqmQ4FVEbEaQNI8YDqQD4jpwOfS8PXANyWpxJoa2oypYx0IZrbdlNnENBZYk3u+No0rnCciNgFPAq9M0/aVtEzSryS9pegFJJ0uqV1S+/r16we2ejOzYW6wdlI/DOwTEVOBc4CrJO1WOVNEXBoRrRHR2tLSst2LNDMbysoMiE5gfO75uDSucB5JI4Ddgcci4vmIeAwgIpYC9wGvLbFWMzOrUGZALAEmSdpX0o7ATGBhxTwLgVPS8LuBWyIiJLWkTm4kvQaYBKwusVYzM6tQWid1RGySdBawCGgCvhsRKyTNAdojYiFwGXC5pFXA42QhAnAEMEfSRuBF4IyIeLysWs3MbEuKiHrXMCBaW1ujvb293mVYjm8uaDb4SVoaEa1F03yzPiuFby5o1vgG61VM1uB8c0GzxueAsFL45oJmjc8BYaXwzQXNGp8DwkrhmwuaNT53UlspfHNBs8bngLDS+OaCZo1t2AeEr9U3Mys2rAPC1+qbmVU3rDupfa2+mVl1wzogfK2+mVl1wzogfK2+mVl1wzogfK2+mVl1w7qT2tfqm5lVN6wDAnytvplZNcO6icnMzKpzQJiZWSEHhJmZFXJAmJlZIQeEmZkVUkTUu4YBIWk98OA2rGIv4NEBKqdsjVQrNFa9jVQrNFa9jVQrNFa921LrhIhoKZowZAJiW0lqj4jWetdRi0aqFRqr3kaqFRqr3kaqFRqr3rJqdROTmZkVckCYmVkhB8TLLq13Af3QSLVCY9XbSLVCY9XbSLVCY9VbSq3ugzAzs0I+gzAzs0IOCDMzKzTsA0LSdyU9IunuetfSF0njJd0qaaWkFZLOrndN1UjaSdLtkv6Yav18vWvqi6QmScsk/ajetfRF0gOSlku6U1J7vevpi6TRkq6XdK+keyS9ud41FZE0OW3T7sdTkj5e77p6I+mf03fsbklXS9ppwNY93PsgJB0BPAP8MCJeX+96eiNpb2DviLhD0iuApcCMiFhZ59K2IEnALhHxjKSRwG3A2RGxuM6lVSXpHKAV2C0i3lHvenoj6QGgNSIa4j9ySfoB8JuI+I6kHYGdI2JDvevqjaQmoBM4LCK25T/hlkbSWLLv1pSI6JJ0LfCTiPj+QKx/2J9BRMSvgcfrXUctIuLhiLgjDT8N3AMMyh+ziMwz6enI9Bi0RyOSxgFvB75T71qGGkm7A0cAlwFExAuDPRyStwH3DdZwyBkBNEsaAewMrBuoFQ/7gGhUkiYCU4E/1LeS6lKTzZ3AI8DNETFoawW+BnwKeLHehdQogJskLZV0er2L6cO+wHrge6kJ7zuSdql3UTWYCVxd7yJ6ExGdwL8DDwEPA09GxE0DtX4HRAOStCtwA/DxiHiq3vVUExGbI+IgYBxwqKRB2YQn6R3AIxGxtN619MPfRsTBwHHAmampdLAaARwM/HdETAWeBc6tb0m9S81gJwDX1buW3kjaA5hOFsJjgF0kvX+g1u+AaDCpPf8G4MqImF/vemqRmhNuBabVu5YqDgdOSO3684CjJF1R35J6l44ciYhHgBuBQ+tbUa/WAmtzZ5DXkwXGYHYccEdE/KXehfThaOD+iFgfERuB+cDfDNTKHRANJHX8XgbcExH/We96eiOpRdLoNNwMHAPcW9+qikXE7IgYFxETyZoVbomIATsKG2iSdkkXKZCaao4FBu1VeBHxZ2CNpMlp1NuAQXdhRYUTGeTNS8lDwJsk7Zz2D28j65scEMM+ICRdDfwemCxpraTT6l1TLw4HTiY7wu2+DO/4ehdVxd7ArZLuApaQ9UEM+stHG8Srgdsk/RG4HfhxRPyszjX15WPAlenzcBBwQZ3rqSqF7jFkR+ODWjorux64A1hOtk8fsNtuDPvLXM3MrNiwP4MwM7NiDggzMyvkgDAzs0IOCDMzK+SAMDOzQg4Is36S9DlJn6x3HWZlc0CY1UG6sZrZoOaAMKuBpM9I+h9JtwGT07j9JP0s3TDvN5L+Ojd+cfq9hvMlPZPGH5nmWwisTDcznCtpiaS7JH0k93qzcuMH/W9p2NDkoxizPkh6I9ktOA4i+87cQfZbHJcCZ0TEnyQdBnwLOAr4OvD1iLha0hkVqzsYeH1E3J/uwvpkRBwiaRTwW0k3AZPS41BAwEJJR6Rb05ttNw4Is769BbgxIp4DSGcAO5HdFO267BY4AIxK/74ZmJGGryK7HXO32yPi/jR8LPAGSe9Oz3cnC4Zj02NZGr9rGu+AsO3KAWG2dXYANqTbmffHs7lhAR+LiEX5GSS1AV+KiEu2sUazbeI+CLO+/RqYIak53UX1ncBzwP2S3gPZnXYl/Z80/2LgXWl4Zi/rXQR8NN3CHUmvTc+VniIAAACwSURBVDeKWwR8KP3uB5LGSnrVgL8rsz44IMz6kH7m9Rrgj8BPye5OC3AScFq6q+oKsh9uAfg4cE66c+n+wJNVVv0dstte3yHpbuASYET6RbCrgN9LWk52t85XDPgbM+uD7+ZqNsAk7Qx0RURImgmcGBHT+1rObLBxH4TZwHsj8M30Ay4bgA/VuR6zreIzCDMzK+Q+CDMzK+SAMDOzQg4IMzMr5IAwM7NCDggzMyv0/wHjSa4ZXYwoMwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOTgKGG1bVwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}