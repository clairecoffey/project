{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "claire_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clairecoffey/project/blob/master/claire_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep76GcW0r5EF",
        "colab_type": "text"
      },
      "source": [
        "# Fairness and the Bias/Variance Tradeoff \n",
        "\n",
        "## Claire Coffey\n",
        "\n",
        "## May 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt8eZN7L71OB",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are studying bias and variance errors in the context of fairness, by exploring recidivism data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5j4K9fEtccc",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPz0FbDrdOB",
        "colab_type": "text"
      },
      "source": [
        "Imports: first import the relevant libraries used throughout. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3xUTfnrkM0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojFZOY-LtsdL",
        "colab_type": "text"
      },
      "source": [
        "# Read in recidivism data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLIFxjZ9roCK",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are studying recidivism data. We utilise the COMPAS recidivism dataset, which uses recidivism data from Broward County jail and has been explored in the following studies:\n",
        "\n",
        "\"The accuracy, fairness, and limits of predicting recidivism\", paper available at:\n",
        "https://advances.sciencemag.org/content/4/1/eaao5580#corresp-1\n",
        "\n",
        "\"Machine Bias\" ProPublica article, available at:\n",
        "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
        "\n",
        "The dataset used can be found at:\n",
        "https://github.com/propublica/compas-analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmO2E_MbHUGY",
        "colab_type": "text"
      },
      "source": [
        "Here we import and read in the recidivism data. Currently, we are using a selection of 500 samples from this dataset for our predictions.\n",
        "\n",
        "We use a selection \n",
        "of fields from this dataset to predict recidivism classification (0 = will not reoffend; 1 = will reoffend)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdCKnj1kqViI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_file():\n",
        "  full_data = False\n",
        "  print(\"loading data\")\n",
        "  if full_data:\n",
        "    # full dataset\n",
        "    file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/compas-scores-two-years%20-%20compas-scores-two-years.csv?token=ABPC6VNTFTGQBANNUJY2O4C6XGJGY\"\n",
        "  else:\n",
        "    # small subset of first 500/1000 people\n",
        "    # file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/500-compas-scores-two-years%20-%20Sheet1%20(1).csv?token=ABPC6VOW7CBEIIGZVE6ZJYS6YKNHO\"\n",
        "    file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/1000-compas-scores-two-years%20-%20Sheet1.csv\"\n",
        "\n",
        "  # load CSV contents\n",
        "  all_data = pd.read_csv(file_path, delimiter=',', dtype={'sex': 'category', \n",
        "                                                          'age_cat': 'category',\n",
        "                                                          'race': 'category',\n",
        "                                                          'c_charge_degree': 'category',\n",
        "                                                          'c_charge_desc': 'category',\n",
        "                                                          'r_charge_degree': 'category',\n",
        "                                                          'r_charge_desc': 'category',\n",
        "                                                          'vr_charge_degree': 'category',\n",
        "                                                          'vr_charge_desc': 'category'\n",
        "                                                          })\n",
        "  return all_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1WpLQEMZUNe",
        "colab_type": "code",
        "outputId": "1b47d370-65d0-455b-e6ca-df6b8b662bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        }
      },
      "source": [
        "all_data = load_file()\n",
        "all_data"
      ],
      "execution_count": 857,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>first</th>\n",
              "      <th>last</th>\n",
              "      <th>sex</th>\n",
              "      <th>dob</th>\n",
              "      <th>age</th>\n",
              "      <th>age_cat</th>\n",
              "      <th>race</th>\n",
              "      <th>juv_fel_count</th>\n",
              "      <th>juv_misd_count</th>\n",
              "      <th>juv_other_count</th>\n",
              "      <th>priors_count</th>\n",
              "      <th>days_b_screening_arrest</th>\n",
              "      <th>c_jail_in</th>\n",
              "      <th>c_jail_out</th>\n",
              "      <th>c_case_number</th>\n",
              "      <th>c_offense_date</th>\n",
              "      <th>c_arrest_date</th>\n",
              "      <th>c_charge_degree</th>\n",
              "      <th>c_charge_desc</th>\n",
              "      <th>is_recid</th>\n",
              "      <th>r_case_number</th>\n",
              "      <th>r_charge_degree</th>\n",
              "      <th>r_days_from_arrest</th>\n",
              "      <th>r_offense_date</th>\n",
              "      <th>r_charge_desc</th>\n",
              "      <th>r_jail_in</th>\n",
              "      <th>r_jail_out</th>\n",
              "      <th>violent_recid</th>\n",
              "      <th>is_violent_recid</th>\n",
              "      <th>vr_case_number</th>\n",
              "      <th>vr_charge_degree</th>\n",
              "      <th>vr_offense_date</th>\n",
              "      <th>vr_charge_desc</th>\n",
              "      <th>in_custody</th>\n",
              "      <th>out_custody</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>event</th>\n",
              "      <th>two_year_recid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>miguel hernandez</td>\n",
              "      <td>miguel</td>\n",
              "      <td>hernandez</td>\n",
              "      <td>Male</td>\n",
              "      <td>1947-04-18</td>\n",
              "      <td>69</td>\n",
              "      <td>Greater than 45</td>\n",
              "      <td>Other</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2013-08-13 06:03:42</td>\n",
              "      <td>2013-08-14 05:41:20</td>\n",
              "      <td>13011352CF10A</td>\n",
              "      <td>2013-08-13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>Aggravated Assault w/Firearm</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2014-07-07</td>\n",
              "      <td>2014-07-14</td>\n",
              "      <td>0</td>\n",
              "      <td>327</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>kevon dixon</td>\n",
              "      <td>kevon</td>\n",
              "      <td>dixon</td>\n",
              "      <td>Male</td>\n",
              "      <td>1982-01-22</td>\n",
              "      <td>34</td>\n",
              "      <td>25 - 45</td>\n",
              "      <td>African-American</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2013-01-26 03:45:27</td>\n",
              "      <td>2013-02-05 05:36:53</td>\n",
              "      <td>13001275CF10A</td>\n",
              "      <td>2013-01-26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>Felony Battery w/Prior Convict</td>\n",
              "      <td>1</td>\n",
              "      <td>13009779CF10A</td>\n",
              "      <td>(F3)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-07-05</td>\n",
              "      <td>Felony Battery (Dom Strang)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>13009779CF10A</td>\n",
              "      <td>(F3)</td>\n",
              "      <td>2013-07-05</td>\n",
              "      <td>Felony Battery (Dom Strang)</td>\n",
              "      <td>2013-01-26</td>\n",
              "      <td>2013-02-05</td>\n",
              "      <td>9</td>\n",
              "      <td>159</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>ed philo</td>\n",
              "      <td>ed</td>\n",
              "      <td>philo</td>\n",
              "      <td>Male</td>\n",
              "      <td>1991-05-14</td>\n",
              "      <td>24</td>\n",
              "      <td>Less than 25</td>\n",
              "      <td>African-American</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2013-04-13 04:58:34</td>\n",
              "      <td>2013-04-14 07:02:04</td>\n",
              "      <td>13005330CF10A</td>\n",
              "      <td>2013-04-13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>Possession of Cocaine</td>\n",
              "      <td>1</td>\n",
              "      <td>13011511MM10A</td>\n",
              "      <td>(M1)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2013-06-16</td>\n",
              "      <td>Driving Under The Influence</td>\n",
              "      <td>2013-06-16</td>\n",
              "      <td>2013-06-16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-06-16</td>\n",
              "      <td>2013-06-16</td>\n",
              "      <td>0</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>marcu brown</td>\n",
              "      <td>marcu</td>\n",
              "      <td>brown</td>\n",
              "      <td>Male</td>\n",
              "      <td>1993-01-21</td>\n",
              "      <td>23</td>\n",
              "      <td>Less than 25</td>\n",
              "      <td>African-American</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13000570CF10A</td>\n",
              "      <td>2013-01-12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>Possession of Cannabis</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1174</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>bouthy pierrelouis</td>\n",
              "      <td>bouthy</td>\n",
              "      <td>pierrelouis</td>\n",
              "      <td>Male</td>\n",
              "      <td>1973-01-22</td>\n",
              "      <td>43</td>\n",
              "      <td>25 - 45</td>\n",
              "      <td>Other</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12014130CF10A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-01-09</td>\n",
              "      <td>F</td>\n",
              "      <td>arrest case no charge</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1102</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1531</td>\n",
              "      <td>jeremy torres</td>\n",
              "      <td>jeremy</td>\n",
              "      <td>torres</td>\n",
              "      <td>Male</td>\n",
              "      <td>1995-03-29</td>\n",
              "      <td>21</td>\n",
              "      <td>Less than 25</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>2013-11-14 09:10:59</td>\n",
              "      <td>2013-11-16 08:34:03</td>\n",
              "      <td>13013996CF10A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-11-14</td>\n",
              "      <td>F</td>\n",
              "      <td>arrest case no charge</td>\n",
              "      <td>1</td>\n",
              "      <td>15026209TC40A</td>\n",
              "      <td>(M2)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-04-30</td>\n",
              "      <td>Unlaw LicTag/Sticker Attach</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-11-14</td>\n",
              "      <td>2013-11-16</td>\n",
              "      <td>0</td>\n",
              "      <td>528</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>1532</td>\n",
              "      <td>leonardo collazos</td>\n",
              "      <td>leonardo</td>\n",
              "      <td>collazos</td>\n",
              "      <td>Male</td>\n",
              "      <td>1956-07-21</td>\n",
              "      <td>59</td>\n",
              "      <td>Greater than 45</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2013-04-26 10:39:22</td>\n",
              "      <td>2013-04-27 01:07:17</td>\n",
              "      <td>92078761TC20A</td>\n",
              "      <td>1992-07-02</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M</td>\n",
              "      <td>License Suspended Revoked</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-04-26</td>\n",
              "      <td>2013-04-27</td>\n",
              "      <td>0</td>\n",
              "      <td>1070</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1533</td>\n",
              "      <td>isaac smith</td>\n",
              "      <td>isaac</td>\n",
              "      <td>smith</td>\n",
              "      <td>Male</td>\n",
              "      <td>1976-05-09</td>\n",
              "      <td>39</td>\n",
              "      <td>25 - 45</td>\n",
              "      <td>African-American</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-9.0</td>\n",
              "      <td>2014-01-05 09:54:57</td>\n",
              "      <td>2014-01-10 07:58:25</td>\n",
              "      <td>12016467CF10A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2014-01-05</td>\n",
              "      <td>F</td>\n",
              "      <td>arrest case no charge</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2014-01-05</td>\n",
              "      <td>2014-01-10</td>\n",
              "      <td>0</td>\n",
              "      <td>808</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1534</td>\n",
              "      <td>trevor parker</td>\n",
              "      <td>trevor</td>\n",
              "      <td>parker</td>\n",
              "      <td>Male</td>\n",
              "      <td>1986-04-23</td>\n",
              "      <td>29</td>\n",
              "      <td>25 - 45</td>\n",
              "      <td>African-American</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2013-05-05 10:38:59</td>\n",
              "      <td>2013-06-12 05:40:10</td>\n",
              "      <td>13006444CF10A</td>\n",
              "      <td>2013-05-05</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>Burglary Conveyance Armed</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-05-05</td>\n",
              "      <td>2013-06-12</td>\n",
              "      <td>37</td>\n",
              "      <td>1061</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1535</td>\n",
              "      <td>eleasar gutierrez</td>\n",
              "      <td>eleasar</td>\n",
              "      <td>gutierrez</td>\n",
              "      <td>Male</td>\n",
              "      <td>1993-10-16</td>\n",
              "      <td>22</td>\n",
              "      <td>Less than 25</td>\n",
              "      <td>Hispanic</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2013-03-02 12:57:44</td>\n",
              "      <td>2013-03-04 05:34:09</td>\n",
              "      <td>13004236MM10A</td>\n",
              "      <td>2013-03-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M</td>\n",
              "      <td>Battery</td>\n",
              "      <td>1</td>\n",
              "      <td>13013833MM10A</td>\n",
              "      <td>(M2)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2013-07-21</td>\n",
              "      <td>Unlaw LicTag/Sticker Attach</td>\n",
              "      <td>2013-07-21</td>\n",
              "      <td>2013-07-21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-07-21</td>\n",
              "      <td>2013-07-21</td>\n",
              "      <td>2</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 41 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                name     first  ...   end event two_year_recid\n",
              "0       1    miguel hernandez    miguel  ...   327     0              0\n",
              "1       3         kevon dixon     kevon  ...   159     1              1\n",
              "2       4            ed philo        ed  ...    63     0              1\n",
              "3       5         marcu brown     marcu  ...  1174     0              0\n",
              "4       6  bouthy pierrelouis    bouthy  ...  1102     0              0\n",
              "..    ...                 ...       ...  ...   ...   ...            ...\n",
              "995  1531       jeremy torres    jeremy  ...   528     1              1\n",
              "996  1532   leonardo collazos  leonardo  ...  1070     0              0\n",
              "997  1533         isaac smith     isaac  ...   808     0              0\n",
              "998  1534       trevor parker    trevor  ...  1061     0              0\n",
              "999  1535   eleasar gutierrez   eleasar  ...   141     0              1\n",
              "\n",
              "[1000 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 857
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19AV7fOG0z4h",
        "colab_type": "text"
      },
      "source": [
        "## Import and process data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB8DQVf7g1Nn",
        "colab_type": "text"
      },
      "source": [
        "We import the data into a pandas DataFrame. We begin by cleaning the data, so the crime descriptions are simplified, removing duplicate catefories. Then,  the categorical data is  split into different fields for each category, and encoded as 0 or 1. For example, an individual with characteristic \"sex: male\" would be encoded as \"male: 1, female: 0\". The sex category is then removed. \n",
        "\n",
        "We then consider which fields to use for prediction. This includes the removal of any fields/columns which contain many NaN values, since these cannot be handled by the classifiers. We choose to remove the columns with many NaNs rather than using an alternative approach such as replacing them with the average so as not to introduce other types of bias. We also then remove rows/individuals containing any further NaN values so there is no longer any NaN values present in the data. \n",
        "\n",
        "We then normalise all of the data in the dataframe, so that when fed into the classifier, the predicitons are not skewed (and potentially different forms of bias introduced).  We do this by using the StandardScaler in the sklearn preprocessing library, and we normalise the data to have a variance of 1.\n",
        "\n",
        "Finally, we define the number of testing/training samples desired and split the data into these two sets appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzJytmnMtkPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def clean_descriptions(description):\n",
        "  description = description.replace(' and ', ' ')\n",
        "  description = description.replace(' / ', ' ')\n",
        "  description = description.replace('possession', 'posess')\n",
        "  description = description.replace('possessing', 'posess')\n",
        "  description = description.replace('with', 'w/')\n",
        "  description = description.replace('w/ ', 'w/')\n",
        "  description = description.replace('attempted', 'att')\n",
        "  description = description.replace('attempt', 'att')\n",
        "  description = description.replace('aggravated', 'agg')\n",
        "  description = description.replace('aggrav', 'agg') \n",
        "  return description\n",
        "\n",
        "def import_data(all_data):\n",
        "\n",
        "  split_by_sex = False\n",
        "  num_testing_samples = 201\n",
        "\n",
        "  encoded_sex = (pd.get_dummies(all_data['sex']))\n",
        "  all_data = all_data.drop(columns=['sex'])\n",
        "  all_data = all_data.join(encoded_sex)\n",
        "\n",
        "  encoded_age_cat = (pd.get_dummies(all_data['age_cat']))\n",
        "  all_data = all_data.drop(columns=['age_cat'])\n",
        "  all_data = all_data.join(encoded_age_cat)\n",
        "\n",
        "  encoded_race = (pd.get_dummies(all_data['race']))\n",
        "  all_data = all_data.drop(columns=['race'])\n",
        "  all_data = all_data.join(encoded_race)\n",
        "\n",
        "  encoded_c_charge_degree = (pd.get_dummies(all_data['c_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['c_charge_degree'])\n",
        "  all_data = all_data.join(encoded_c_charge_degree, rsuffix='_c')\n",
        "\n",
        "  #these are joined with suffixes because otherwise columns overlap \n",
        "  all_data['c_charge_desc'] = all_data['c_charge_desc'].astype(str).str.lower()\n",
        "  all_data['c_charge_desc'] = all_data['c_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_c_charge_desc = (pd.get_dummies(all_data['c_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['c_charge_desc'])\n",
        "  all_data = all_data.join(encoded_c_charge_desc, rsuffix='_c')\n",
        "\n",
        "  encoded_r_charge_degree = (pd.get_dummies(all_data['r_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['r_charge_degree'])\n",
        "  all_data = all_data.join(encoded_r_charge_degree, rsuffix='_r')\n",
        "\n",
        "  all_data['r_charge_desc'] = all_data['r_charge_desc'].astype(str).str.lower()\n",
        "  all_data['r_charge_desc'] = all_data['r_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_r_charge_desc = (pd.get_dummies(all_data['r_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['r_charge_desc'])\n",
        "  all_data = all_data.join(encoded_r_charge_desc, rsuffix='_r')\n",
        "\n",
        "  encoded_vr_charge_degree = (pd.get_dummies(all_data['vr_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['vr_charge_degree'])\n",
        "  all_data = all_data.join(encoded_vr_charge_degree, rsuffix='_vr')\n",
        "\n",
        "  all_data['vr_charge_desc'] = all_data['vr_charge_desc'].astype(str).str.lower()\n",
        "  all_data['vr_charge_desc'] = all_data['vr_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_vr_charge_desc = (pd.get_dummies(all_data['vr_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['vr_charge_desc'])\n",
        "  all_data = all_data.join(encoded_vr_charge_desc, rsuffix='_vr')\n",
        "\n",
        "  all_data = all_data.drop(columns=['nan'])\n",
        "  all_data = all_data.drop(columns=['nan_vr'])\n",
        "  all_data = all_data.drop(columns=['nan_r'])\n",
        "\n",
        "  #drop columns not used for predictions, including info such as names, and coluns with many NaN values \n",
        "  all_data_simplified = all_data.drop(columns=['two_year_recid', 'r_days_from_arrest', 'id','name','first','last','dob','days_b_screening_arrest','c_jail_in','c_jail_out','c_case_number','c_offense_date','c_arrest_date','r_case_number','r_offense_date','r_jail_in','r_jail_out','vr_case_number','vr_offense_date','in_custody','out_custody','start','end','violent_recid', 'age'])\n",
        "\n",
        "  #drop demographic info such as age, gender, race; keep only criminal records for predictions\n",
        "  # all_data_simplified = all_data.drop(columns=['two_year_recid', 'r_days_from_arrest', 'id','name','first','last','dob','days_b_screening_arrest','c_jail_in','c_jail_out','c_case_number','c_offense_date','c_arrest_date','r_case_number','r_offense_date','r_jail_in','r_jail_out','vr_case_number','vr_offense_date','in_custody','out_custody','start','end','violent_recid', 'age','Female','Male',\t'25 - 45',\t'Greater than 45',\t'Less than 25',\t'African-American',\t'Caucasian',\t'Hispanic',\t'Other',\t'F',\t'M'])\n",
        "\n",
        "  #remove rows containing NaN values \n",
        "  all_data_simplified = all_data_simplified.dropna()\n",
        "\n",
        "  #Renormalise the data so we have unit variance and mean 0 using built-in preprocessing method in sklearn\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  all_data_scaled = pd.DataFrame(scaler.fit_transform(all_data_simplified),columns=all_data_simplified.columns)\n",
        "\n",
        "  all_data_and_labels = all_data_scaled.join(all_data[['two_year_recid']])\n",
        "  all_data_and_labels.columns = map(str.lower, all_data_and_labels.columns)\n",
        "\n",
        "  #split into training and testing with specific number of testing samples\n",
        "  #for now just set testing set to be first num_testing_samples samples in table \n",
        "  testing_data_and_labels = all_data_and_labels[:num_testing_samples]\n",
        "  #and training set to be the remainder\n",
        "  #this is also then consistent which is good for seeing patterns etc \n",
        "  training_data_and_labels = all_data_and_labels[num_testing_samples:]\n",
        "\n",
        "  if(split_by_sex):\n",
        "    testing_data_and_labels = pd.DataFrame.reset_index(testing_data_and_labels[testing_data_and_labels['Female']>0], drop=True)\n",
        "\n",
        "  # print(\"normalised\")\n",
        "  # print(training_data_and_labels)\n",
        "\n",
        "  # print(testing_data_and_labels)\n",
        "  return training_data_and_labels, testing_data_and_labels\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Vx5Lghqe2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "9da7205a-902a-4e6b-982c-6534b33b6fe6"
      },
      "source": [
        "training_data_and_labels, testing_data_and_labels = import_data(all_data)\n",
        "training_data_and_labels"
      ],
      "execution_count": 859,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>juv_fel_count</th>\n",
              "      <th>juv_misd_count</th>\n",
              "      <th>juv_other_count</th>\n",
              "      <th>priors_count</th>\n",
              "      <th>is_recid</th>\n",
              "      <th>is_violent_recid</th>\n",
              "      <th>event</th>\n",
              "      <th>female</th>\n",
              "      <th>male</th>\n",
              "      <th>25 - 45</th>\n",
              "      <th>greater than 45</th>\n",
              "      <th>less than 25</th>\n",
              "      <th>african-american</th>\n",
              "      <th>asian</th>\n",
              "      <th>caucasian</th>\n",
              "      <th>hispanic</th>\n",
              "      <th>native american</th>\n",
              "      <th>other</th>\n",
              "      <th>f</th>\n",
              "      <th>m</th>\n",
              "      <th>agg assault</th>\n",
              "      <th>agg assault w/dead weap</th>\n",
              "      <th>agg assault w/firearm</th>\n",
              "      <th>agg battery</th>\n",
              "      <th>agg battery (firearm/actual posess)</th>\n",
              "      <th>agg battery grt/bod/harm</th>\n",
              "      <th>agg battery pregnant</th>\n",
              "      <th>agg battery w/deadly weapon</th>\n",
              "      <th>agg fleeing eluding</th>\n",
              "      <th>agg fleeing/eluding high speed</th>\n",
              "      <th>agg stalking after injunctn</th>\n",
              "      <th>arrest case no charge</th>\n",
              "      <th>arson ii (vehicle)</th>\n",
              "      <th>assault</th>\n",
              "      <th>att armed burglary dwell</th>\n",
              "      <th>att burg/struct/unocc</th>\n",
              "      <th>att burgl conv occp</th>\n",
              "      <th>att burgl unoccupied dwel</th>\n",
              "      <th>att robbery  no weapon</th>\n",
              "      <th>att robbery firearm</th>\n",
              "      <th>...</th>\n",
              "      <th>(m1)_vr</th>\n",
              "      <th>(m2)_vr</th>\n",
              "      <th>(mo3)_vr</th>\n",
              "      <th>agg assault law enforc officer</th>\n",
              "      <th>agg assault w/dead weap_vr</th>\n",
              "      <th>agg assault w/firearm_vr</th>\n",
              "      <th>agg battery_vr</th>\n",
              "      <th>agg battery grt/bod/harm_vr</th>\n",
              "      <th>agg battery law enforc officer</th>\n",
              "      <th>agg battery pregnant_vr</th>\n",
              "      <th>agg battery w/deadly weapon_vr</th>\n",
              "      <th>agg flee/eluding (injury/prop damage)</th>\n",
              "      <th>agg fleeing/eluding high speed_vr</th>\n",
              "      <th>assault_vr</th>\n",
              "      <th>att murder in the first degree_vr</th>\n",
              "      <th>battery_vr</th>\n",
              "      <th>battery on a person over 65_vr</th>\n",
              "      <th>battery on law enforc officer_vr</th>\n",
              "      <th>burglary dwelling assault/batt_vr</th>\n",
              "      <th>burglary w/assault/battery_vr</th>\n",
              "      <th>carjacking</th>\n",
              "      <th>child abuse_vr</th>\n",
              "      <th>doc/fighting/threatening words_vr</th>\n",
              "      <th>felony battery_vr</th>\n",
              "      <th>felony battery (dom strang)_vr</th>\n",
              "      <th>felony battery w/prior convict_vr</th>\n",
              "      <th>kidnapping (facilitate felony)</th>\n",
              "      <th>manslaughter w/weapon</th>\n",
              "      <th>murder in the first degree_vr</th>\n",
              "      <th>robbery no weapon_vr</th>\n",
              "      <th>robbery sudd snatch no weapon_vr</th>\n",
              "      <th>robbery sudd snatch w/weapon_vr</th>\n",
              "      <th>robbery w/firearm_vr</th>\n",
              "      <th>robbery weapon_vr</th>\n",
              "      <th>stalking (agg)_vr</th>\n",
              "      <th>strong armed  robbery_vr</th>\n",
              "      <th>threat public servant_vr</th>\n",
              "      <th>threaten throw destruct device_vr</th>\n",
              "      <th>throw deadly missile into veh_vr</th>\n",
              "      <th>two_year_recid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>-0.700393</td>\n",
              "      <td>-0.949284</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>-0.811403</td>\n",
              "      <td>2.071474</td>\n",
              "      <td>-2.071474</td>\n",
              "      <td>0.904534</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>-0.548079</td>\n",
              "      <td>-1.010051</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>1.415275</td>\n",
              "      <td>-0.323994</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>-1.371803</td>\n",
              "      <td>1.371803</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.442913</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>0.098458</td>\n",
              "      <td>-0.949284</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>-0.811403</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.482748</td>\n",
              "      <td>0.904534</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>-0.548079</td>\n",
              "      <td>-1.010051</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>1.415275</td>\n",
              "      <td>-0.323994</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>-1.371803</td>\n",
              "      <td>1.371803</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.442913</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>1.703553</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>3.893002</td>\n",
              "      <td>1.053425</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>1.232433</td>\n",
              "      <td>2.071474</td>\n",
              "      <td>-2.071474</td>\n",
              "      <td>0.904534</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>-0.548079</td>\n",
              "      <td>0.990050</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.706577</td>\n",
              "      <td>-0.323994</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>0.728967</td>\n",
              "      <td>-0.728967</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.442913</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>0.298171</td>\n",
              "      <td>1.053425</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>1.232433</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.482748</td>\n",
              "      <td>-1.105542</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>1.824556</td>\n",
              "      <td>-1.010051</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.706577</td>\n",
              "      <td>-0.323994</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>3.923448</td>\n",
              "      <td>0.728967</td>\n",
              "      <td>-0.728967</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>2.257778</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>-0.300967</td>\n",
              "      <td>1.053425</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>1.232433</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.482748</td>\n",
              "      <td>0.904534</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>-0.548079</td>\n",
              "      <td>0.990050</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.706577</td>\n",
              "      <td>-0.323994</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>0.728967</td>\n",
              "      <td>-0.728967</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.442913</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>2.133524</td>\n",
              "      <td>-0.500680</td>\n",
              "      <td>1.053425</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>1.232433</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.482748</td>\n",
              "      <td>-1.105542</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>1.824556</td>\n",
              "      <td>-1.010051</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.706577</td>\n",
              "      <td>3.086473</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>0.728967</td>\n",
              "      <td>-0.728967</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>2.257778</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>-0.500680</td>\n",
              "      <td>-0.949284</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>-0.811403</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.482748</td>\n",
              "      <td>-1.105542</td>\n",
              "      <td>1.888441</td>\n",
              "      <td>-0.548079</td>\n",
              "      <td>-1.010051</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.706577</td>\n",
              "      <td>3.086473</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>-1.371803</td>\n",
              "      <td>1.371803</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.442913</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>-0.500680</td>\n",
              "      <td>-0.949284</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>-0.811403</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.482748</td>\n",
              "      <td>0.904534</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>-0.548079</td>\n",
              "      <td>0.990050</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.706577</td>\n",
              "      <td>-0.323994</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>0.728967</td>\n",
              "      <td>-0.728967</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>2.257778</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>-0.300967</td>\n",
              "      <td>-0.949284</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>-0.811403</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.482748</td>\n",
              "      <td>0.904534</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>-0.548079</td>\n",
              "      <td>0.990050</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.706577</td>\n",
              "      <td>-0.323994</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>0.728967</td>\n",
              "      <td>-0.728967</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.442913</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-0.133977</td>\n",
              "      <td>-0.170543</td>\n",
              "      <td>-0.250302</td>\n",
              "      <td>-0.500680</td>\n",
              "      <td>1.053425</td>\n",
              "      <td>-0.386556</td>\n",
              "      <td>-0.811403</td>\n",
              "      <td>-0.482748</td>\n",
              "      <td>0.482748</td>\n",
              "      <td>-1.105542</td>\n",
              "      <td>-0.529537</td>\n",
              "      <td>1.824556</td>\n",
              "      <td>-1.010051</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.706577</td>\n",
              "      <td>3.086473</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>-1.371803</td>\n",
              "      <td>1.371803</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.114766</td>\n",
              "      <td>-0.095298</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.08396</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.442913</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.231821</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.077693</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.226991</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.070888</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.044766</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.100504</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.054855</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.063372</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>-0.031639</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>799 rows × 366 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     juv_fel_count  ...  two_year_recid\n",
              "201      -0.133977  ...               0\n",
              "202      -0.133977  ...               0\n",
              "203      -0.133977  ...               1\n",
              "204      -0.133977  ...               1\n",
              "205      -0.133977  ...               1\n",
              "..             ...  ...             ...\n",
              "995      -0.133977  ...               1\n",
              "996      -0.133977  ...               0\n",
              "997      -0.133977  ...               0\n",
              "998      -0.133977  ...               0\n",
              "999      -0.133977  ...               1\n",
              "\n",
              "[799 rows x 366 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 859
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5FvDlXt4se",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmKSjOeK4aRj",
        "colab_type": "text"
      },
      "source": [
        "##Selecting Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs36jQoqYmpp",
        "colab_type": "text"
      },
      "source": [
        "Here we select the classification model to use. We are using a selection of built-in classifiers in scikit-learn. \n",
        "\n",
        "Currently, we are using RBF SVM models (https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html). \n",
        "\n",
        "We define the boolean values ```vary_gamma ``` and  ```vary_c``` to define whether we are varying the gamma or C value in the classifiers. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZv_kuCWYo_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, neighbors, svm, gaussian_process, tree, ensemble, neural_network, metrics\n",
        "\n",
        "def define_classifiers():\n",
        "\n",
        "  vary_gamma = True\n",
        "  vary_c = False \n",
        "  gammas = []\n",
        "  cs = []\n",
        "  classifiers = []\n",
        "\n",
        "  if vary_gamma:\n",
        "    # gammas = [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100, 1000, 10000]\n",
        "    gammas = [0.001]\n",
        "    # gammas = [1]\n",
        "    c_val = 1000\n",
        "    #fix size of C if varying gamma\n",
        "    for gamma_val in gammas:\n",
        "      classifiers.append(svm.SVC(gamma=gamma_val,C=c_val))\n",
        "\n",
        "  if vary_c:\n",
        "    cs = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 10000000]\n",
        "    gamma_val = 1\n",
        "    #fix size of gamma if varying C\n",
        "    for c_val in cs:\n",
        "      classifiers.append(svm.SVC(gamma=gamma_val,C=c_val))\n",
        "\n",
        "  return classifiers, gammas, cs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQD8oVE-a4-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define_classifiers()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7PUBJeb4SQh",
        "colab_type": "text"
      },
      "source": [
        "## Bootstrapping "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxB1XKpC4fYi",
        "colab_type": "text"
      },
      "source": [
        "The classification process then uses a bootstrapping procedure with the chosen model, to generate predictions of recidivism classifications (1 = will reoffend; 0 = will not reoffend).\n",
        "\n",
        "Bootstrapping (https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_41) is a sampling with replacement procedure. Here, the sample size is the same as the size of the (training) dataset. The bootstrapping procedure is run many times to generate different training datasets, which will then be used for classification. In turn, the classification results will be used to calculate and study the bias and variance errors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzZ6oTWgv3FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_bootstrap(training_data_and_labels):\n",
        "  # this is one bootstrap sample \n",
        "  indices = np.random.randint(0,training_data_and_labels.shape[0] , training_data_and_labels.shape[0])\n",
        "  indices.sort()\n",
        "  data_points = []\n",
        "\n",
        "  for i in indices:\n",
        "    data_points.append(training_data_and_labels.iloc[i])\n",
        "\n",
        "  b_sample = pd.DataFrame(data_points)\n",
        "\n",
        "  return b_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RPB3I-bwLF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b_sample = do_bootstrap(training_data_and_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yArDHhIdCfIS",
        "colab_type": "text"
      },
      "source": [
        "### Calculate average prediction for each individual over all bootstrap samples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjQeJu-rCpkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_avg_prediction(predictions):\n",
        "  #each row is bootstrap sample, each column an individual\n",
        "  return majority_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0p40u-y5sf2",
        "colab_type": "text"
      },
      "source": [
        "## Perform classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDUs2m5A6yP3",
        "colab_type": "text"
      },
      "source": [
        "Fit the model on the training data (which is one bootstrap data sample as defined above)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaofDUc-P8Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(clf, b_sample, testing_data_and_labels):\n",
        "\n",
        "    #training data is everything apart from two year recid 0/1 label from the bootstrap sample\n",
        "    X_train = b_sample.drop(columns=['two_year_recid'])\n",
        "    y_train = b_sample['two_year_recid']\n",
        "    test_aa = testing_data_and_labels.loc[testing_data_and_labels['african-american'] > 0]\n",
        "    X_test = test_aa.drop(columns=['two_year_recid'])\n",
        "    y_test = test_aa['two_year_recid']\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_true = y_test\n",
        "\n",
        "    return y_pred, y_true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8e8STm77eG",
        "colab_type": "text"
      },
      "source": [
        "Perform classification for each bootstrap sample separately, and store these in a DataFrame, to be passed into the bias/variance calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm8R-Gt-OmNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(training_data_and_labels, testing_data_and_labels, clf):\n",
        "    count = 0\n",
        "\n",
        "    num_bootstraps = len(training_data_and_labels);\n",
        "    while count <= num_bootstraps:\n",
        "      b_sample = do_bootstrap(training_data_and_labels)\n",
        "      y_pred, y_true = fit_model(clf, b_sample, testing_data_and_labels)\n",
        "      if(count == 0):\n",
        "        predictions = pd.DataFrame(pd.Series(y_pred)).transpose()\n",
        "        #true labels are the same for every sample so we only need 1 row in df\n",
        "        true_labels = pd.DataFrame(pd.Series(y_true)).transpose()\n",
        "      else:\n",
        "        predictions = predictions.append(pd.DataFrame(pd.Series(y_pred)).transpose())\n",
        "      count += 1\n",
        "      \n",
        "    return predictions, true_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H75T2WV0OqIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classify(training_data_and_labels, testing_data_and_labels, clf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMfhfngzvTX",
        "colab_type": "text"
      },
      "source": [
        "## Correcting for Fairness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBQkNPgD4D6o",
        "colab_type": "text"
      },
      "source": [
        "#Fairness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frcBqdbB3M22",
        "colab_type": "text"
      },
      "source": [
        "The definition of fairness is disputed, and there is not a single correct approach to ensuring fairness in machine learning. In general, as stated in https://arxiv.org/pdf/1711.08513.pdf, fairness in machine learning can be approached in two ways: fairness of the dataset itself; fairness of the model.\n",
        "\n",
        "Since we cannot control the process by which the data is collected, and the recidivism dataset already exists (likely with human and societal biases built-in), we will not be focusing on the former category. Although, there have been recent trends within the fairness and machine learning communities to argue the importance of the fairness in data collection, for example, in http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, the authors discuss the necessity of correcting for bias in the dataset, an approach which may actually increase the accuracy of the predictions, in contrast to approaches that exclusively focus on correcting for fairness in the models, at the expense of accuracy. Another area in which recent trends in fairness research have addressed is the importance of developing context-aware fairness measurements (https://arxiv.org/pdf/1805.05859.pdf). However, in our project we will focus on model-based fairness correction - ensuring the machine learning models are not perpetuating existing biases, or introducing new biases. We do this by using a widely used and accepted fairness measurement which is context-independent, known as **Equalised Odds** (http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf). This approach is not without criticism, however it provides a clear and well-motivated approach to achieving fair predictions across subgroups with different protected characteristics. We attempt to correct for fairness in relation to the protected characteristics found in the recidivism dataset (sex, race, age). Once our models are 'fair' in relation to this description, we can explore the relationship between bias and variance errors and the potential discovery of discrimination against new categories. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vab-_uQEx46Q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Equalised Odds\n",
        "\n",
        "As stated above, we are considering fairness in relation to the equalised odds metric (http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf). The definition as stated in this paper is as follows: \\\\\n",
        "We say that a predictor $\\hat{Y}$ satisfies equalized odds with respect to\n",
        "protected attribute $A$ and outcome $Y$, if $\\hat{Y}$ and $A$ are independent conditional on $Y$. Therefore, if the classification labels are $Y$ and $\\hat{Y}$, for an outcome $ y=1 $, $\\hat{Y}$ has equal true positive rates across all demographic groups, for example, the categories not female and female will have equal true positive rates. For an outcome  $ y=0 $, $\\hat{Y}$ has equal false positive rates across all demographic groups. This enforces equal bias and accuracy in all demographics. This can formally be stated as:\n",
        "$$ Pr \\left\\{ \\hat{Y}=1 | A = 0, Y = y \\right\\} = Pr \\left\\{ \\hat{Y}=1 | A = 0, Y = y \\right\\} , y \\in \\left\\{ 0,1 \\right\\}$$\n",
        "\n",
        "This approach punishes models that only perform well on the majority demographics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5hobrODGwef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#this method doesn't actually make anything fair, it just calculates what we need in order to configure the models to be fair \n",
        "def make_fair(y_true, y_pred):\n",
        "  # get confusion matrix and compute tn,fp,fn,tp\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true.iloc[0].to_numpy(), y_pred.iloc[0].to_numpy()).ravel()\n",
        "  print(\"true negatives:\", tn, \"false positives:\", fp,\"false negatives:\", fn, \"true positives:\",tp)\n",
        "  #reconfigure model based on these results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnKB0sC3uU6-",
        "colab_type": "text"
      },
      "source": [
        "# Compute bias/variance errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTcfbjcOMgYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdwnNxEnWqth",
        "colab_type": "text"
      },
      "source": [
        "Using all these bootstrap predictions, we calculate the average misclassification error. This is done by first calculating the overall misclassification loss across bootstrap sample predictions, finding the average misclassification error for each datapoint (individual). As described in:\n",
        "http://www.cems.uwe.ac.uk/~irjohnso/coursenotes/uqc832/tr-bias.pdf\n",
        "\n",
        "We can then decompose the error into the errors due to bias, and the errors due to variance, in order to study the behaviour of the model and the bias/variance tradeoff. This decomposition for classification is described in: \n",
        "https://homes.cs.washington.edu/~pedrod/bvd.pdf\n",
        "https://pdfs.semanticscholar.org/9253/f3e13bca7e845e60394d85ddaec0d4cfc6d6.pdf https://www.stat.berkeley.edu/users/breiman/arcall96.pdf. \n",
        "\n",
        "The error is also comprised of an error due to noise (in addition to bias and variance). However, as stated in http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, the noise is dependent on the data, not the model, so comparing the discrimination level in the form of bias and variance errors, the noise terms cancel since they are independent of the model. Therefore, differences in bias can be explored even without knowing the underlying noise of the data. \n",
        "\n",
        "We calculate the bias and variance errors for each individual, following zero-one loss rules under misclassification loss, as described in https://homes.cs.washington.edu/~pedrod/bvd.pdf. We can then calculate the overall average bias error and variance error for the prediction. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7WHEx8dq6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_bias_variance(predictions, true_labels):\n",
        "\n",
        "  # print(\"predictions: \")\n",
        "  # print(predictions)\n",
        "  # print(\"true labels: \")\n",
        "  # print(true_labels)\n",
        "\n",
        "  biases = []\n",
        "  variances = []\n",
        "  avg_errors = []\n",
        "  misclassified_individuals = []\n",
        "\n",
        "  # calculate the bias and variance for each value of X,y\n",
        "  # for misclassification loss\n",
        "  \n",
        "  #find whether each element is misclassified for each bootstrap sample \n",
        "  predictions_misclassified = predictions.apply(lambda x : x != true_labels.iloc[0], axis=1)\n",
        "\n",
        "  #count number of times misclassified for each datapoint across all bootstrap samples \n",
        "  counts = predictions_misclassified.apply(np.sum)\n",
        "\n",
        "  #average misclassification error for each individual/datapoint \n",
        "  avg_errors = counts.apply(lambda y : np.divide(y,len(predictions)))\n",
        "\n",
        "  index = 0\n",
        "\n",
        "  for avg_error in avg_errors:\n",
        "    (bias, variance) = (0, avg_error) if (avg_error <= 0.5) else (1, (1-avg_error))\n",
        "    # print(bias)\n",
        "    # print(variance)\n",
        "    biases.append(bias)\n",
        "    variances.append(variance)\n",
        "    if avg_error > 0.5:\n",
        "      misclassified_individuals.append(index)\n",
        "    index+= 1\n",
        "\n",
        "  avg_bias = np.mean(biases)\n",
        "  # avg_var = abs(np.mean(avg_errors) - avg_bias)\n",
        "  avg_var = abs(np.mean(variances))\n",
        "  avg_error = np.mean(avg_errors)\n",
        "\n",
        "  print(\"average error:\")\n",
        "  print(avg_error)\n",
        "  print(\"average bias:\")\n",
        "  print(avg_bias)\n",
        "  print(\"average variance:\")\n",
        "  print(avg_var)\n",
        "\n",
        "  return avg_bias, avg_var, avg_error, misclassified_individuals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvS_8PYmsWxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ygDcLeBeNq",
        "colab_type": "text"
      },
      "source": [
        "## Identifying Categories of Discrimination "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAxWqE2JBiT9",
        "colab_type": "text"
      },
      "source": [
        "We hope to address the question: Are models that exhibit high bias errors likely to introduce new categories of discrimination? \n",
        "\n",
        "We can therefore look at the bias and variance errors for different models.\n",
        "\n",
        "\n",
        "We want to see that if the variance is low and bias high, is it consistently discriminating against a certain subgroup, potentially introducing a new type of discrimination? Unlike other work such as http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, it doesn't have to be a protected characteristic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS-RcQVHudhq",
        "colab_type": "text"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kav2TxMqeTy2",
        "colab_type": "text"
      },
      "source": [
        "Creating the appropriate plots to visualise our results. We plot: \n",
        "\n",
        "1.   Bias error vs Variance error\n",
        "2.   Gamma value of RBF SVM vs Variance error\n",
        "3.   Gamma value of RBF SVM vs Bias error\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcztpNvvfDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt                                  \n",
        "def plot_bias_variance(biases, variances, gammas, cs, errors):   \n",
        "  print(\"plotting bias/var\") \n",
        "  plt.scatter(biases, variances)                                              \n",
        "  plt.title('bias vs variance errors')                                     \n",
        "  plt.xlabel('bias')                                                       \n",
        "  plt.ylabel('variance')                                                   \n",
        "  plt.show()\n",
        "\n",
        "  # plt.scatter(gammas, variances)\n",
        "  # plt.xscale('log')                                              \n",
        "  # plt.title('RBF SVM, C = 1 \\n gamma size vs variance errors')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('variance')                                                   \n",
        "  # plt.show()                                                            \n",
        "\n",
        "  # plt.scatter(gammas, biases)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, C = 1 \\n gamma size vs bias errors')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('bias')                                                   \n",
        "  # plt.show()            \n",
        "\n",
        "  # plt.scatter(gammas, errors)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, C = 1 \\n gamma size vs total error')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('error')                                                   \n",
        "  # plt.show()   \n",
        "\n",
        "  plt.scatter(cs, biases)                               \n",
        "  plt.xscale('log')                                                             \n",
        "  plt.title('RBF SVM, gamma=0.001, C value vs bias errors')                                     \n",
        "  plt.xlabel('C value')                                                       \n",
        "  plt.ylabel('bias')                                                   \n",
        "  plt.show()       \n",
        "\n",
        "  plt.scatter(cs, variances)                               \n",
        "  plt.xscale('log')                                                             \n",
        "  plt.title('RBF SVM, gamma=0.001, C value vs variance errors')                                     \n",
        "  plt.xlabel('C value')                                                       \n",
        "  plt.ylabel('variance')                                                   \n",
        "  plt.show()       \n",
        "\n",
        "#just an example of if we want to plot the misclassified individuals against a characteristic from the dataframe \n",
        "#might help to look for patterns \n",
        "def plot_misclassified(misclassified):\n",
        "  misclassified.reset_index().plot(kind='scatter', x='index', y='age') \n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmVUHkRN8YMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download CSV file containing all the info for the individuals who are consistently misclassified (i.e. >50% of the time, resulting in bias errors)\n",
        "def download_misclassified(misclassified):\n",
        "  misclassified.reset_index().to_csv('misclassified.csv', index=False)\n",
        "  from google.colab import files\n",
        "  files.download('misclassified.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwemXVIVuhGq",
        "colab_type": "text"
      },
      "source": [
        "# Main method (execute code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vpU8QERWV4i",
        "colab_type": "text"
      },
      "source": [
        "Main method to run the system, executing methods in appropriate sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCBsC158supR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  all_data = load_file()\n",
        "  training_data_and_labels, testing_data_and_labels = import_data(all_data)\n",
        "  biases = []\n",
        "  variances = []\n",
        "  total_errors = []\n",
        "  classifiers, gammas, cs = define_classifiers()\n",
        "  misclassified = []\n",
        "  equalised_odds = True\n",
        "\n",
        "  for classifier in classifiers:\n",
        "    print(classifier)\n",
        "    # clf = classifiers[classifier_names.index(classifier)]\n",
        "    clf = classifier\n",
        "    predictions, true_labels = classify(training_data_and_labels, testing_data_and_labels, clf)  \n",
        "    majority_predictions = predictions.mode().astype('int64')\n",
        "    if(equalised_odds):\n",
        "      make_fair(true_labels, majority_predictions)\n",
        "    bias, variance, total_error, misclassified_individuals = compute_bias_variance(predictions, true_labels)\n",
        "    biases.append(bias)\n",
        "    variances.append(variance)\n",
        "    total_errors.append(total_error)\n",
        "    #get the individuals which are misclassified on average (hence contributing to bias errors)\n",
        "    print(misclassified_individuals)\n",
        "    misclassified = testing_data_and_labels.iloc[misclassified_individuals]\n",
        "    # download_misclassified(misclassified)\n",
        "    # plot_misclassified(misclassified)\n",
        "\n",
        "  plot_bias_variance(biases, variances, gammas, cs, total_errors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-iJVK2Ec2w0",
        "colab_type": "code",
        "outputId": "6d04279b-89db-4153-ad05-91e12d63d4e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 873,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data\n",
            "SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "true negatives: 30 false positives: 4 false negatives: 2 true positives: 56\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-873-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-872-e85b26455a12>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequalised_odds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mmake_fair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajority_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmisclassified_individuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_bias_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mbiases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mvariances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-869-b12e153a9306>\u001b[0m in \u001b[0;36mcompute_bias_variance\u001b[0;34m(predictions, true_labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m#find whether each element is misclassified for each bootstrap sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mpredictions_misclassified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m#count number of times misclassified for each datapoint across all bootstrap samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-869-b12e153a9306>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m#find whether each element is misclassified for each bootstrap sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mpredictions_misclassified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m#count number of times misclassified for each datapoint across all bootstrap samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indexed_same\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only compare identically-labeled Series objects\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can only compare identically-labeled Series objects"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA8uQhlb8ARv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}