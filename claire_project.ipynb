{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "claire_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uBQkNPgD4D6o"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clairecoffey/project/blob/master/claire_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep76GcW0r5EF",
        "colab_type": "text"
      },
      "source": [
        "# Fairness and the bias-variance trade-off \n",
        "\n",
        "## Claire Coffey\n",
        "\n",
        "## June 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt8eZN7L71OB",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are studying bias and variance errors in the context of fairness, by exploring recidivism data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5j4K9fEtccc",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPz0FbDrdOB",
        "colab_type": "text"
      },
      "source": [
        "Imports: first import the relevant libraries used throughout. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cpVvJ1Dwm48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  from sklearn import svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3xUTfnrkM0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojFZOY-LtsdL",
        "colab_type": "text"
      },
      "source": [
        "# Read in recidivism data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLIFxjZ9roCK",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are studying recidivism data. We utilise the COMPAS recidivism dataset, which uses recidivism data from Broward County jail and has been explored in the following studies:\n",
        "\n",
        "\"The accuracy, fairness, and limits of predicting recidivism\", paper available at:\n",
        "https://advances.sciencemag.org/content/4/1/eaao5580#corresp-1\n",
        "\n",
        "\"Machine Bias\" ProPublica article, available at:\n",
        "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
        "\n",
        "The dataset used can be found at:\n",
        "https://github.com/propublica/compas-analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmO2E_MbHUGY",
        "colab_type": "text"
      },
      "source": [
        "Here we import and read in the recidivism data. Currently, we are using a selection of 1000 samples from this dataset for our predictions (the first 1000 samples of the dataset)\n",
        "\n",
        "We use a selection \n",
        "of fields from this dataset to predict recidivism classification (1 = will reoffend; 0 = will not reoffend). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk62wPdCURBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdCKnj1kqViI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_file():\n",
        "  full_data = False\n",
        "  print(\"loading data\")\n",
        "  if full_data:\n",
        "    # full dataset\n",
        "    file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/compas-scores-two-years%20-%20compas-scores-two-years.csv?token=ABPC6VNTFTGQBANNUJY2O4C6XGJGY\"\n",
        "  else:\n",
        "    # small subset of first 500/1000/2000 people\n",
        "    # file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/500-compas-scores-two-years%20-%20Sheet1%20(1).csv?token=ABPC6VOW7CBEIIGZVE6ZJYS6YKNHO\"\n",
        "    # file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/1000-compas-scores-two-years%20-%20Sheet1.csv\"\n",
        "    file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/2000-compas-scores-2-years.csv\"\n",
        "    # file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/2400-compas-scores-two-years%20-%20Sheet1.csv\"\n",
        "\n",
        "  # load CSV contents\n",
        "  all_data = pd.read_csv(file_path, delimiter=',', dtype={'sex': 'category', \n",
        "                                                          'age_cat': 'category',\n",
        "                                                          'race': 'category',\n",
        "                                                          'c_charge_degree': 'category',\n",
        "                                                          'c_charge_desc': 'category',\n",
        "                                                          'r_charge_degree': 'category',\n",
        "                                                          'r_charge_desc': 'category',\n",
        "                                                          'vr_charge_degree': 'category',\n",
        "                                                          'vr_charge_desc': 'category'\n",
        "                                                          })\n",
        "  print('loaded data')\n",
        "  #shuffle into random order so we aren't always testing/training with the same people\n",
        "  #but reset index (each individual still has the same ID)\n",
        "  all_data = all_data.sample(frac=1).reset_index(drop=True)\n",
        "  return all_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1WpLQEMZUNe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f5a96689-da85-4346-a930-026ddc7d0b5f"
      },
      "source": [
        "all_data = load_file()\n"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data\n",
            "loaded data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19AV7fOG0z4h",
        "colab_type": "text"
      },
      "source": [
        "## Import and process data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB8DQVf7g1Nn",
        "colab_type": "text"
      },
      "source": [
        "We import the data into a pandas DataFrame. We begin by cleaning the data, so the crime descriptions are simplified, removing duplicate categories. For example, we merge descriptions such as 'possession of cocaine' and 'possess cocaine', or 'burglary/weapon' and 'burglary and weapon', by removing prepositions, and replacing abreviations and similies. \n",
        "\n",
        "Then,  the categorical data is  split into different fields for each category, and encoded as 0 or 1. For example, an individual with characteristic \"sex: male\" would be encoded as \"male: 1, female: 0\". The sex category is then removed.\n",
        "\n",
        "We then consider which fields to use for prediction. This includes the removal of any fields/columns which contain many NaN values, since these cannot be handled by the classifiers. We choose to remove the columns with many NaNs rather than using an alternative approach such as replacing them with the average so as not to introduce other types of bias. We also then remove rows/individuals containing any further NaN values so there is no longer any NaN values present in the data. \n",
        "\n",
        "We then normalise all of the data in the dataframe, so that when fed into the classifier, the predicitons are not skewed (and potentially different forms of bias introduced).  We do this by using the StandardScaler in the sklearn preprocessing library, and we normalise the data to have a variance of 1.\n",
        "\n",
        "Finally, we define the number of testing/training samples desired and split the data into these two sets appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL_WqMSrbJM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_testing_samples = 400\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzJytmnMtkPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def clean_descriptions(description):\n",
        "  description = description.replace(' and ', ' ')\n",
        "  description = description.replace(' / ', ' ')\n",
        "  description = description.replace('possession', 'posess')\n",
        "  description = description.replace('possessing', 'posess')\n",
        "  description = description.replace('with', 'w/')\n",
        "  description = description.replace('w/ ', 'w/')\n",
        "  description = description.replace('w/', ' ')\n",
        "  description = description.replace('attempted', 'att')\n",
        "  description = description.replace('attempt', 'att')\n",
        "  description = description.replace('aggravated', 'agg')\n",
        "  description = description.replace('aggrav', 'agg') \n",
        "  description = description.replace(' of ', ' ')\n",
        "  return description\n",
        "\n",
        "def import_data(all_data):\n",
        "\n",
        "  encoded_sex = (pd.get_dummies(all_data['sex']))\n",
        "  all_data = all_data.drop(columns=['sex'])\n",
        "  all_data = all_data.join(encoded_sex)\n",
        "\n",
        "  encoded_age_cat = (pd.get_dummies(all_data['age_cat']))\n",
        "  all_data = all_data.drop(columns=['age_cat'])\n",
        "  all_data = all_data.join(encoded_age_cat)\n",
        "\n",
        "  encoded_race = (pd.get_dummies(all_data['race']))\n",
        "  all_data = all_data.drop(columns=['race'])\n",
        "  all_data = all_data.join(encoded_race)\n",
        "\n",
        "  encoded_c_charge_degree = (pd.get_dummies(all_data['c_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['c_charge_degree'])\n",
        "  all_data = all_data.join(encoded_c_charge_degree, rsuffix='_c')\n",
        "\n",
        "  #these are joined with suffixes because otherwise columns overlap \n",
        "  all_data['c_charge_desc'] = all_data['c_charge_desc'].astype(str).str.lower()\n",
        "  all_data['c_charge_desc'] = all_data['c_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_c_charge_desc = (pd.get_dummies(all_data['c_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['c_charge_desc'])\n",
        "  all_data = all_data.join(encoded_c_charge_desc, rsuffix='_c')\n",
        "\n",
        "  encoded_r_charge_degree = (pd.get_dummies(all_data['r_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['r_charge_degree'])\n",
        "  all_data = all_data.join(encoded_r_charge_degree, rsuffix='_r')\n",
        "\n",
        "  all_data['r_charge_desc'] = all_data['r_charge_desc'].astype(str).str.lower()\n",
        "  all_data['r_charge_desc'] = all_data['r_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_r_charge_desc = (pd.get_dummies(all_data['r_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['r_charge_desc'])\n",
        "  all_data = all_data.join(encoded_r_charge_desc, rsuffix='_r')\n",
        "\n",
        "  encoded_vr_charge_degree = (pd.get_dummies(all_data['vr_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['vr_charge_degree'])\n",
        "  all_data = all_data.join(encoded_vr_charge_degree, rsuffix='_vr')\n",
        "\n",
        "  all_data['vr_charge_desc'] = all_data['vr_charge_desc'].astype(str).str.lower()\n",
        "  all_data['vr_charge_desc'] = all_data['vr_charge_desc'].apply(clean_descriptions)\n",
        "  encoded_vr_charge_desc = (pd.get_dummies(all_data['vr_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['vr_charge_desc'])\n",
        "  all_data = all_data.join(encoded_vr_charge_desc, rsuffix='_vr')\n",
        "\n",
        "  all_data = all_data.drop(columns=['nan'])\n",
        "  all_data = all_data.drop(columns=['nan_vr'])\n",
        "  all_data = all_data.drop(columns=['nan_r'])\n",
        "\n",
        "  #drop columns not used for predictions, including info such as names, and columns with many NaN values \n",
        "  all_data.columns = map(str.lower, all_data.columns)\n",
        "  #dont use individual crimes, too much and too slow, only use severity of crimes and other info\n",
        "  all_data_simplified = all_data[['juv_fel_count','juv_misd_count','juv_other_count','priors_count','is_recid','is_violent_recid','event','female','male','25 - 45','greater than 45','less than 25','african-american','asian','caucasian','hispanic','native american','other','f','m','(f1)','(f2)','(f3)','(f6)','(m1)','(m2)','(mo3)']]\n",
        "\n",
        "  #remove rows containing NaN values \n",
        "  all_data_simplified = all_data_simplified.dropna()\n",
        "\n",
        "  #Renormalise the data so we have unit variance and mean 0 using built-in preprocessing method in sklearn\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  all_data_scaled = pd.DataFrame(scaler.fit_transform(all_data_simplified),columns=all_data_simplified.columns)\n",
        "  testing_data = all_data_scaled[:num_testing_samples]\n",
        "  training_data = all_data_scaled[num_testing_samples:]\n",
        "  all_labels = all_data[['two_year_recid']]\n",
        "  all_data = all_data_scaled\n",
        "  testing_labels = all_labels[:num_testing_samples]\n",
        "  training_labels = all_labels[num_testing_samples:]\n",
        "\n",
        "  # print(\"testing normalisation, printing mean and variance: \")\n",
        "  # print(all_data_scaled.mean())\n",
        "  # print(all_data_scaled.var())\n",
        "\n",
        "  all_data_and_labels = all_data_scaled.join(all_labels)\n",
        "\n",
        "  #split into training and testing with specific number of testing samples\n",
        "  #and training set to be the remainder\n",
        "  testing_data_and_labels = all_data_and_labels[:num_testing_samples]\n",
        "  training_data_and_labels = all_data_and_labels[num_testing_samples:]\n",
        "\n",
        "  # if(demographic_to_test != 'all'):\n",
        "    # testing_data_and_labels =  pd.DataFrame.reset_index(testing_data_and_labels.loc[testing_data_and_labels[demographic_to_test] > 0],drop=True)\n",
        "\n",
        "  print(\"training data:\")\n",
        "  print(training_data_and_labels)\n",
        "  print(\"testing data\")\n",
        "  print(testing_data_and_labels)\n",
        "\n",
        "  return training_data_and_labels, testing_data_and_labels, training_data, training_labels, all_data, all_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxX_6fJ7cUFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "085b4e87-eb12-44c7-927c-0b62d7402319"
      },
      "source": [
        "import_data(all_data)"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data:\n",
            "      juv_fel_count  juv_misd_count  ...     (mo3)  two_year_recid\n",
            "400       -0.114991       -0.181568  ... -0.080906               1\n",
            "401       -0.114991       -0.181568  ... -0.080906               0\n",
            "402        1.550712        1.749043  ... -0.080906               1\n",
            "403       -0.114991       -0.181568  ... -0.080906               1\n",
            "404       -0.114991       -0.181568  ... -0.080906               1\n",
            "...             ...             ...  ...       ...             ...\n",
            "1994      -0.114991       -0.181568  ... -0.080906               1\n",
            "1995      -0.114991       -0.181568  ... -0.080906               1\n",
            "1996      -0.114991       -0.181568  ... -0.080906               1\n",
            "1997      -0.114991       -0.181568  ... -0.080906               1\n",
            "1998      -0.114991       -0.181568  ... -0.080906               0\n",
            "\n",
            "[1599 rows x 28 columns]\n",
            "testing data\n",
            "     juv_fel_count  juv_misd_count  ...     (mo3)  two_year_recid\n",
            "0        -0.114991       -0.181568  ... -0.080906               0\n",
            "1         3.216414        1.749043  ... -0.080906               1\n",
            "2        -0.114991       -0.181568  ... -0.080906               0\n",
            "3        -0.114991       -0.181568  ... -0.080906               0\n",
            "4        -0.114991       -0.181568  ... -0.080906               0\n",
            "..             ...             ...  ...       ...             ...\n",
            "395      -0.114991       -0.181568  ... -0.080906               1\n",
            "396       1.550712       -0.181568  ... -0.080906               1\n",
            "397      -0.114991       -0.181568  ... -0.080906               0\n",
            "398      -0.114991       -0.181568  ... -0.080906               1\n",
            "399      -0.114991       -0.181568  ... -0.080906               1\n",
            "\n",
            "[400 rows x 28 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(      juv_fel_count  juv_misd_count  ...     (mo3)  two_year_recid\n",
              " 400       -0.114991       -0.181568  ... -0.080906               1\n",
              " 401       -0.114991       -0.181568  ... -0.080906               0\n",
              " 402        1.550712        1.749043  ... -0.080906               1\n",
              " 403       -0.114991       -0.181568  ... -0.080906               1\n",
              " 404       -0.114991       -0.181568  ... -0.080906               1\n",
              " ...             ...             ...  ...       ...             ...\n",
              " 1994      -0.114991       -0.181568  ... -0.080906               1\n",
              " 1995      -0.114991       -0.181568  ... -0.080906               1\n",
              " 1996      -0.114991       -0.181568  ... -0.080906               1\n",
              " 1997      -0.114991       -0.181568  ... -0.080906               1\n",
              " 1998      -0.114991       -0.181568  ... -0.080906               0\n",
              " \n",
              " [1599 rows x 28 columns],\n",
              "      juv_fel_count  juv_misd_count  ...     (mo3)  two_year_recid\n",
              " 0        -0.114991       -0.181568  ... -0.080906               0\n",
              " 1         3.216414        1.749043  ... -0.080906               1\n",
              " 2        -0.114991       -0.181568  ... -0.080906               0\n",
              " 3        -0.114991       -0.181568  ... -0.080906               0\n",
              " 4        -0.114991       -0.181568  ... -0.080906               0\n",
              " ..             ...             ...  ...       ...             ...\n",
              " 395      -0.114991       -0.181568  ... -0.080906               1\n",
              " 396       1.550712       -0.181568  ... -0.080906               1\n",
              " 397      -0.114991       -0.181568  ... -0.080906               0\n",
              " 398      -0.114991       -0.181568  ... -0.080906               1\n",
              " 399      -0.114991       -0.181568  ... -0.080906               1\n",
              " \n",
              " [400 rows x 28 columns],\n",
              "       juv_fel_count  juv_misd_count  ...      (m2)     (mo3)\n",
              " 400       -0.114991       -0.181568  ...  2.503815 -0.080906\n",
              " 401       -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 402        1.550712        1.749043  ... -0.399390 -0.080906\n",
              " 403       -0.114991       -0.181568  ...  2.503815 -0.080906\n",
              " 404       -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " ...             ...             ...  ...       ...       ...\n",
              " 1994      -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 1995      -0.114991       -0.181568  ...  2.503815 -0.080906\n",
              " 1996      -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 1997      -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 1998      -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " \n",
              " [1599 rows x 27 columns],\n",
              "       two_year_recid\n",
              " 400                1\n",
              " 401                0\n",
              " 402                1\n",
              " 403                1\n",
              " 404                1\n",
              " ...              ...\n",
              " 1994               1\n",
              " 1995               1\n",
              " 1996               1\n",
              " 1997               1\n",
              " 1998               0\n",
              " \n",
              " [1599 rows x 1 columns],\n",
              "       juv_fel_count  juv_misd_count  ...      (m2)     (mo3)\n",
              " 0         -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 1          3.216414        1.749043  ... -0.399390 -0.080906\n",
              " 2         -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 3         -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 4         -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " ...             ...             ...  ...       ...       ...\n",
              " 1994      -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 1995      -0.114991       -0.181568  ...  2.503815 -0.080906\n",
              " 1996      -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 1997      -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " 1998      -0.114991       -0.181568  ... -0.399390 -0.080906\n",
              " \n",
              " [1999 rows x 27 columns],\n",
              "       two_year_recid\n",
              " 0                  0\n",
              " 1                  1\n",
              " 2                  0\n",
              " 3                  0\n",
              " 4                  0\n",
              " ...              ...\n",
              " 1994               1\n",
              " 1995               1\n",
              " 1996               1\n",
              " 1997               1\n",
              " 1998               0\n",
              " \n",
              " [1999 rows x 1 columns])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUKy0e7IBQYI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Vx5Lghqe2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training_data_and_labels, testing_data_and_labels = import_data(all_data)\n",
        "# training_data_and_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5FvDlXt4se",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmKSjOeK4aRj",
        "colab_type": "text"
      },
      "source": [
        "##Selecting Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs36jQoqYmpp",
        "colab_type": "text"
      },
      "source": [
        "Here we select the classification model to use. We are using a selection of built-in classifiers in scikit-learn. \n",
        "\n",
        "Currently, we are using RBF SVM models (https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html). \n",
        "\n",
        "We define the boolean values ```vary_gamma ``` and  ```vary_c``` to define whether we are varying the gamma or C parameters; C defines the misclassification penalty and gamma defines the spread of the kernel. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZv_kuCWYo_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, neighbors, svm, gaussian_process, tree, ensemble, neural_network, metrics\n",
        "\n",
        "def define_classifiers():\n",
        "\n",
        "  vary_gamma = False\n",
        "  vary_c = False \n",
        "  polynomial = True\n",
        "  gammas = []\n",
        "  cs = []\n",
        "  classifiers = []\n",
        "  degrees = []\n",
        "\n",
        "  if vary_gamma:\n",
        "    gammas = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1,5,10,50,100,500,1000]\n",
        "    # gammas = [100, 1000, 10000, 100000]\n",
        "    cs=[10000]\n",
        "    # cs = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
        "    # gammas = [0.001]\n",
        "    # gammas = [1]\n",
        "    # c_val = 1000\n",
        "    #fix size of C if varying gamma\n",
        "\n",
        "    for gamma_val in gammas:\n",
        "      for c_val in cs:\n",
        "        classifiers.append(svm.SVC(gamma=gamma_val,C=c_val, probability=True))\n",
        "\n",
        "  if vary_c:\n",
        "    cs = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 10000000]\n",
        "    gamma_val = 1\n",
        "    #fix size of gamma if varying C\n",
        "    for c_val in cs:\n",
        "      classifiers.append(svm.SVC(gamma=gamma_val,C=c_val, probability=True))\n",
        "\n",
        "  if polynomial:\n",
        "    degrees = [0,1,2,3,4,5,6,7,8]\n",
        "    # degrees = [0]\n",
        "    for degree in degrees:\n",
        "      classifiers.append(svm.SVC(kernel='poly', degree=degree, probability=True))\n",
        "\n",
        "  return classifiers, gammas, cs, degrees\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQD8oVE-a4-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define_classifiers()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7PUBJeb4SQh",
        "colab_type": "text"
      },
      "source": [
        "## Bootstrapping "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxB1XKpC4fYi",
        "colab_type": "text"
      },
      "source": [
        "The classification process then uses a bootstrapping procedure with the chosen model, to generate predictions of recidivism classifications (1 = will not reoffend (positive case); 0 = will reoffend (negative case)).\n",
        "\n",
        "Bootstrapping (https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_41) is a sampling with replacement procedure. The sample size is the same as the size of the (training) dataset. The bootstrapping procedure is run many times to generate different training datasets, which will then be used for classification. In turn, the classification results will be used to calculate and study the bias and variance errors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzZ6oTWgv3FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_bootstrap(training_data_and_labels):\n",
        "  # this is one bootstrap sample \n",
        "  indices = np.random.randint(0,training_data_and_labels.shape[0] , training_data_and_labels.shape[0])\n",
        "  indices.sort()\n",
        "  data_points = []\n",
        "\n",
        "  for i in indices:\n",
        "    data_points.append(training_data_and_labels.iloc[i])\n",
        "\n",
        "  b_sample = pd.DataFrame(data_points)\n",
        "  \n",
        "  return b_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RPB3I-bwLF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b_sample = do_bootstrap(training_data_and_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yArDHhIdCfIS",
        "colab_type": "text"
      },
      "source": [
        "### Calculate average prediction for each individual over all bootstrap samples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjQeJu-rCpkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_avg_prediction(predictions):\n",
        "  #each row is bootstrap sample, each column an individual\n",
        "  return majority_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0p40u-y5sf2",
        "colab_type": "text"
      },
      "source": [
        "## Perform classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDUs2m5A6yP3",
        "colab_type": "text"
      },
      "source": [
        "Fit the model on the training data (which is one bootstrap data sample as defined above)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaofDUc-P8Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_and_predict(clf, b_sample, testing_data_and_labels):\n",
        "\n",
        "    #training data is everything apart from two year recid 0/1 label from the bootstrap sample\n",
        "    X_train = b_sample.drop(columns=['two_year_recid'])\n",
        "    y_train = b_sample['two_year_recid']\n",
        "    X_test = testing_data_and_labels.drop(columns=['two_year_recid'])\n",
        "    y_test = testing_data_and_labels['two_year_recid']\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    soft_score = clf.decision_function(X_test)\n",
        "    y_true = y_test\n",
        "\n",
        "    return y_pred, y_true, soft_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8e8STm77eG",
        "colab_type": "text"
      },
      "source": [
        "Perform classification for each bootstrap sample separately, and store these in a DataFrame, to be passed into the bias/variance calculations.\n",
        "This returns all of the predictions for each individual for each bootstrap sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm8R-Gt-OmNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(training_data_and_labels, testing_data_and_labels, clf):\n",
        "    count = 0\n",
        "\n",
        "    num_bootstraps = 100;\n",
        "    while count <= num_bootstraps:\n",
        "      b_sample = do_bootstrap(training_data_and_labels)\n",
        "      y_pred, y_true, soft_score = fit_and_predict(clf, b_sample, testing_data_and_labels)\n",
        "      # calc_conf_matrix_bootstrap(y_true, y_pred)\n",
        "      if(count == 0):\n",
        "        predictions = pd.DataFrame(pd.Series(y_pred)).transpose()\n",
        "        #true labels are the same for every sample so we only need 1 row in df\n",
        "        true_labels = pd.DataFrame(pd.Series(y_true)).transpose()\n",
        "      else:\n",
        "        predictions = predictions.append(pd.DataFrame(pd.Series(y_pred)).transpose())\n",
        "      count += 1\n",
        "      \n",
        "    return predictions, true_labels, soft_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H75T2WV0OqIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classify(training_data_and_labels, testing_data_and_labels, clf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMfhfngzvTX",
        "colab_type": "text"
      },
      "source": [
        "## Correcting for Fairness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBQkNPgD4D6o",
        "colab_type": "text"
      },
      "source": [
        "#Fairness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frcBqdbB3M22",
        "colab_type": "text"
      },
      "source": [
        "The definition of fairness is disputed, and there is not a single correct approach to ensuring fairness in machine learning. In general, as stated in https://arxiv.org/pdf/1711.08513.pdf, fairness in machine learning can be approached in two ways: fairness of the dataset itself; fairness of the model.\n",
        "\n",
        "Since we cannot control the process by which the data is collected, and the recidivism dataset already exists (likely with human and societal biases built-in), we will not be focusing on the former category. Although, there have been recent trends within the fairness and machine learning communities to argue the importance of the fairness in data collection, for example, in http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, the authors discuss the necessity of correcting for bias in the dataset, an approach which may actually increase the accuracy of the predictions, in contrast to approaches that exclusively focus on correcting for fairness in the models, at the expense of accuracy. Another area in which recent trends in fairness research have addressed is the importance of developing context-aware fairness measurements (https://arxiv.org/pdf/1805.05859.pdf). However, in our project we will focus on model-based fairness correction - ensuring the machine learning models are not perpetuating existing biases, or introducing new biases. We do this by using a widely used and accepted fairness measurement which is context-independent, known as **Equalised Odds** (http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf). This approach is not without criticism, however it provides a clear and well-motivated approach to achieving fair predictions across subgroups with different protected characteristics. We attempt to correct for fairness in relation to the protected characteristics found in the recidivism dataset (sex, race, age). Once our models are 'fair' in relation to this description, we can explore the relationship between bias and variance errors and the potential discovery of discrimination against new categories. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vab-_uQEx46Q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Equalised Odds\n",
        "\n",
        "As stated above, we are considering fairness in relation to the equalised odds metric (http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf). The definition as stated in this paper is as follows: \\\\\n",
        "We say that a predictor $\\hat{Y}$ satisfies equalized odds with respect to\n",
        "protected attribute $A$ and outcome $Y$, if $\\hat{Y}$ and $A$ are independent conditional on $Y$. Therefore, if the classification labels are $Y$ and $\\hat{Y}$, for an outcome $ y=1 $, $\\hat{Y}$ has equal true positive rates across all demographic groups, for example, the categories not female and female will have equal true positive rates. For an outcome  $ y=0 $, $\\hat{Y}$ has equal false positive rates across all demographic groups. This enforces equal bias and accuracy in all demographics. This can formally be stated as:\n",
        "$$ Pr \\left\\{ \\hat{Y}=1 | A = 0, Y = y \\right\\} = Pr \\left\\{ \\hat{Y}=1 | A = 0, Y = y \\right\\} , y \\in \\left\\{ 0,1 \\right\\}$$\n",
        "\n",
        "This approach punishes models that only perform well on the majority demographics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cat-Jl0kJu2C",
        "colab_type": "text"
      },
      "source": [
        "The following code is the implementation of equalised odds from the paper http://papers.nips.cc/paper/7151-on-fairness-and-calibration.pdf, the code is from the github repository https://github.com/gpleiss/equalized_odds_and_calibration/blob/master/eq_odds.py.\n",
        "\n",
        "This implementation equalises false positives and false negatives across demoographics, since in general, African Americans receive\n",
        "a disproportionate number of F.P. predictions as compared with Caucasians when automated risk tools are used in practice. In the context of recidivism, the 'positive case' is in fact a prediction of 0: the individual is predicted not to reoffend, so equalising false positives in this case fits with the equalised odds definition above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDXXgyFKJZOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_scores(predictions, soft_scores):\n",
        "\n",
        "  maximum = soft_scores.max()\n",
        "  minimum = soft_scores.min()\n",
        "\n",
        "  normalise = lambda x: ((x-minimum)/(maximum-minimum))\n",
        "  n_soft_scores = []\n",
        "  for score in soft_scores:\n",
        "    soft_score = normalise(score)\n",
        "    n_soft_scores.append(soft_score)\n",
        "\n",
        "  return n_soft_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2WbfvPkLImE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def constrain_with_equalised_odds(true_labels, group_ids, demographic, soft_scores):\n",
        "\n",
        "  #create csv file containing relevant fields: prediction, label, group \n",
        "  predictions = pd.DataFrame(soft_scores, columns=['prediction'])\n",
        "  true_labels = pd.melt(true_labels).rename(columns={'value':'label'})\n",
        "  group_ids = pd.melt(group_ids).rename(columns={'value':'group'})\n",
        "  eq_odds_input = pd.concat([predictions, true_labels], axis=1)\n",
        "  eq_odds_input = pd.concat([eq_odds_input, group_ids], axis=1)\n",
        "  eq_odds_input = eq_odds_input.drop(columns=[\"variable\"])\n",
        "  eq_odds_input.to_csv('/content/drive/My Drive/project_data/eq_odds.csv', index=True)\n",
        "\n",
        "  !python2 \"/content/drive/My Drive/project_data/eq_odds.py\" \"/content/drive/My Drive/project_data/eq_odds.csv\"\n",
        "\n",
        "  eq_odds_pred_group_0 = pd.read_csv(filepath_or_buffer='/content/group_0.csv', delimiter=',', header=0)\n",
        "  eq_odds_pred_group_1 = pd.read_csv(filepath_or_buffer='/content/group_1.csv', delimiter=',',header=0 )\n",
        "  eq_odds_pred = pd.concat([eq_odds_pred_group_0,eq_odds_pred_group_1])\n",
        "  eq_odds_pred =  pd.DataFrame.reset_index(eq_odds_pred,drop=True)\n",
        "  eq_odds_pred['round_predictions'] = [0 if (row < 0.5) else 1 for row in eq_odds_pred['predictions']]\n",
        "\n",
        "  return eq_odds_pred['round_predictions'], eq_odds_pred['true_labels']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5hobrODGwef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calc_conf_matrix(y_true, y_pred):\n",
        "  # get confusion matrix and compute tn,fp,fn,tp\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true.iloc[0].to_numpy(), y_pred.iloc[0].to_numpy()).ravel()\n",
        "  print(\"true negatives:\", tn, \"rate:\" , tn/(tn+fp+fn+tp), \"false positives:\", fp, \"rate:\", fp/(tn+fp+fn+tp) ,\"false negatives:\", fn,\"rate:\", fn/(tn+fp+fn+tp), \"true positives:\",tp,\"rate:\", tp/(tn+fp+fn+tp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrCnCymebZ9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calc_conf_matrix_bootstrap(y_true, y_pred):\n",
        "  # get confusion matrix and compute tn,fp,fn,tp\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "  print(\"true negatives:\", tn, \"rate:\" , tn/(tn+fp+fn+tp), \"false positives:\", fp, \"rate:\", fp/(tn+fp+fn+tp) ,\"false negatives:\", fn,\"rate:\", fn/(tn+fp+fn+tp), \"true positives:\",tp,\"rate:\", tp/(tn+fp+fn+tp))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnKB0sC3uU6-",
        "colab_type": "text"
      },
      "source": [
        "# Compute bias/variance errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdwnNxEnWqth",
        "colab_type": "text"
      },
      "source": [
        "We can decompose the error into the errors due to bias, and the errors due to variance, in order to study the behaviour of the model and the bias/variance tradeoff, as described by Domingos in:\n",
        "https://www.aaai.org/Papers/AAAI/2000/AAAI00-086.pdf\n",
        "\n",
        "The definitions of bias and variance are as follows:\n",
        "\n",
        "\n",
        "* **Bias** is the zero-one loss incurred for the main prediction relative to the optimal prediction. Where the \"main prediction\" is the modal classification for each individual across all bootstrap samples. \n",
        "*  **Variance** is the average zero-one loss incurred by all predictions relativeto the main prediction.  The variance is the loss incurred by the fluctuations around the main (modal) prediction in response to different bootstrap training sets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!-- https://www.stat.berkeley.edu/users/breiman/arcall96.pdf.  -->\n",
        "\n",
        "<!-- The error is also comprised of an error due to noise (in addition to bias and variance). However, as stated in http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, the noise is dependent on the data, not the model, so comparing the discrimination level in the form of bias and variance errors, the noise terms cancel since they are independent of the model. Therefore, differences in bias can be explored even without knowing the underlying noise of the data.  -->\n",
        "We can then calculate the overall average bias error and variance error for the model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uLPjLTYXiOZ",
        "colab_type": "code",
        "outputId": "24110589-1c48-4110-a204-78b6a41b8540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "predictions.mode()"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>360</th>\n",
              "      <th>361</th>\n",
              "      <th>362</th>\n",
              "      <th>363</th>\n",
              "      <th>364</th>\n",
              "      <th>365</th>\n",
              "      <th>366</th>\n",
              "      <th>367</th>\n",
              "      <th>368</th>\n",
              "      <th>369</th>\n",
              "      <th>370</th>\n",
              "      <th>371</th>\n",
              "      <th>372</th>\n",
              "      <th>373</th>\n",
              "      <th>374</th>\n",
              "      <th>375</th>\n",
              "      <th>376</th>\n",
              "      <th>377</th>\n",
              "      <th>378</th>\n",
              "      <th>379</th>\n",
              "      <th>380</th>\n",
              "      <th>381</th>\n",
              "      <th>382</th>\n",
              "      <th>383</th>\n",
              "      <th>384</th>\n",
              "      <th>385</th>\n",
              "      <th>386</th>\n",
              "      <th>387</th>\n",
              "      <th>388</th>\n",
              "      <th>389</th>\n",
              "      <th>390</th>\n",
              "      <th>391</th>\n",
              "      <th>392</th>\n",
              "      <th>393</th>\n",
              "      <th>394</th>\n",
              "      <th>395</th>\n",
              "      <th>396</th>\n",
              "      <th>397</th>\n",
              "      <th>398</th>\n",
              "      <th>399</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 400 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2    3    4    5    6    ...  393  394  395  396  397  398  399\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[1 rows x 400 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7WHEx8dq6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_bias_variance(predictions, true_labels):\n",
        "\n",
        "  # print(\"predictions: \")\n",
        "  # print(predictions)\n",
        "  # print(\"true labels: \")\n",
        "\n",
        "  biases = []\n",
        "  variances = []\n",
        "  avg_errors = []\n",
        "  misclassified_individuals = []\n",
        "  losses = []\n",
        "  noises = []\n",
        " \n",
        "  #find 'main' (mode) predction - used to calculate bias error \n",
        "  main_predictions = predictions.mode(dropna=False)  \n",
        "  print(main_predictions)\n",
        "  main_predictions_transposed = main_predictions.copy()\n",
        "  main_predictions_transposed = main_predictions_transposed.iloc[0,:]\n",
        "\n",
        "  #compare main (modal) prediction to true prediction \n",
        "  main_predictions_misclassified_relative_to_true = main_predictions.apply(lambda z : z != true_labels.iloc[0], axis=1)\n",
        "  #find whether each element is misclassified for each bootstrap sample \n",
        "  #find if each prediction is the same as the true prediction \n",
        "  predictions_misclassified_relative_to_true = predictions.apply(lambda x : x != true_labels.iloc[0], axis=1)\n",
        "  #find if the predictions are the same as the main prediction - used to calculate variance error\n",
        "  predictions_misclassified_relative_to_main = predictions.apply(lambda y : y != main_predictions_transposed, axis = 1)\n",
        "\n",
        "  #count number of times misclassified for each datapoint across all bootstrap samples \n",
        "  main_misclassified_true_counts = main_predictions_misclassified_relative_to_true.apply(np.sum)\n",
        "  misclassified_true_counts = predictions_misclassified_relative_to_true.apply(np.sum)\n",
        "  misclassified_main_counts = predictions_misclassified_relative_to_main.apply(np.sum)\n",
        "  # misclassified_noise_counts = predictions_misclassific_noise.apply(np.sum)\n",
        "\n",
        "  #average misclassification error for each individual/datapoint \n",
        "  #same as probability of incorrect classification\n",
        "  avg_true_errors = misclassified_true_counts.apply(lambda a : np.divide(a,len(predictions)))\n",
        "  avg_main_errors = misclassified_main_counts.apply(lambda b : np.divide(b,len(predictions)))\n",
        "  avg_main_true_errors = main_misclassified_true_counts.apply(lambda c : np.divide(c,len(main_predictions)))\n",
        "\n",
        "  for i in range(len(avg_true_errors)):\n",
        "    # if average error is less than 0.5 then it means the main prediction is the same as the optimal one\n",
        "    avg_main_true_error = avg_main_true_errors[i]\n",
        "    avg_true_error = avg_true_errors[i]\n",
        "    avg_main_error = avg_main_errors[i]\n",
        "    bias = 0 if avg_main_true_error <=0.5 else 1\n",
        "    variance = avg_main_error\n",
        "    #noise is the underlying variance of the data: error when the optimal and true classifications are the same\n",
        "    #independent of model, we can say it is 0 as doesn't effect bias/var relationship\n",
        "    #and when we tested this, it returned a value of 0 (all labels predicted correctly across all samples so no loss incurred)\n",
        "    noise = 0\n",
        "    c1 = ((2*(1-avg_true_error))-1)\n",
        "    c2 = 1 if avg_main_true_error <= 0.5 else -1\n",
        "    #loss according to domingos' decomposition\n",
        "    loss = (c1*noise) + bias + (c2*variance)\n",
        "    biases.append(bias)\n",
        "    variances.append(variance)\n",
        "    noises.append(noise)\n",
        "    losses.append(loss)\n",
        "    if avg_true_error > 0.5:\n",
        "      misclassified_individuals.append(i)\n",
        "\n",
        "  avg_bias = np.mean(biases)\n",
        "  # avg_var = abs(np.mean(avg_errors) - avg_bias)\n",
        "  avg_var = np.mean(variances)\n",
        "  # avg_error = np.mean(avg_errors)\n",
        "  avg_loss = np.mean(losses)\n",
        "  avg_noise = np.mean(noises)\n",
        "\n",
        "  print(\"average loss:\") \n",
        "  print(avg_loss)\n",
        "  print(\"average noise:\")\n",
        "  print(avg_noise)\n",
        "  print(\"average bias:\")\n",
        "  print(avg_bias)\n",
        "  print(\"average variance:\")\n",
        "  print(avg_var)\n",
        "\n",
        "  return avg_bias, avg_var, avg_loss, misclassified_individuals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAxWqE2JBiT9",
        "colab_type": "text"
      },
      "source": [
        "We hope to address the question: Are models that exhibit high bias errors likely to introduce new categories of discrimination? \n",
        "\n",
        "We can therefore look at the bias and variance errors for different models.\n",
        "\n",
        "\n",
        "We want to see that if the variance is low and bias high, is it consistently discriminating against a certain subgroup, potentially introducing a new type of discrimination? Unlike other work such as http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf, it doesn't have to be a protected characteristic.\n",
        "\n",
        "\n",
        "\n",
        "I think perhaps after fairness correction, we can look at who is misclassified and then what they have in common? Or, we can extract each \"subgroup\" based on whatever characteristics and analyse these - i.e. did FPR/FNR go up/down for a different subgroup after fairness correction?  Do this in same way as we do for \"demographics\" initially used for fairness correction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS-RcQVHudhq",
        "colab_type": "text"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kav2TxMqeTy2",
        "colab_type": "text"
      },
      "source": [
        "Creating the appropriate plots to visualise our results. We plot: \n",
        "\n",
        "1.   Bias error vs Variance error\n",
        "2.   Gamma value of RBF SVM vs Variance error\n",
        "3.   Gamma value of RBF SVM vs Bias error\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcztpNvvfDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt                                  \n",
        "def plot_bias_variance(biases, variances, gammas, cs, degrees, losses):   \n",
        "  # print(\"plotting bias/var\") \n",
        "  # plt.scatter(biases, variances)                                              \n",
        "  # plt.title('bias vs variance errors')                                     \n",
        "  # plt.xlabel('bias')                                                       \n",
        "  # plt.ylabel('variance')                                                   \n",
        "  # plt.show()\n",
        "\n",
        "  # plt.scatter(gammas, variances)\n",
        "  # plt.xscale('log')                                              \n",
        "  # plt.title('RBF SVM, C = 100000 \\n gamma size vs variance errors')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('variance')                                                   \n",
        "  # plt.show()                                                            \n",
        "\n",
        "  # plt.scatter(gammas, biases)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, C = 100000 \\n gamma size vs bias errors')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('bias')                                                   \n",
        "  # plt.show()            \n",
        "\n",
        "  # plt.scatter(gammas, losses)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, C = 100000 \\n gamma size vs total error')                                     \n",
        "  # plt.xlabel('gamma')                                                       \n",
        "  # plt.ylabel('error')                                                   \n",
        "  # plt.show()   \n",
        "\n",
        "  # plt.scatter(cs, biases)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, gamma=0.1, C value vs bias errors')                                     \n",
        "  # plt.xlabel('C value')                                                       \n",
        "  # plt.ylabel('bias')                                                   \n",
        "  # plt.show()       \n",
        "\n",
        "  # plt.scatter(cs, variances)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, gamma=0.1, C value vs variance errors')                                     \n",
        "  # plt.xlabel('C value')                                                       \n",
        "  # plt.ylabel('variance')                                                   \n",
        "  # plt.show()    \n",
        "\n",
        "  # plt.scatter(cs, losses)                               \n",
        "  # plt.xscale('log')                                                             \n",
        "  # plt.title('RBF SVM, gamma=0.1, C value vs zero-one loss')                                     \n",
        "  # plt.xlabel('C value')                                                       \n",
        "  # plt.ylabel('loss')                                                   \n",
        "  # plt.show()   \n",
        "\n",
        "  plt.scatter(biases, variances)                                              \n",
        "  plt.title('bias vs variance errors')                                     \n",
        "  plt.xlabel('bias')                                                       \n",
        "  plt.ylabel('variance')                                                   \n",
        "  plt.show()\n",
        "\n",
        "  plt.scatter(degrees, variances)\n",
        "  plt.title('poly SVM, degree size vs variance errors')                                     \n",
        "  plt.xlabel('degree')                                                       \n",
        "  plt.ylabel('variance')                                                   \n",
        "  plt.show()                                                            \n",
        "\n",
        "  plt.scatter(degrees, biases)                               \n",
        "  plt.title('poly SVM, degree size vs bias errors')                                     \n",
        "  plt.xlabel('degree')                                                       \n",
        "  plt.ylabel('bias')                                                   \n",
        "  plt.show()            \n",
        "\n",
        "  plt.scatter(degrees, losses)                               \n",
        "  plt.title('poly SVM, degree size vs generalisation errors')                                     \n",
        "  plt.xlabel('degree')                                                       \n",
        "  plt.ylabel('error')                                                   \n",
        "  plt.show()  \n",
        "\n",
        "#just an example of if we want to plot the misclassified individuals against a characteristic from the dataframe \n",
        "#might help to look for patterns \n",
        "def plot_misclassified(misclassified):\n",
        "  misclassified.reset_index().plot(kind='scatter', x='index', y='age') \n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmVUHkRN8YMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download CSV file containing all the info for the individuals who are consistently misclassified (i.e. >50% of the time, resulting in bias errors)\n",
        "def download_misclassified(misclassified, name):\n",
        "  # misclassified = np.asarray(misclassified)\n",
        "  csv_name = name+'.csv'\n",
        "  # misclassified = pd.DataFrame(misclassified)\n",
        "  # print(misclassified)\n",
        "  # np.savetxt(csv_name, misclassified, delimiter=\",\")\n",
        "  misclassified.to_csv(r''+name+'.csv', index=False)\n",
        "  # from google.colab import files\n",
        "  # files.download(csv_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nbxldKcpOHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vpU8QERWV4i",
        "colab_type": "text"
      },
      "source": [
        "Main method to run the system, executing methods in appropriate sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z19f8UDgdVYP",
        "colab_type": "code",
        "outputId": "7bde5ac3-c792-4719-fcad-622c0b8ff617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "all_data = load_file()\n",
        "training_data_and_labels, testing_data_and_labels, X, X_labels, all_X, all_X_labels = import_data(all_data)\n"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data\n",
            "loaded data\n",
            "training data:\n",
            "      juv_fel_count  juv_misd_count  ...     (mo3)  two_year_recid\n",
            "400       -0.114991       -0.181568  ... -0.080906               0\n",
            "401       -0.114991       -0.181568  ... -0.080906               0\n",
            "402       -0.114991       -0.181568  ... -0.080906               0\n",
            "403       -0.114991       -0.181568  ... -0.080906               0\n",
            "404       -0.114991       -0.181568  ... -0.080906               0\n",
            "...             ...             ...  ...       ...             ...\n",
            "1994      -0.114991       -0.181568  ... -0.080906               0\n",
            "1995      -0.114991       -0.181568  ... -0.080906               0\n",
            "1996      -0.114991       -0.181568  ... -0.080906               1\n",
            "1997      -0.114991       -0.181568  ... -0.080906               0\n",
            "1998      -0.114991       -0.181568  ... -0.080906               1\n",
            "\n",
            "[1599 rows x 28 columns]\n",
            "testing data\n",
            "     juv_fel_count  juv_misd_count  ...     (mo3)  two_year_recid\n",
            "0        -0.114991       -0.181568  ... -0.080906               0\n",
            "1        -0.114991       -0.181568  ... -0.080906               0\n",
            "2        -0.114991       -0.181568  ... -0.080906               1\n",
            "3        -0.114991       -0.181568  ... -0.080906               0\n",
            "4        -0.114991       -0.181568  ... -0.080906               1\n",
            "..             ...             ...  ...       ...             ...\n",
            "395      -0.114991       -0.181568  ... -0.080906               0\n",
            "396      -0.114991       -0.181568  ... -0.080906               1\n",
            "397      -0.114991       -0.181568  ... -0.080906               1\n",
            "398      -0.114991       -0.181568  ... -0.080906               0\n",
            "399      -0.114991       -0.181568  ... -0.080906               0\n",
            "\n",
            "[400 rows x 28 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVLKqQyndjw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "u,s,v = np.linalg.svd(all_X)\n",
        "# U = U[:,:2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPl-NUUfgZDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_labels = all_X_labels['two_year_recid'] == 1\n",
        "negative_labels = all_X_labels['two_year_recid'] == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_189KbCeq8H",
        "colab_type": "code",
        "outputId": "470796e1-1968-45c5-960f-b35001abe698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt                                  \n",
        "\n",
        "dim1 = 12\n",
        "dim2 = 13\n",
        "\n",
        "plt.plot(u[positive_labels,dim1], u[positive_labels,dim2], 'r.')\n",
        "plt.plot(u[negative_labels,dim1], u[negative_labels,dim2], 'b.')"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5754ab9898>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 303
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6osp14XIC9M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#so u has a value at each point, so i can get this by getting all entries for 12th dimesion to get 1 feature?\n",
        "dim12 = u[:,12]\n",
        "dim13 = u[:,13]\n",
        "\n",
        "#so then we can create dataframe from these to use for classification?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wBZXi_fR7yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_X_transformed = pd.DataFrame({'dim12':dim12, 'dim13':dim13}, index=all_X.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEYlBI7AXPa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_X_transformed_labels = all_X_transformed.join(all_X_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azH4k1sdXjsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data_and_labels = all_X_transformed_labels[num_testing_samples:]\n",
        "testing_data_and_labels = all_X_transformed_labels[:num_testing_samples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3n-ysxInG5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maxpc = 23\n",
        "# fig, ax = plt.subplots(maxpc, maxpc, figsize=(20,20))\n",
        "# for pc1 in range(maxpc):\n",
        "#   for pc2 in range(maxpc):\n",
        "#     ax[pc1,pc2].plot(u[positive_labels,pc1], u[positive_labels, pc2], 'r.')\n",
        "#     ax[pc1,pc2].plot(u[negative_labels,pc1], u[negative_labels, pc2], 'b.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCBsC158supR",
        "colab_type": "code",
        "outputId": "41d5e2d3-2d1c-4f66-e358-c3b98981c704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# all_data = load_file()\n",
        "# demographics = ['african-american', 'caucasian', 'hispanic', 'asian', 'other', 'male', 'female', 'less than 25', '25 - 45', 'greater than 45']\n",
        "demographics = ['african-american']\n",
        "# demographics = [ 'hispanic', 'asian', 'other', 'male', 'female', 'less than 25', '25 - 45', 'greater than 45']\n",
        "testing_data_and_labels_list = []\n",
        "training_data_and_labels_list = []\n",
        "# training_data_and_labels, testing_data_and_labels, X, X_labels = import_data(all_data)\n",
        "\n",
        "training_data_and_labels_list.append(training_data_and_labels)\n",
        "testing_data_and_labels_list.append(testing_data_and_labels)\n",
        "\n",
        "biases = []\n",
        "variances = []\n",
        "total_errors = []\n",
        "avg_losses = []\n",
        "biases_eq = []\n",
        "variances_eq = []\n",
        "total_errors_eq = []\n",
        "avg_losses_eq = []\n",
        "classifiers, gammas, cs, degrees = define_classifiers()\n",
        "all_misclassified = pd.DataFrame()\n",
        "all_misclassified_eq = pd.DataFrame()\n",
        "# misclassified = []\n",
        "# misclassified_eq= []\n",
        "equalised_odds = False\n",
        "k = 0\n",
        "\n",
        "for classifier in classifiers:\n",
        "  print(classifier)\n",
        "  clf = classifier\n",
        "  for i in range(len(training_data_and_labels_list)):\n",
        "    predictions, true_labels, soft_scores = classify(training_data_and_labels_list[i], testing_data_and_labels_list[i], clf)  \n",
        "    # predictions, true_labels = classify_noise(training_data_and_labels_list[i], testing_data_and_labels_list[i], clf)\n",
        "    majority_predictions = predictions.mode(numeric_only=True)\n",
        "    print(\"avg pred conf matrix:\")\n",
        "    calc_conf_matrix(true_labels, majority_predictions)\n",
        "    if(equalised_odds):\n",
        "      soft_scores = calculate_scores(predictions, soft_scores)\n",
        "      for demographic in demographics:\n",
        "        group_ids = [] \n",
        "        print(\"Equalising odds for \", demographic)\n",
        "        group_ids_normalised = testing_data_and_labels[demographic]\n",
        "        for n_id in group_ids_normalised:\n",
        "          group_id = 0 if n_id < 0 else 1\n",
        "          group_ids.append(group_id)\n",
        "        group_ids = pd.DataFrame([group_ids])\n",
        "        all_eq_predictions = []\n",
        "        for j in range (len(predictions.index)):\n",
        "          eq_predictions, true_eq_labels = constrain_with_equalised_odds(true_labels, group_ids, demographic, soft_scores)\n",
        "          all_eq_predictions.append(np.array(eq_predictions))\n",
        "        all_eq_predictions = pd.DataFrame(all_eq_predictions)\n",
        "        true_eq_labels = pd.DataFrame(np.array(true_eq_labels), columns=['two_year_recid'])\n",
        "        true_eq_labels = true_eq_labels.transpose()\n",
        "    if(equalised_odds):\n",
        "      #if not equalised odds we have to only use half of the predictions somehow otherwise it's not fair \n",
        "      #because in equalised odds we use half of it to determine parameters (validation)\n",
        "      predictions = predictions.iloc[:,:int(len(predictions.columns)/2)]\n",
        "      true_labels = true_labels.iloc[:,:int(len(true_labels.columns)/2)]\n",
        "    # noise_predictions = noise_predictions.iloc[:,:int(len(predictions.columns)/2)]\n",
        "    # print(predictions)\n",
        "    # fake_predictions = predictions.replace(predictions, 0)\n",
        "    # fake_true_labels = true_labels.replace(true_labels, 1)\n",
        "    bias, variance, avg_loss, misclassified_individuals = compute_bias_variance(predictions, true_labels)\n",
        "    biases.append(bias)\n",
        "    variances.append(variance)\n",
        "    avg_losses.append(avg_loss)\n",
        "    if(equalised_odds):\n",
        "      print(\"after fairness correction:\")\n",
        "      bias_eq, variance_eq, avg_loss_eq, misclassified_individuals_eq = compute_bias_variance(all_eq_predictions, true_eq_labels)\n",
        "      biases_eq.append(bias_eq)\n",
        "      variances_eq.append(variance_eq)\n",
        "      avg_losses_eq.append(avg_loss_eq)\n",
        "    #get the individuals which are misclassified on average (hence contributing to bias errors)\n",
        "    print(\"misclassified before: \", misclassified_individuals)\n",
        "    if(equalised_odds):\n",
        "      print(\"misclassified after: \", misclassified_individuals_eq)\n",
        "    for i in range(len(testing_data_and_labels_list)):\n",
        "      if(len(misclassified_individuals) > 0 ):\n",
        "        misclassified = testing_data_and_labels_list[i].iloc[misclassified_individuals]\n",
        "        # print(misclassified)\n",
        "        all_misclassified = all_misclassified.append(misclassified)\n",
        "        # all_misclassified = pd.concat(all_misclassified, misclassified)\n",
        "      if(equalised_odds):\n",
        "        if(len(misclassified_individuals_eq) > 0):\n",
        "          misclassified_eq = testing_data_and_labels_list[i].iloc[misclassified_individuals_eq]\n",
        "          # all_misclassified_eq = pd.concat(all_misclassified_eq, misclassified_eq)\n",
        "          all_misclassified_eq = all_misclassified_eq.append(misclassified_eq)\n",
        "    download_misclassified(all_misclassified, str(k)+'_misclassified_before')\n",
        "    if(equalised_odds):\n",
        "      download_misclassified(all_misclassified_eq, str(k)+'_misclassified_after')\n",
        "    k+=1\n",
        "    # plot_misclassified(misclassified)\n",
        "    \n",
        "print(\"before fairness correction:\")\n",
        "plot_bias_variance(biases, variances, gammas, cs, degrees, avg_losses)\n",
        "if(equalised_odds):\n",
        "  print(\"after fairness correction:\")\n",
        "  plot_bias_variance(biases_eq, variances_eq, gammas, cs, degrees, avg_losses_eq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=0, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "avg pred conf matrix:\n",
            "true negatives: 231 rate: 0.5775 false positives: 0 rate: 0.0 false negatives: 169 rate: 0.4225 true positives: 0 rate: 0.0\n",
            "   0    1    2    3    4    5    6    ...  393  394  395  396  397  398  399\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[1 rows x 400 columns]\n",
            "average loss:\n",
            "0.4225\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.4225\n",
            "average variance:\n",
            "0.0\n",
            "misclassified before:  [2, 4, 7, 12, 14, 15, 17, 19, 22, 23, 25, 26, 27, 34, 37, 40, 41, 43, 47, 49, 52, 53, 55, 56, 62, 65, 66, 67, 68, 69, 70, 74, 75, 79, 84, 85, 87, 88, 90, 92, 94, 96, 100, 102, 103, 105, 106, 111, 112, 114, 117, 120, 122, 128, 132, 134, 136, 140, 143, 145, 147, 159, 163, 167, 178, 181, 182, 189, 190, 191, 192, 193, 195, 197, 198, 201, 203, 205, 208, 209, 210, 212, 215, 217, 218, 221, 222, 223, 224, 225, 226, 227, 232, 237, 239, 240, 247, 251, 252, 253, 256, 257, 261, 262, 265, 268, 269, 275, 276, 280, 281, 283, 284, 288, 289, 292, 294, 295, 296, 297, 300, 303, 304, 312, 314, 316, 321, 325, 327, 329, 330, 333, 334, 335, 336, 338, 345, 347, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 365, 366, 367, 368, 371, 372, 373, 374, 377, 379, 382, 383, 384, 387, 388, 389, 393, 396, 397]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=1, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "avg pred conf matrix:\n",
            "true negatives: 231 rate: 0.5775 false positives: 0 rate: 0.0 false negatives: 169 rate: 0.4225 true positives: 0 rate: 0.0\n",
            "   0    1    2    3    4    5    6    ...  393  394  395  396  397  398  399\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[1 rows x 400 columns]\n",
            "average loss:\n",
            "0.4197029702970297\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.4225\n",
            "average variance:\n",
            "0.00353960396039604\n",
            "misclassified before:  [2, 4, 7, 12, 14, 15, 17, 19, 22, 23, 25, 26, 27, 34, 37, 40, 41, 43, 47, 49, 52, 53, 55, 56, 62, 65, 66, 67, 68, 69, 70, 74, 75, 79, 84, 85, 87, 88, 90, 92, 94, 96, 100, 102, 103, 105, 106, 111, 112, 114, 117, 120, 122, 128, 132, 134, 136, 140, 143, 145, 147, 159, 163, 167, 178, 181, 182, 189, 190, 191, 192, 193, 195, 197, 198, 201, 203, 205, 208, 209, 210, 212, 215, 217, 218, 221, 222, 223, 224, 225, 226, 227, 232, 237, 239, 240, 247, 251, 252, 253, 256, 257, 261, 262, 265, 268, 269, 275, 276, 280, 281, 283, 284, 288, 289, 292, 294, 295, 296, 297, 300, 303, 304, 312, 314, 316, 321, 325, 327, 329, 330, 333, 334, 335, 336, 338, 345, 347, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 365, 366, 367, 368, 371, 372, 373, 374, 377, 379, 382, 383, 384, 387, 388, 389, 393, 396, 397]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=2, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "avg pred conf matrix:\n",
            "true negatives: 230 rate: 0.575 false positives: 1 rate: 0.0025 false negatives: 153 rate: 0.3825 true positives: 16 rate: 0.04\n",
            "   0    1    2    3    4    5    6    ...  393  394  395  396  397  398  399\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[1 rows x 400 columns]\n",
            "average loss:\n",
            "0.3843069306930693\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.385\n",
            "average variance:\n",
            "0.0030693069306930694\n",
            "misclassified before:  [2, 4, 14, 15, 17, 19, 22, 23, 25, 26, 27, 34, 37, 40, 41, 43, 47, 49, 52, 53, 55, 56, 62, 65, 66, 67, 70, 74, 75, 79, 84, 85, 87, 88, 92, 94, 96, 100, 102, 103, 105, 106, 111, 112, 117, 120, 122, 128, 132, 134, 136, 140, 143, 147, 163, 167, 178, 181, 189, 190, 191, 192, 193, 195, 197, 198, 201, 203, 205, 208, 209, 210, 212, 215, 217, 218, 221, 222, 223, 225, 226, 232, 237, 239, 240, 247, 251, 253, 256, 257, 261, 265, 268, 269, 275, 276, 280, 281, 283, 284, 288, 289, 292, 294, 295, 296, 297, 300, 304, 312, 314, 316, 321, 325, 327, 329, 330, 333, 334, 335, 336, 338, 343, 347, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 365, 366, 367, 368, 371, 372, 373, 374, 377, 379, 382, 383, 384, 388, 389, 393, 396, 397]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "avg pred conf matrix:\n",
            "true negatives: 231 rate: 0.5775 false positives: 0 rate: 0.0 false negatives: 154 rate: 0.385 true positives: 15 rate: 0.0375\n",
            "   0    1    2    3    4    5    6    ...  393  394  395  396  397  398  399\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[1 rows x 400 columns]\n",
            "average loss:\n",
            "0.3895049504950495\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.385\n",
            "average variance:\n",
            "0.004752475247524753\n",
            "misclassified before:  [2, 4, 14, 15, 17, 19, 22, 23, 25, 26, 27, 34, 37, 40, 41, 43, 47, 49, 52, 53, 55, 56, 62, 65, 66, 67, 70, 74, 75, 79, 84, 85, 87, 88, 92, 94, 96, 100, 102, 103, 105, 106, 111, 112, 117, 120, 122, 128, 132, 134, 136, 140, 143, 147, 163, 167, 178, 181, 189, 190, 191, 192, 193, 195, 197, 198, 201, 203, 205, 208, 209, 210, 212, 215, 217, 218, 221, 222, 223, 225, 226, 232, 237, 239, 240, 247, 251, 252, 253, 256, 257, 261, 265, 268, 269, 275, 276, 280, 281, 283, 284, 288, 289, 292, 294, 295, 296, 297, 300, 304, 312, 314, 316, 321, 325, 327, 329, 330, 333, 334, 335, 336, 338, 347, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 365, 366, 367, 368, 371, 372, 373, 374, 377, 379, 382, 383, 384, 388, 389, 393, 396, 397]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=4, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "avg pred conf matrix:\n",
            "true negatives: 230 rate: 0.575 false positives: 1 rate: 0.0025 false negatives: 150 rate: 0.375 true positives: 19 rate: 0.0475\n",
            "   0    1    2    3    4    5    6    ...  393  394  395  396  397  398  399\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[1 rows x 400 columns]\n",
            "average loss:\n",
            "0.38304455445544555\n",
            "average noise:\n",
            "0.0\n",
            "average bias:\n",
            "0.3775\n",
            "average variance:\n",
            "0.01\n",
            "misclassified before:  [2, 4, 14, 15, 17, 19, 21, 22, 23, 25, 26, 27, 34, 37, 40, 43, 47, 52, 53, 55, 56, 62, 65, 66, 67, 70, 74, 75, 79, 84, 85, 87, 88, 92, 94, 96, 100, 102, 103, 105, 106, 111, 112, 114, 117, 120, 122, 128, 132, 134, 136, 140, 143, 145, 147, 163, 167, 178, 181, 189, 190, 191, 192, 193, 195, 197, 198, 201, 203, 205, 208, 209, 210, 212, 215, 217, 218, 221, 222, 223, 225, 226, 232, 237, 239, 240, 247, 251, 253, 256, 261, 265, 268, 269, 275, 280, 281, 283, 284, 288, 289, 292, 294, 295, 296, 297, 300, 304, 312, 314, 316, 321, 325, 327, 329, 330, 333, 334, 335, 336, 338, 347, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 365, 366, 367, 368, 371, 372, 373, 374, 377, 379, 382, 384, 388, 389, 393, 396, 397]\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=5, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOTgKGG1bVwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}