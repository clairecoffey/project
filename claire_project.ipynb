{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "claire_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZ1KJzPSP/8RfYjoOs2fur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clairecoffey/project/blob/master/claire_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPz0FbDrdOB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3xUTfnrkM0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.warn = warn\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import statistics\n",
        "from sklearn import model_selection, neighbors, svm, gaussian_process, tree, ensemble, neural_network, metrics\n",
        "from sklearn.model_selection import cross_val_score, LeaveOneOut, KFold, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.utils import resample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxA5DfO_ra1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup\n",
        "full_data = False\n",
        "bootstrapping = True\n",
        "cross_val = False\n",
        "recidivism_data = True\n",
        "X_train = []\n",
        "X_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "n_train = 0\n",
        "n_test = 0\n",
        "num_bootstraps = 50;\n",
        "num_datapoints = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLIFxjZ9roCK",
        "colab_type": "text"
      },
      "source": [
        "Import and read in data - using recidivism data. Currently use a selection of fields from this to predict recidivism classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N6qu4h_rsRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import data from git\n",
        "def import_data():\n",
        "    global num_datapoints\n",
        "    print(\"importing data\")\n",
        "    #At the moment we are always using recidivism data\n",
        "    if(recidivism_data):\n",
        "        if full_data:\n",
        "            # full 2 year compas scores dataset\n",
        "            file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/compas-scores-two-years%20-%20compas-scores-two-years.csv?token=ABPC6VJE3BXQDQ25BHIL7DK6SWGT2\"\n",
        "        else:\n",
        "            # small subset of first 500 people\n",
        "            file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/500-compas-scores-two-years%20-%20Sheet1%20(1).csv?token=ABPC6VPCC3IHHWDAJ3UKPVS6SWIYA\"\n",
        "        # Load CSV contents\n",
        "        # convert to numpy array?\n",
        "        all_data = (pd.read_csv(file_path, delimiter=',').values)\n",
        "\n",
        "        # We also preprocess relevant data (i.e. convert strings to ints)\n",
        "        # for all of these, 0 means missing or not valid category\n",
        "\n",
        "        # store fields separately for easy access; maybe this is overkill?\n",
        "        ids = all_data[:, 0]\n",
        "        full_names = all_data[:, 1]\n",
        "        firsts = all_data[:, 2]\n",
        "        lasts = all_data[:, 3]\n",
        "\n",
        "        # convert to integer categories where 0 is female, 1 is male, -1 is other\n",
        "        sexes = all_data[:, 4]\n",
        "        for i, sex in enumerate(sexes):\n",
        "            if sexes[i] == 'Female':\n",
        "                sexes[i] = 0\n",
        "            elif sexes[i] == 'Male':\n",
        "                sexes[i] = 1\n",
        "            else:\n",
        "                sexes[i] = -1\n",
        "\n",
        "        dobs = all_data[:, 5]\n",
        "        ages = all_data[:, 6]\n",
        "\n",
        "        # convert to integer categories where < 25 = 0; 25-45 = 1; >45 = 2\n",
        "        age_cats = all_data[:, 7]\n",
        "        for i, age_cat in enumerate(age_cats):\n",
        "            if age_cats[i] == 'Less than 25':\n",
        "                age_cats[i] = 0\n",
        "            elif age_cats[i] == '25 - 45':\n",
        "                age_cats[i] = 1\n",
        "            elif age_cats[i] == 'Greater than 45':\n",
        "                age_cats[i] = 2\n",
        "            else:\n",
        "                age_cats[i] = -1\n",
        "\n",
        "        #convert to integer categories\n",
        "        races = all_data[:, 8]\n",
        "        for i, race in enumerate(races):\n",
        "            if races[i] == 'African-American':\n",
        "                races[i] = 0\n",
        "            elif races[i] == 'Asian':\n",
        "                races[i] = 1\n",
        "            elif races[i] == 'Caucasian':\n",
        "                races[i] = 2\n",
        "            elif races[i] == 'Hispanic':\n",
        "                races[i] = 3\n",
        "            elif races[i] == 'Native American':\n",
        "                races[i] = 4\n",
        "            elif races[i] == 'Other':\n",
        "                races[i] = 5\n",
        "            else:\n",
        "                races[i] = -1\n",
        "\n",
        "        # juv_fel_counts = all_data[:,9]\n",
        "        # juv_misd_counts =\tall_data[:,10]\n",
        "        # juv_other_counts =\tall_data[:,11]\n",
        "        priors_counts = all_data[:, 12]\n",
        "        # days_b_screening_arrests =\tall_data[:,13]\n",
        "        # c_jail_ins\t= all_data[:,14]\n",
        "        # c_jail_outs =\tall_data[:,15]\n",
        "\n",
        "        # jail_times = np.empty(c_jail_ins.size)\n",
        "\n",
        "        # for i, jail_in in enumerate(c_jail_ins):\n",
        "        #   for j, jail_out in enumerate(c_jail_outs):\n",
        "        #     date_out = pd.to_datetime(c_jail_outs[j])\n",
        "        #     date_in = pd.to_datetime(c_jail_ins[i])\n",
        "        #     jail_times[i] = int(pd.to_numeric((date_out - date_in).days))\n",
        "\n",
        "        # c_charge_degrees =\tall_data[:,19]\n",
        "        # is_recids =\tall_data[:,21]\n",
        "\n",
        "        # # r_charge_degrees =\tall_data[:,23]\n",
        "        # # r_days_from_arrests =\tall_data[:,24]\n",
        "        # # r_jail_ins =\tall_data[:,27]\n",
        "        # # r_jail_outs =\tall_data[:,28]\n",
        "\n",
        "        # # for i, r_jail_in in enumerate(r_jail_ins):\n",
        "        # #   for j, r_jail_out in enumerate(r_jail_outs):\n",
        "        # #     date_out = pd.to_datetime(r_jail_outs[j])\n",
        "        # #     date_in = pd.to_datetime(r_jail_ins[i])\n",
        "        # #     r_jail_times[i] = int(pd.to_numeric((date_out - date_in).days))\n",
        "\n",
        "        # # is_violent_recids =\tall_data[:,33]\n",
        "        # # vr_charge_degrees =\tall_data[:,35]\n",
        "        # in_custodys =\tall_data[:,35]\n",
        "        # out_custodys =\tall_data[:,36]\n",
        "\n",
        "        # custody_length = np.empty(in_custodys.size)\n",
        "\n",
        "        # for i, in_custody in enumerate(in_custodys):\n",
        "        #   for j, out_custody in enumerate(out_custodys):\n",
        "        #     date_out = pd.to_datetime(out_custodys[j])\n",
        "        #     date_in = pd.to_datetime(in_custodys[i])\n",
        "        #     custody_length[i] = int(pd.to_numeric((date_out - date_in).days))\n",
        "\n",
        "        # priors_counts =\tall_data[:,37]\n",
        "        # starts =\tall_data[:,38]\n",
        "        # ends =\tall_data[:,39]\n",
        "        # events =\tall_data[:,40]\n",
        "        two_year_recids = all_data[:, 40]\n",
        "\n",
        "        #normal recidivism - 0 or 1 - this is what we are predicting\n",
        "        labels = two_year_recids.astype(int)\n",
        "        # #then make this the label\n",
        "        labels_list = [0, 1]\n",
        "\n",
        "        # For now we use a selection of fields to predict recidivism\n",
        "        # This is kinda arbitrary not what we would actually use \n",
        "        training_data = all_data[:, 6:12].astype(int)\n",
        "        # print(all_data[:, 6:12])\n",
        "\n",
        "        training_data_and_labels = []\n",
        "\n",
        "        # print(\"training data\")\n",
        "        # print(training_data)\n",
        "\n",
        "        # print(\"labels\")\n",
        "        # print(labels)\n",
        "\n",
        "        for i, individual in enumerate(training_data):\n",
        "            data_label_tuple = individual, labels[i]\n",
        "            # print(data_label_tuple)\n",
        "            training_data_and_labels.append(data_label_tuple)\n",
        "\n",
        "        # print(len(training_data_and_labels))\n",
        "        num_datapoints = len(training_data_and_labels)\n",
        "\n",
        "    return training_data, labels, labels_list, training_data_and_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRE1j4tas-1X",
        "colab_type": "text"
      },
      "source": [
        "Specify data and lables, and split data into training/test if using cross-validation instead of bootstrapping (not currently used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w-JwVdUtNwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_data(labels, training_data):\n",
        "  # Use global values for easy access everywhere \n",
        "    global X_train\n",
        "    global X_test\n",
        "    global y_train\n",
        "    global y_test\n",
        "    global n_train\n",
        "    global n_test\n",
        "\n",
        "    # Setting dataX and dataY\n",
        "    dataX = training_data.astype('int')\n",
        "\n",
        "    # Making sure labels are int values\n",
        "    dataY = labels.astype('int')\n",
        "\n",
        "    #  Creating Training and Test splits\n",
        "    # Do this because we still want to reserve portion of data as final test set\n",
        "    X_train, X_test, y_train, y_test = model_selection.train_test_split(dataX, dataY, test_size=0.2)\n",
        "\n",
        "    n_train = len(y_test)\n",
        "    n_test = len(y_test)\n",
        "\n",
        "    # return full data\n",
        "    return dataX, dataY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs36jQoqYmpp",
        "colab_type": "text"
      },
      "source": [
        "Choose the classifer/model to be used "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZv_kuCWYo_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_classifiers():\n",
        "    print(\"defining classifiers\")\n",
        "    # random classifiers to test\n",
        "    classifier_names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
        "                        \"Decision Tree\", \"Random Forest\", \"Neural Network\"]\n",
        "    # need to do grid search etc to find optimal parameters; these are hand-tweaked ones\n",
        "    classifiers = [\n",
        "        neighbors.KNeighborsClassifier(5),\n",
        "        svm.SVC(kernel=\"linear\", C=0.025),\n",
        "        svm.SVC(gamma=2, C=2),\n",
        "        gaussian_process.GaussianProcessClassifier(1.0 * gaussian_process.kernels.RBF(1.0), multi_class='one_vs_one'),\n",
        "        tree.DecisionTreeClassifier(max_depth=10),\n",
        "        ensemble.RandomForestClassifier(max_depth=10, n_estimators=100, max_features=2),\n",
        "        neural_network.MLPClassifier(alpha=0.01, max_iter=1000)]\n",
        "    # SVM with different kernels\n",
        "\n",
        "    # choose classifier\n",
        "    classifier = \"Random Forest\"\n",
        "    print(\"Classifer used: \", classifier)\n",
        "    return classifier, classifiers, classifier_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pu_SMPou1QV",
        "colab_type": "text"
      },
      "source": [
        "Classification process, in which - currently - we use bootstrapping and the defined classifier to generate predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7MVMALrvCm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(training_data, labels, labels_list, training_data_and_labels):\n",
        "    #firstly choose the classifier used \n",
        "    classifier, classifiers, classifier_names = define_classifiers()\n",
        "\n",
        "    # Creating a classifier object\n",
        "    clf = classifiers[classifier_names.index(classifier)]\n",
        " \n",
        "    count = 0\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    # If we are using bootstrapping\n",
        "    if (bootstrapping):\n",
        "        while count <= num_bootstraps:\n",
        "            #call bootstrap method to classify\n",
        "            clf, y_pred, y_true = bootstrap(clf, training_data_and_labels)\n",
        "            predictions.append(y_pred)\n",
        "            true_labels.append(y_true)\n",
        "            count += 1\n",
        "\n",
        "    if (cross_val):\n",
        "        # if we want to use cross validation:\n",
        "        clf = cross_validation(clf, classifier, training_data, labels, labels_list, classifiers, classifier_names)\n",
        "\n",
        "    return predictions, true_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5y-GVn5PM1h",
        "colab_type": "text"
      },
      "source": [
        "Run bootstrapping to do sampling with replacement to generate training and testing datasets (boot and out of bag examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaofDUc-P8Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bootstrapping is sampling with replacement\n",
        "def bootstrap(clf, training_data_and_labels):\n",
        "    global X_train\n",
        "    global y_train\n",
        "    global X_test\n",
        "    global y_test\n",
        "\n",
        "    # generate \"boot\" samples by sampling from whole dataset, using resample method\n",
        "    # specify number of samples\n",
        "    boot = resample(training_data_and_labels, replace=True, n_samples=round(num_datapoints*0.7), random_state=1)\n",
        "\n",
        "   \n",
        "    # initialise \n",
        "    found = False\n",
        "    # create out of bag samples list (to be used for testing)\n",
        "    oob = []\n",
        "\n",
        "    # use boot to fit model then use the out of bag samples for testing\n",
        "    # check if each datapoint is in 'boot' samples; append to 'oob' if not\n",
        "    for data_and_label in training_data_and_labels:\n",
        "        for element in boot:\n",
        "            current_elem = element\n",
        "            if np.array_equal(data_and_label[0], element[0]):\n",
        "                found = True\n",
        "                break\n",
        "            else:\n",
        "                found = False\n",
        "        if not found:\n",
        "            oob.append(data_and_label)\n",
        "            found = False\n",
        "\n",
        "    #define appropriate training/testing data and labels\n",
        "    training_data = []\n",
        "    training_labels = []\n",
        "    testing_data = []\n",
        "    testing_labels = []\n",
        "\n",
        "    for data_and_label in boot:\n",
        "        training_data.append(data_and_label[0])\n",
        "        training_labels.append(data_and_label[1])\n",
        "\n",
        "    for data_and_label in oob:\n",
        "        testing_data.append(data_and_label[0])\n",
        "        testing_labels.append(data_and_label[1])\n",
        "\n",
        "    # model is fit on the drawn sample and evaluated on the out-of-bag sample\n",
        "    X_train = training_data\n",
        "    y_train = training_labels\n",
        "    X_test = testing_data\n",
        "    y_test = testing_labels\n",
        "\n",
        "    # fit classifier \n",
        "    clf.fit(X_train, y_train)\n",
        "    #make predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_true = y_test\n",
        "\n",
        "    return clf, y_pred, y_true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jQQ4eXvWL5p",
        "colab_type": "text"
      },
      "source": [
        "Perform cross-validation if using (not working right now but also not using right now)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn5S8f4NWYCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation(clf, classifier, training_data, labels, labels_list, classifiers, classifier_names):\n",
        "    # Training the data using the train set\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    n_splits = 5\n",
        "\n",
        "    kf = KFold(n_splits)\n",
        "\n",
        "    # Create empty accuracy array of size k\n",
        "    accuracies = np.empty(n_splits)\n",
        "    count = 0\n",
        "    for train, validate in kf.split(X_train):\n",
        "        clf = classifiers[classifier_names.index(classifier)]\n",
        "        # Split the data into training and validation sets\n",
        "        X_training = X_train[train]\n",
        "        y_training = y_train[train]\n",
        "        x_validate = X_train[validate]\n",
        "        y_validate = y_train[validate]\n",
        "        # Train model\n",
        "        clf.fit(X_training, y_training)\n",
        "        # Calculate the performance for validation set but don't print\n",
        "        performance_metrics, y_pred = compute_metrics(clf, x_validate, y_validate, labels_list, show=False)\n",
        "        # Store accuracies\n",
        "        accuracies[count] = performance_metrics['ACC'].mean()\n",
        "        count += 1\n",
        "\n",
        "    # Compute the average cross validation score using the accuracies\n",
        "    cv_score = accuracies.mean()\n",
        "\n",
        "    # Compute the CV_score as average score across all n trials\n",
        "    # print(\"5-Fold cross validation scores:\", cv_score)\n",
        "    print(\"avg 5-Fold cross validation score:\", cv_score.mean())\n",
        "\n",
        "    return clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdwnNxEnWqth",
        "colab_type": "text"
      },
      "source": [
        "Compute Bias/Variance as in http://www.cems.uwe.ac.uk/~irjohnso/coursenotes/uqc832/tr-bias.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7WHEx8dq6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_bias_variance(predictions, true_labels):\n",
        "    # calculate bias and variance for each datapoint using bootstrap samples\n",
        "    # then we can use these to get the overall/avg across predictions\n",
        "    biases = []\n",
        "    variances = []\n",
        "    count = 0\n",
        "    prob_misclassified = 0\n",
        "    total_misclassified = 0\n",
        "    avg_errors = []\n",
        "\n",
        "    # calculate the bias and variance for each value of X,y\n",
        "    # for misclassification loss\n",
        "    # can also look at squared error loss as in https://pdfs.semanticscholar.org/9253/f3e13bca7e845e60394d85ddaec0d4cfc6d6.pdf\n",
        "    for pred_labels in predictions:\n",
        "        index = 0\n",
        "        labels = true_labels[count]\n",
        "        count += 1\n",
        "        total_misclassified = 0\n",
        "        # using all the bootstrap sample predictions\n",
        "        # loop through all of the predictions for a particular index (X value)\n",
        "        # and calculate the average misclassification error for this X\n",
        "        for pred_label in pred_labels:\n",
        "            true_label = labels[index]\n",
        "            if true_label == pred_label:\n",
        "                prob_misclassified = 0\n",
        "            else:\n",
        "                prob_misclassified = 1\n",
        "            index += 1\n",
        "            total_misclassified += prob_misclassified\n",
        "\n",
        "        avg_error = total_misclassified / len(pred_labels)\n",
        "        avg_errors.append(avg_error)\n",
        "\n",
        "    # print(avg_errors)\n",
        "\n",
        "    # define bias and variance as http://www.cems.uwe.ac.uk/~irjohnso/coursenotes/uqc832/tr-bias.pdf\n",
        "    for avg_error in avg_errors:\n",
        "        if avg_error <= 0.5:\n",
        "            biases.append(0)\n",
        "            variances.append(avg_error)\n",
        "        else:\n",
        "            biases.append(1)\n",
        "            variances.append(avg_error - 1)\n",
        "\n",
        "    avg_bias = (1/len(biases)) * sum(biases)\n",
        "    avg_var = abs((sum(avg_errors)/(len(avg_errors))) - avg_bias)\n",
        "\n",
        "    print(avg_bias)\n",
        "    print(avg_var)\n",
        "\n",
        "    # print(biases)\n",
        "    # print(variances)\n",
        "\n",
        "    return avg_bias, avg_var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeAo-_BGfz8A",
        "colab_type": "text"
      },
      "source": [
        "Computing/plotting/printing performance metrics (used in cross-val)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qj73KS6f3hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computing the different Performance Metrics                                                             \n",
        "def compute_metrics(clf, dataX, dataY, labels_list, show=True):                                           \n",
        "    # Using Confusion Matrix to compute metrics                                                           \n",
        "    def performance_measures(y_true, y_pred):                                                             \n",
        "        # Creating confusion matrix                                                                       \n",
        "        cnf_matrix = confusion_matrix(y_true, y_pred)                                                     \n",
        "        # Calculating metrics from conf matrixa                                                           \n",
        "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)                                                 \n",
        "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)                                                 \n",
        "        TP = np.diag(cnf_matrix)                                                                          \n",
        "        TN = cnf_matrix.sum() - (FP + FN + TP)                                                            \n",
        "        TP_rate = TP / (TP + FN)                                                                          \n",
        "        FP_rate = FP / (FP + TN)                                                                          \n",
        "        precision = TP / (TP + FP)                                                                        \n",
        "        f_measure = 2 * ((precision * TP_rate) / (precision + TP_rate))                                   \n",
        "        acc = (TP + TN) / (TP + FP + FN + TN)                                                             \n",
        "        avg_acc = acc.mean()                                                                              \n",
        "        # Returning                                                                                       \n",
        "        return (acc, TP_rate, FP_rate, precision, f_measure,                                              \n",
        "                np.round(cnf_matrix / cnf_matrix.sum(axis=1), 2))                                         \n",
        "                                                                                                          \n",
        "    y_pred = clf.predict(dataX)                                                                           \n",
        "                                                                                                          \n",
        "    acc, tp_rate, fp_rate, precision, f_measure, cnf_matrix = performance_measures(y_true=dataY,          \n",
        "                                                                                   y_pred=y_pred)         \n",
        "                                                                                                          \n",
        "    performance_metrics = {\"ACC\": acc, \"TP_Rate\": tp_rate, \"FP_Rate\": fp_rate,                            \n",
        "                           \"Precision\": precision, \"F_Measure\": f_measure,                                \n",
        "                           \"Confusion Matrix\": cnf_matrix}                                                \n",
        "                                                                                                          \n",
        "    if show:                                                                                              \n",
        "        show_metrics(performance_metrics, labels_list)                                                    \n",
        "                                                                                                          \n",
        "    return performance_metrics, y_pred                                                                    \n",
        "                                                                                                          \n",
        "                                                                                                          \n",
        "# printing performance metrics                                                                            \n",
        "def show_metrics(performance_metrics, labels_list):                                                       \n",
        "    for metric_name, metric in performance_metrics.items():                                               \n",
        "        if metric_name.startswith(\"Confusion\"):                                                           \n",
        "            print(\"Confusion Matrix: \")                                                                   \n",
        "            print(pd.DataFrame(metric,                                                                    \n",
        "                               index=labels_list,                                                         \n",
        "                               columns=labels_list))                                                      \n",
        "        else:                                                                                             \n",
        "            # We want the average of all classes                                                          \n",
        "            avg_metric = metric.mean()                                                                    \n",
        "            print(\"Metric : % s, Score : % 5.2f\" % (metric_name, avg_metric))                             \n",
        "                                                                                                                                                                                              \n",
        "                                                                                                          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6hzWMl_gCBb",
        "colab_type": "text"
      },
      "source": [
        "Plotting confusion matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwcCnFZ6gBDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                                                                                          \n",
        "# Plotting the Confusion Matrix                                                                           \n",
        "def plot_confusion_matrix(confusion_matrix, labels_list):                                                 \n",
        "    df_cm = pd.DataFrame(confusion_matrix,                                                                \n",
        "                         index=[i for i in labels_list],                                                  \n",
        "                         columns=[i for i in labels_list])                                                \n",
        "    plt.figure()                                                                                          \n",
        "    sn.set(font_scale=1.4)  # for label size                                                              \n",
        "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=\"Blues\")                                   \n",
        "    plt.show()      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vpU8QERWV4i",
        "colab_type": "text"
      },
      "source": [
        "Main method to run the system\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCBsC158supR",
        "colab_type": "code",
        "outputId": "07f2c4e5-a7e5-425a-9109-a5477c4980c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def main():\n",
        "    training_data, labels, labels_list, training_data_and_labels = import_data()\n",
        "    # dataX, dataY = define_data(labels, training_data)\n",
        "    predictions, true_labels = classify(training_data, labels, labels_list, training_data_and_labels)\n",
        "    bias, variance = compute_bias_variance(predictions, true_labels)\n",
        "    # plot_bias_variance(bias, variance)\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing data\n",
            "defining classifiers\n",
            "Classifer used:  Random Forest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-iJVK2Ec2w0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}