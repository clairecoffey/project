{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "claire_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clairecoffey/project/blob/master/claire_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep76GcW0r5EF",
        "colab_type": "text"
      },
      "source": [
        "# Fairness and the Bias/Variance Tradeoff\n",
        "\n",
        "## Claire Coffey\n",
        "\n",
        "## 1st May 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt8eZN7L71OB",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are studying bias and variance errors in the context of recidivism data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5j4K9fEtccc",
        "colab_type": "text"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPz0FbDrdOB",
        "colab_type": "text"
      },
      "source": [
        "Imports: first import the relevant libraries used throughout. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3xUTfnrkM0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojFZOY-LtsdL",
        "colab_type": "text"
      },
      "source": [
        "# Read in recidivism data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLIFxjZ9roCK",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are studying recidivism data. We utilise the COMPAS recidivism dataset, which uses recidivism data from Broward County jail and has been explored in the following studies:\n",
        "\n",
        "\"The accuracy, fairness, and limits of predicting recidivism\", paper available at:\n",
        "https://advances.sciencemag.org/content/4/1/eaao5580#corresp-1\n",
        "\n",
        "\"Machine Bias\" ProPublica article, available at:\n",
        "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
        "\n",
        "The dataset used can be found at:\n",
        "https://github.com/propublica/compas-analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmO2E_MbHUGY",
        "colab_type": "text"
      },
      "source": [
        "Here we import and read in the recidivism data. \n",
        "\n",
        "Currently, we use a selection \n",
        "of fields from this dataset to predict recidivism classification (0 = will not reoffend; 1 = will reoffend)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdCKnj1kqViI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_file():\n",
        "  full_data = False\n",
        "  print(\"importing data\")\n",
        "  if full_data:\n",
        "    # full dataset\n",
        "    file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/compas-scores-two-years%20-%20compas-scores-two-years.csv?token=ABPC6VJE3BXQDQ25BHIL7DK6SWGT2\"\n",
        "  else:\n",
        "    # small subset of first 500 people\n",
        "    file_path = \"https://raw.githubusercontent.com/clairecoffey/project/master/mphilproject/500-compas-scores-two-years%20-%20Sheet1%20(1).csv?token=ABPC6VM4ZHOTALXKV3BIB2K6WVJ7Q\"\n",
        "\n",
        "  # load CSV contents\n",
        "  all_data = pd.read_csv(file_path, delimiter=',', dtype={'sex': 'category', \n",
        "                                                          'age_cat': 'category',\n",
        "                                                          'race': 'category',\n",
        "                                                          'c_charge_degree': 'category',\n",
        "                                                          'c_charge_desc': 'category',\n",
        "                                                          'r_charge_degree': 'category',\n",
        "                                                          'r_charge_desc': 'category',\n",
        "                                                          'vr_charge_degree': 'category',\n",
        "                                                          'vr_charge_desc': 'category'\n",
        "                                                          })\n",
        "  return all_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1WpLQEMZUNe",
        "colab_type": "code",
        "outputId": "46d0737f-58a6-4544-858b-b72b14db30e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_data = load_file()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzJytmnMtkPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def import_data(all_data):\n",
        "\n",
        "  #is get_dummies the best way to do this?\n",
        "  encoded_sex = (pd.get_dummies(all_data['sex']))\n",
        "  all_data = all_data.drop(columns=['sex'])\n",
        "  all_data = all_data.join(encoded_sex)\n",
        "\n",
        "  encoded_age_cat = (pd.get_dummies(all_data['age_cat']))\n",
        "  all_data = all_data.drop(columns=['age_cat'])\n",
        "  all_data = all_data.join(encoded_age_cat)\n",
        "\n",
        "  encoded_race = (pd.get_dummies(all_data['race']))\n",
        "  all_data = all_data.drop(columns=['race'])\n",
        "  all_data = all_data.join(encoded_race)\n",
        "\n",
        "  encoded_c_charge_desc = (pd.get_dummies(all_data['c_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['c_charge_degree'])\n",
        "  all_data = all_data.join(encoded_c_charge_desc, rsuffix='_c')\n",
        "\n",
        "  #these are joined with suffixes because otherwise columns overlap \n",
        "  encoded_c_charge_desc = (pd.get_dummies(all_data['c_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['c_charge_desc'])\n",
        "  all_data = all_data.join(encoded_c_charge_desc, rsuffix='_c')\n",
        "\n",
        "  encoded_c_charge_desc = (pd.get_dummies(all_data['r_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['r_charge_degree'])\n",
        "  all_data = all_data.join(encoded_c_charge_desc, rsuffix='_r')\n",
        "\n",
        "  encoded_r_charge_desc = (pd.get_dummies(all_data['r_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['r_charge_desc'])\n",
        "  all_data = all_data.join(encoded_r_charge_desc, rsuffix='_r')\n",
        "\n",
        "  encoded_c_charge_desc = (pd.get_dummies(all_data['vr_charge_degree']))\n",
        "  all_data = all_data.drop(columns=['vr_charge_degree'])\n",
        "  all_data = all_data.join(encoded_c_charge_desc, rsuffix='_vr')\n",
        "\n",
        "  encoded_vr_charge_desc = (pd.get_dummies(all_data['vr_charge_desc']))\n",
        "  all_data = all_data.drop(columns=['vr_charge_desc'])\n",
        "  all_data = all_data.join(encoded_vr_charge_desc, rsuffix='_vr')\n",
        "\n",
        "  #drop columns not used for predictions, including info such as names, and coluns with many NaN values \n",
        "  training_data = all_data.drop(columns=['two_year_recid', 'r_days_from_arrest', 'id','name','first','last','dob','days_b_screening_arrest','c_jail_in','c_jail_out','c_case_number','c_offense_date','c_arrest_date','r_case_number','r_offense_date','r_jail_in','r_jail_out','vr_case_number','vr_offense_date','in_custody','out_custody','start','end','violent_recid'])\n",
        "\n",
        "  training_data_and_labels = training_data.join(all_data[['two_year_recid']])\n",
        "\n",
        "  #need to handle NaN values\n",
        "  #remove rows containing NaN values \n",
        "  training_data_and_labels = training_data_and_labels.dropna()\n",
        "\n",
        "  #split into training and testing with specific number of testing samples\n",
        "  #for now just set testing set to be first 50 samples in table \n",
        "  testing_data_and_labels = training_data_and_labels[:51]\n",
        "  #and training set to be the remainder\n",
        "  #this is also then consistent which is good for seeing patterns etc \n",
        "  training_data_and_labels = training_data_and_labels[51:]\n",
        "\n",
        "  return training_data_and_labels, testing_data_and_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Vx5Lghqe2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data_and_labels, testing_data_and_labels = import_data(all_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5FvDlXt4se",
        "colab_type": "text"
      },
      "source": [
        "# Classification - choose model and perform predictions using bootstrapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs36jQoqYmpp",
        "colab_type": "text"
      },
      "source": [
        "Here we select the classification model to use. We are using a selection of built-in classifiers in scikit-learn. (Currently, the parameters of the models are not optimal and are for testing purposes only. In order to select optimal parameters, a procedure such as grid search should be used.)\n",
        "\n",
        "The classification process then uses a bootstrapping procedure with the chosen model, to generate predictions of recidivism classifications Bootstrapping is a sampling with replacement procedure. We use this to generate many classification predictions for the given dataset, by running bootstrapping many times to generate different training and testing datasets. The training and testing datasets are called the \"boot\" and \"out of bag\" examples respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZv_kuCWYo_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, neighbors, svm, gaussian_process, tree, ensemble, neural_network, metrics\n",
        "\n",
        "def define_classifiers():\n",
        "    print(\"defining classifiers\")\n",
        "    # random classifiers to test\n",
        "    classifier_names = [\"RBF SVM\"]\n",
        "\n",
        "    classifiers = [\n",
        "        svm.SVC(gamma=2, C=2),\n",
        "        ]\n",
        "\n",
        "    # choose classifier\n",
        "    classifier = \"RBF SVM\"\n",
        "    print(\"Classifer used: \", classifier)\n",
        "    return classifier, classifiers, classifier_names\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQD8oVE-a4-R",
        "colab_type": "code",
        "outputId": "2109e6b9-3f0f-4f51-a326-58f612c87b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "define_classifiers()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defining classifiers\n",
            "Classifer used:  RBF SVM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('RBF SVM',\n",
              " [SVC(C=2, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "      decision_function_shape='ovr', degree=3, gamma=2, kernel='rbf', max_iter=-1,\n",
              "      probability=False, random_state=None, shrinking=True, tol=0.001,\n",
              "      verbose=False)],\n",
              " ['RBF SVM'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzZ6oTWgv3FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_bootstrap(training_data_and_labels):\n",
        "  #this is one bootstrap sample \n",
        "  #sample size is the same as the number of datapoints we have \n",
        "  indices = np.random.randint(0,training_data_and_labels.shape[0] , training_data_and_labels.shape[0])\n",
        "  #do we need to sort it? makes it clearer anyway\n",
        "  indices.sort()\n",
        "  data_points = []\n",
        "  out_of_bag_points = []\n",
        "  for i in indices:\n",
        "    data_points.append(training_data_and_labels.iloc[i])\n",
        "\n",
        "  b_sample = pd.DataFrame(data_points)\n",
        "\n",
        "  #need to find all of the datapoints not in the bootstrap sample  and add these to out of bag sample\n",
        "  # out_of_bag = training_data_and_labels.merge(b_sample, how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']\n",
        "  # out_of_bag = out_of_bag.drop(columns='_merge')\n",
        "\n",
        "  return b_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RPB3I-bwLF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_sample = do_bootstrap(training_data_and_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm8R-Gt-OmNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(training_data_and_labels, testing_data_and_labels):\n",
        "    classifier, classifiers, classifier_names = define_classifiers()\n",
        "    clf = classifiers[classifier_names.index(classifier)]\n",
        "    count = 0\n",
        "\n",
        "    # predictions = pd.DataFrame()\n",
        "    # true_labels = pd.DataFrame()\n",
        "\n",
        "    num_bootstraps = len(training_data_and_labels);\n",
        "    while count <= num_bootstraps:\n",
        "      b_sample = do_bootstrap(training_data_and_labels)\n",
        "      y_pred, y_true = fit_model(clf, b_sample, testing_data_and_labels)\n",
        "      if(count == 0):\n",
        "        predictions = pd.DataFrame(pd.Series(y_pred)).transpose()\n",
        "        #true labels are the same for every sample so we only need 1 row in df\n",
        "        true_labels = pd.DataFrame(pd.Series(y_true)).transpose()\n",
        "      else:\n",
        "        predictions = predictions.append(pd.DataFrame(pd.Series(y_pred)).transpose())\n",
        "      count += 1\n",
        "    return predictions, true_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaofDUc-P8Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(clf, b_sample, testing_data_and_labels):\n",
        "\n",
        "    #training data is everything apart from two year recid 0/1 label\n",
        "    # model is fit on the boot sample and evaluated on the out-of-bag sample\n",
        "    X_train = b_sample.drop(columns=['two_year_recid'])\n",
        "    y_train = b_sample['two_year_recid']\n",
        "    X_test = testing_data_and_labels.drop(columns=['two_year_recid'])\n",
        "    y_test = testing_data_and_labels['two_year_recid']\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_true = y_test\n",
        "\n",
        "    return y_pred, y_true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H75T2WV0OqIR",
        "colab_type": "code",
        "outputId": "ce814766-700f-4245-81a9-7d76ac469a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "classify(training_data_and_labels, testing_data_and_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defining classifiers\n",
            "Classifer used:  RBF SVM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(    0   1   2   3   4   5   6   7   8   9   ...  41  42  43  44  45  46  47  48  49  50\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " 0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
              " \n",
              " [287 rows x 51 columns],\n",
              "                 0   1   2   3   4   5   6   7   ...  43  44  45  46  47  48  49  50\n",
              " two_year_recid   0   1   1   0   0   0   1   0  ...   0   1   1   1   0   1   0   1\n",
              " \n",
              " [1 rows x 51 columns])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnKB0sC3uU6-",
        "colab_type": "text"
      },
      "source": [
        "# Compute bias/variance errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdwnNxEnWqth",
        "colab_type": "text"
      },
      "source": [
        "Using all these bootstrap predictions, we calculate the average misclassification error, as described in this paper: \n",
        "http://www.cems.uwe.ac.uk/~irjohnso/coursenotes/uqc832/tr-bias.pdf\n",
        "\n",
        "We can then decompose the error into the errors due to bias, and the errors due to variance, in order to study the behaviour of the model and the bias/variance tradeoff. This decomposition for classification is described in the following paper:\n",
        "https://homes.cs.washington.edu/~pedrod/bvd.pdf\n",
        "\n",
        "We define bias and variance in this context as in:\n",
        " http://www.cems.uwe.ac.uk/~irjohnso/coursenotes/uqc832/tr-bias.pdf\n",
        "\n",
        "Currently, bias error is always ```0.0```, because the average misclassification error for each datapoint is ```<0.5```, so, on average, each datapoint is classified correctly, and the errors are therefore all due to variance. This is not the result wanted/expected. \n",
        "\n",
        "Perhaps we can use the mean squared error (as in https://pdfs.semanticscholar.org/9253/f3e13bca7e845e60394d85ddaec0d4cfc6d6.pdf), instead of misclassification error and results would be different. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7WHEx8dq6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_bias_variance(predictions, true_labels):\n",
        "\n",
        "  print(predictions)\n",
        "  #predictions is a dataframe, where each row contains is for each bootstrap training sample and\n",
        "  # is a 0/1 classification prediction for each sample in the test set \n",
        "  #and each row  is for a bootstrap training sample \n",
        "\n",
        "  # calculate bias and variance for each datapoint using bootstrap samples\n",
        "  # then we can use these to get the overall/avg across predictions\n",
        "  biases = []\n",
        "  variances = []\n",
        "  avg_errors = []\n",
        "\n",
        "  # calculate the bias and variance for each value of X,y\n",
        "  # for misclassification loss\n",
        "\n",
        "  #find whether each element is misclassified for each bootstrap sample \n",
        "  predictions_misclassified = predictions.apply(lambda x : x != true_labels.iloc[0], axis=1)\n",
        "  print(predictions_misclassified)\n",
        "\n",
        "  #count number of times misclassified for each datapoint across all bootstrap samples \n",
        "  counts = predictions_misclassified.apply(np.sum)\n",
        "  print(counts)\n",
        "\n",
        "  #average misclassification error for each individual/datapoint \n",
        "  avg_errors = counts.apply(lambda y : np.divide(y,len(predictions)))\n",
        "\n",
        "  for avg_error in avg_errors:\n",
        "    (bias, variance) = (0, avg_error) if (avg_error <= 0.5) else (1, (avg_error-1))\n",
        "    biases.append(bias)\n",
        "    variances.append(variance)\n",
        "\n",
        "  avg_bias = np.mean(biases)\n",
        "  #is this defo the right way to do variance?\n",
        "  avg_var = abs(np.mean(avg_errors) - avg_bias)\n",
        "\n",
        "  print(avg_bias)\n",
        "  print(avg_var)\n",
        "\n",
        "  return avg_bias, avg_var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXSXrXQUppe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS-RcQVHudhq",
        "colab_type": "text"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCq-r-I2vbVb",
        "colab_type": "text"
      },
      "source": [
        "Plotting bias/variance errors for diff classifiers - just 1 at a time for now!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcztpNvvfDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt                                  \n",
        "def plot_bias_variance(bias, variance):                                                                             \n",
        "    plt.scatter(bias, variance)                                              \n",
        "    plt.title('bias vs variance errors')                                     \n",
        "    plt.xlabel('bias')                                                       \n",
        "    plt.ylabel('variance')                                                   \n",
        "    plt.show()                                                               \n",
        "                                                                             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwemXVIVuhGq",
        "colab_type": "text"
      },
      "source": [
        "# Main method (execute code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vpU8QERWV4i",
        "colab_type": "text"
      },
      "source": [
        "Main method to run the system\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCBsC158supR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  all_data = load_file()\n",
        "  training_data_and_labels, testing_data_and_labels = import_data(all_data)\n",
        "  predictions, true_labels = classify(training_data_and_labels, testing_data_and_labels)  \n",
        "  bias, variance = compute_bias_variance(predictions, true_labels)\n",
        "  plot_bias_variance(bias, variance)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-iJVK2Ec2w0",
        "colab_type": "code",
        "outputId": "2e9b5ddb-1091-4b5c-bf7e-d9b718e0c8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing data\n",
            "defining classifiers\n",
            "Classifer used:  RBF SVM\n",
            "    0   1   2   3   4   5   6   7   8   9   ...  41  42  43  44  45  46  47  48  49  50\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "0    0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   0\n",
            "\n",
            "[287 rows x 51 columns]\n",
            "       0     1     2      3      4   ...    46     47    48     49    50\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "..    ...   ...   ...    ...    ...  ...   ...    ...   ...    ...   ...\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "0   False  True  True  False  False  ...  True  False  True  False  True\n",
            "\n",
            "[287 rows x 51 columns]\n",
            "0      15\n",
            "1     272\n",
            "2     272\n",
            "3      15\n",
            "4      15\n",
            "5      15\n",
            "6     272\n",
            "7      15\n",
            "8      15\n",
            "9     272\n",
            "10     15\n",
            "11    272\n",
            "12      6\n",
            "13     15\n",
            "14    272\n",
            "15    272\n",
            "16     15\n",
            "17     15\n",
            "18    272\n",
            "19    272\n",
            "20    272\n",
            "21    272\n",
            "22    272\n",
            "23     15\n",
            "24    272\n",
            "25     15\n",
            "26      6\n",
            "27    272\n",
            "28     13\n",
            "29    272\n",
            "30    272\n",
            "31    272\n",
            "32    272\n",
            "33     12\n",
            "34    272\n",
            "35     15\n",
            "36    272\n",
            "37    272\n",
            "38    272\n",
            "39     13\n",
            "40     12\n",
            "41     15\n",
            "42    272\n",
            "43     15\n",
            "44    272\n",
            "45    272\n",
            "46    272\n",
            "47     15\n",
            "48    272\n",
            "49     15\n",
            "50    272\n",
            "dtype: int64\n",
            "0.5490196078431373\n",
            "0.00703696112591401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}